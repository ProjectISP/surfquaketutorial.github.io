{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to","text":"<p>Surfquake has been designed to streamline the workflow of estimating seismic source parameters. Comprehensive set of toolboxes automates the determination of arrival times, event locations, event magnitudes , attenuation, and moment tensor inversion. The software is programmed in Python 3 and offers the users the possibility of three programming levels for flexibility and customization.</p> <ul> <li> <p>The Core Library  allows users to integrate the core of surfQuake into their preexisting scripts, giving advanced users full control.</p> </li> <li> <p>The Command Line Interface  gives users access to an upper layer that simplifies the use of the core, enabling task execution through simple commands.</p> </li> <li> <p>The Graphical User Interface (GUI): Wraps the SurfQuake core in a user-friendly interface, making it accessible to users who prefer visual interaction. The GUI is connected to a SQLite database to store all results, ensuring easy retrieval and management of data.</p> </li> </ul>"},{"location":"#news","title":"News","text":"<p>Combine the flexibility of command-line tools, Python scripts, and Bash to supercharge your seismic trace processing and workflow:</p> <ul> <li> <p>surfquake 0.1.7 by October 2025: New Coincidence Trigger tool</p> </li> <li> <p>surfquake 0.1.6 by October 2025: New First Motion Polarity tool</p> </li> <li> <p>surfquake 0.1.5 by September 2025: New Signal Processing Module</p> </li> </ul> <p>Take a look at our recently published paper!</p> <ul> <li>Seismological Research Letters paper</li> </ul> Links Grid See surfQuake open-source core code Follow us on Twitter Subscribe on YouTube Questions and Issues Subscribe to news and future workshops"},{"location":"#toolboxes","title":"ToolBoxes","text":"<ul> <li>Create your Project </li> <li>Signal Processing </li> <li>Phase Picker </li> <li>Event Associator </li> <li>Coincidence Trigger </li> <li>Event Location </li> <li>Source Parameters </li> <li>First Motion Polarity </li> <li>Moment Tensor Inversion </li> <li>Data Base </li> <li>Utils </li> </ul>"},{"location":"#how-to-follow-surfquake-tutorial","title":"How to follow surfQuake Tutorial","text":"<p>The first step is to select which programming level is the most convenient for you. SurfQuake is divided into five Toolboxes: Picking, Association, Locate, Source, and Moment Tensor Inversion (MTI). Each toolbox link below contains a description of the software according to your programming level and an example.</p> <p>Let's start with Project  and then continue with the next toolboxes.</p>"},{"location":"#case-of-study-and-supporting-materials","title":"Case of Study and Supporting Materials","text":"<p>First: Case of study, contains a full example of using surfQuake with core Library Python scripts, Core Library bash script and expected results. Of course you can try to run the example using the GUI.</p> <p>Second: Earth Velocity models examples. Event Location format and MTI format.</p>"},{"location":"#cite-surfquake","title":"Cite surfQuake:","text":"<p>Cabieces, R., Junqueira, T. C., Harris, K., Relinque, J., Satriano, C. &amp; Vack\u00e1\u0159, J: SurfQuake: A new Python toolbox for the workflow process of seismic sources, Seismological Research Letters, may 2025.</p>"},{"location":"associate/","title":"Arraival times Assocotiation","text":"<p>The main goal of this tool is to associate the arrival times of different P- and S-waves to the their corresponding events. The associator algorythm used in surfquake is REAL (Zhang et al., 2019). The user needs to set the input parameters and point to the folder where the picks have been storaged. In th following sections it will be describe the surfquake imprementation of REAL, for a full description of the parameters visit REAL cookbook</p>"},{"location":"associate/#events-associator-gui","title":"Events Associator GUI","text":"<p>This is a screenshot of the Associator GUI.</p> <p></p> <p>The basics settings to associate inside a medium size region (300 x 300) km are show in the upper screenshot.</p> <ul> <li>Picking Directory: Root path to the folders containing the picking files. This files are automatically generated after running the picking tool.</li> <li> <p>Output Directory: Path to the forlder where the users wants the results of the arrival times association. Picking file nll_input.txt to Event Locaion tool will be also saved in this folder.</p> </li> <li> <p>Geographic Framework: Set the coordinates of your study region and remind loading the metadata from project tool. You can plot a map to be sure tour settings are ok.</p> </li> <li>Associator Parameters: These parameters are described in this link.</li> </ul>"},{"location":"associate/#config-files","title":"Config files","text":"<p>The parametrization of the Event Assocciator tool can be storaged in a file.ini. An example of this file is as follows:</p> <pre><code>[GEOGRAPHIC_FRAME]\nLAT_REF_MAX = 43.0000\nLAT_REF_MIN = 42.0000\nLON_REF_MIN = 0.8000\nLON_REF_MAX = 2.2000\nDEPTH = 20.00\n#\n[GRID_SEARCH_PARAMETERS]\nHORIZONTAL_SEARCH_RANGE = 4.80\nDEPTH_SEARCH_RANGE = 50.00\nEVENT_TIME_WINDOW = 120.00\nHORIZONTAL_SEARCH_GRID_SIZE = 0.60\nDEPTH_SEARCH_GRID_SIZE = 10.00\n#\n[TRAVEL_TIME_GRID_SEARCH]\nHORIZONTAL_RANGE = 5.00\nDEPTH_RANGE = 50.00\nDEPTH_GRID_RESOLUTION_SIZE = 2.00\nHORIZONTAL_GRID_RESOLUTION_SIZE = 0.01\n#\n[THRESHOLD_PICKS]\nMIN_NUM_P_WAVE_PICKS = 3\nMIN_NUM_S_WAVE_PICKS = 1\nNUM_STATIONS_RECORDED = 1\n</code></pre>"},{"location":"associate/#events-associator-from-cli","title":"Events Associator from CLI","text":""},{"location":"associate/#usage","title":"Usage","text":"<pre><code>&gt;&gt; surfquake associate [-h] -i INVENTORY_FILE_PATH -p DATA_DIR -c CONFIG_FILE_PATH -w WORK_DIR_PATH -s SAVE_DIR [-v]\n</code></pre>"},{"location":"associate/#interactive-help","title":"Interactive help","text":"<pre><code>&gt;&gt; surfquake associate -h\n</code></pre>"},{"location":"associate/#run-phase-picker-from-cli","title":"Run Phase Picker from CLI","text":"<pre><code>&gt;&gt; surfquake associate -i /surfquake_test/metadata/inv_all.xml -p /surfquake_test/test_picking_final -c /surfquake_test/config_files/real_config.ini\n</code></pre>"},{"location":"associate/#events-associator-from-library","title":"Events Associator from Library","text":""},{"location":"associate/#classes","title":"Classes","text":"<p><code>RealCore</code></p> <pre><code>class RealCore:\n    def __init__(self, metadata_file: str, real_config: Union[str, RealConfig], picking_directory: str, working_directory: str,\n                 output_directory: str):\n\n        \"\"\"\n        ----------\n        Parameters\n        ----------\n        metadata_file str: Path to the inventory information of stations coordinates and instrument description\n        real_config: Either the path to a real_config.ini or a RealConfig object.\n        picking_directory str: Root path to the folder wher picks P and S wave arrival time picks are storage\n        working_directory str: Root path to the folder that the associator uses to save intermediate files sucha as travel-times.\n        \"\"\"\n</code></pre>"},{"location":"associate/#methods","title":"Methods","text":"<p><code>run_real</code></p> <pre><code># instance method\ndef run_real(self):\n    # starts the events associator\n</code></pre>"},{"location":"associate/#example-using-library","title":"example using library","text":"<pre><code>from surfquakecore.real.real_core import RealCore\n\n# Inventory Information\ninventory_path = \"/meta/inv_all.xml\"\n\n# picking Output of PhaseNet\npicks_path = '/test_surfquake_core/picks'\n\n# Set working_directory and output\nworking_directory = '/test_surfquake_core/test_real/working_directory'\noutput_directory = '/test_surfquake_core/test_real/output_directory'\n\n# Set path to REAL configuration\nconfig_path = surfquake_test/config_files/real_config.ini\n# Run association\nrc = RealCore(inventory_path, config_path, picks_path, working_directory, output_directory)\nrc.run_real()\nprint(\"End of Events AssociationProcess, please see for results: \", output_directory)\n</code></pre>"},{"location":"bash_script/","title":"Bash Script Interaction with SurfQuake","text":"<p>SurfQuake provides a command-line interface (CLI) that integrates seamlessly with bash scripting. This allows users to automate large-scale waveform processing, post-analysis scripting, and plotting tasks efficiently.</p>"},{"location":"bash_script/#example-bash-workflow-for-seismic-event-processing","title":"Example: Bash Workflow for Seismic Event Processing","text":"<p>Below is a sample <code>bash</code> script that chains together two major SurfQuake commands: <code>quick</code> and <code>processing</code>.</p> <pre><code>#!/bin/bash\n\n# Activate your conda or virtual environment\nsource activate surfquake  # Replace with your environment manager if needed (e.g., 'conda activate surfquake')\n\n# Step 1: Quickly process individual waveform files with a custom post-processing script\nsurfquake quick \\\n  -w \"/Users/admin/Desktop/surf_test/nuclear/cut_event/*\" \\\n  --post_script /Users/admin/Desktop/surf_test/scripts/my_script.py \\\n  -c /Users/admin/Desktop/surf_test/config/config.yaml \\\n  --plot_config /Users/admin/Desktop/surf_test/config/plot.yaml \\\n  -o /Users/admin/Desktop/surf_test/output \\\n  -a \n\n# Step 2: Process a set of previously detected or catalogued events\nsurfquake processing \\\n  -p /Users/admin/Desktop/surf_test/project.pkl \\\n  -i /Users/admin/Desktop/surf_test/metadata/inv_all.xml \\\n  -e /Users/admin/Desktop/surf_test/events/eventos.csv \\\n  -c /Users/admin/Desktop/surf_test/config/config.yaml \\\n  -o /Users/admin/Desktop/surf_test/processed_events\n</code></pre>"},{"location":"bash_script/#key-components-explained","title":"Key Components Explained","text":"Parameter Description <code>-w</code> Path or glob pattern to waveform files (MiniSEED, HDF5, etc.) <code>-c</code> Configuration YAML file specifying preprocessing steps (filtering, tapering, normalization, etc.) <code>--post_script</code> Path to a Python script that can modify with your own processing <code>--plot_config</code> YAML file that defines plotting layout, styling, and pick output <code>-o</code> Output directory for saving processed or modified traces <code>-p</code> Path to a <code>.pkl</code> file with previously created surfquake project object <code>-i</code> Station inventory file (StationXML format) used for geolocation and instrument correction <code>-e</code> CSV file with origin times, locations, and hypocenter parameters for seismic events <code>-a</code> For automatic processing, no plotting waveforms"},{"location":"bash_script/#tips-for-reliable-bash-integration","title":"Tips for Reliable Bash Integration","text":"<ul> <li>Use absolute paths to avoid ambiguity, especially when running the script from <code>cron</code> or remote sessions.</li> <li>Quote paths that include wildcards (<code>*</code>) to ensure the shell doesn't prematurely expand them.</li> <li>Always activate your environment if SurfQuake is installed via Conda or a virtualenv.</li> <li>Use <code>--auto</code> flag if you want the quick command to run without any user interaction (headless processing).</li> </ul>"},{"location":"bash_script/#advanced-scheduling-or-looping","title":"Advanced: Scheduling or Looping","text":"<p>You can wrap this script into a <code>cron</code> job for periodic processing, or loop over multiple event folders:</p> <pre><code>for folder in /data/events/day_*; do\n  surfquake quick -w \"$folder/*.mseed\" -c config.yaml -o \"$folder/processed\"\ndone\n</code></pre>"},{"location":"beamforming_documentation/","title":"Beamforming","text":""},{"location":"beamforming_documentation/#description","title":"Description","text":"<p>The <code>beamforming</code> method performs array-based slowness and back-azimuth analysis of seismic waveforms. It is based on frequency-wavenumber (FK) analysis and supports additional methods like MUSIC, CAPON, and Multitaper Coherence. This process is useful for identifying wave arrival directions and apparent velocities in local, regional, or teleseismic arrays.</p> <p>This method computes slowness and azimuth values over time windows by stacking energy across an array of stations. It is particularly effective in resolving wavefront parameters and isolating coherent seismic phases in noisy environments.</p> <p>Internally, it calls the FK analysis provided by <code>surfquakecore.arrayanalysis</code>, and uses ObsPy\u2019s data structures for waveforms and metadata.</p> <p>Scientific Reference:</p> <p>Rost, S., &amp; Thomas, C. (2002). Array seismology: Methods and applications. Reviews of Geophysics, 40(3).  </p>"},{"location":"beamforming_documentation/#parameters","title":"Parameters","text":"<ul> <li><code>timewindow</code> (float): Duration of each sliding window for analysis (in seconds). Smaller windows improve temporal resolution; larger windows improve frequency resolution.</li> <li><code>overlap</code> (float): Fractional overlap between consecutive time windows. Values close to 1 provide smoother results.</li> <li><code>fmin</code>, <code>fmax</code> (float): Frequency band for filtering before beamforming. Filters out irrelevant noise and focuses on desired signal bands.</li> <li><code>smax</code> (float): Maximum slowness value (s/km) to consider in the slowness grid.</li> <li><code>slow_grid</code> (float): Resolution of the slowness grid (s/km). Smaller values yield higher-resolution slowness maps but are computationally more intensive.</li> </ul>"},{"location":"beamforming_documentation/#config-example","title":"Config Example","text":"<pre><code>Analysis:\n  process_1:\n    name: 'beam'\n    timewindow: 3.0\n    overlap: 0.05\n    fmin: 0.8\n    fmax: 2.2\n    smax: 0.3\n    slow_grid: 0.05\n    output_folder: \"/documents/beam_results\"\n</code></pre>"},{"location":"beamforming_documentation/#outputs","title":"Outputs","text":"<p>After running the beamforming step, the following attributes are available in the result object:</p> <ul> <li><code>relpower</code>: Relative power for each time window.</li> <li><code>abspower</code>: Absolute power estimates (if computed).</li> <li><code>AZ</code>: Back-azimuth (degrees from North) per window.</li> <li><code>Slowness</code>: Apparent slowness (s/km).</li> <li><code>T</code>: Center times of each window.</li> </ul> <p>You may visualize the results using the built-in <code>plot_beam()</code> method, which produces: - Time series of relative power - Time evolution of slowness and azimuth - Optional FK/slowness maps on keypress</p>"},{"location":"beamforming_documentation/#peak-detection","title":"Peak Detection","text":"<p>To extract beamforming-derived arrivals, use the <code>detect_beam_peaks()</code> method. It supports:</p> <ul> <li>Slowness filtering by phase type (regional/teleseismic)</li> <li>Minimum power thresholding</li> <li>Azimuth constraints</li> <li>Minimum time separation between detected peaks</li> </ul> <p>This is useful for automated or semi-automated phase picking and event localization.</p>"},{"location":"beamforming_documentation/#notes","title":"Notes","text":"<ul> <li>Requires an <code>ObsPy Stream</code> and corresponding <code>Inventory</code>.</li> <li>Trims traces to overlapping time intervals.</li> <li>Traces are filtered to the defined bandpass before analysis.</li> <li>Outputs can be saved and reloaded with <code>to_pickle()</code> and <code>from_pickle()</code> methods.</li> </ul>"},{"location":"beamforming_documentation/#plotting-and-cli-usage","title":"Plotting and CLI Usage","text":"<p>Beamforming results can be visualized and analyzed using the <code>surfquake beamplot</code> command-line utility. This tool reads a serialized <code>.beam</code> file and provides rich options for interactive visualization and automated peak detection.</p>"},{"location":"beamforming_documentation/#basic-usage","title":"Basic Usage","text":"<pre><code># Plot a saved FK beamforming result\nsurfquake beamplot --file ./output/2024.123.beam\n</code></pre>"},{"location":"beamforming_documentation/#peak-detection-examples","title":"Peak Detection Examples","text":"<pre><code># Detect phase peaks with regional constraints\nsurfquake beamplot --file event.beam --find_solutions regional --write_path solutions.txt\n\n# Apply custom slowness constraints (JSON format)\nsurfquake beamplot --file beam.beam --find_solutions '{\"Pn\": [0.05, 0.08], \"Lg\": [0.18, 0.30]}'\n\n# Add azimuth filtering\nsurfquake beamplot -f beam.beam --find_solutions regional --baz_range 100 150\n\n# Set minimum relative power threshold\nsurfquake beamplot -f beam.beam --find_solutions teleseismic --min_power 0.2\n</code></pre>"},{"location":"beamforming_documentation/#cli-options","title":"CLI Options","text":"<ul> <li><code>--file</code>, <code>-f</code> (required): Path to the <code>.beam</code> file (gzip-pickled <code>TraceBeamResult</code>).</li> <li><code>--save_path</code>: File path to save the plotted figure.</li> <li><code>--find_solutions</code>: Define phase constraints for peak detection. Accepts <code>'regional'</code>, <code>'teleseismic'</code>, or a custom JSON dictionary.</li> <li><code>--baz_range</code>: Filter solutions within a specific backazimuth range (in degrees).</li> <li><code>--min_power</code>: Minimum relative power to accept a peak (default: <code>0.6</code>).</li> <li><code>--write_path</code>: Output file to append detected peak information.</li> </ul>"},{"location":"beamforming_documentation/#slowness-map","title":"Slowness map","text":"<ul> <li>The figure includes time evolution of relative power, slowness, and backazimuth.</li> <li>Clicking or keypresses in the GUI will trigger slowness map popups for selected time points.</li> <li>This visualization is highly interactive and useful for manual phase inspection.</li> </ul> <p>Point with the mouse to the power peaks and press a key number to select a method to plot the slowness map:</p> <ul> <li><code>Available methods</code>: Beamforming algorithm to use:<ul> <li>1 <code>FK</code>: Classical frequency-wavenumber method.</li> <li>2 <code>CAPON</code>: Adaptive beamforming with improved resolution.</li> <li>3 <code>MTP.COHERENCE</code>: Multitaper-based coherence stacking.</li> <li>4 <code>MUSIC</code>: High-resolution direction-of-arrival estimation method. Using one cluster.</li> </ul> </li> </ul>"},{"location":"characteristic_functions_documentation/","title":"Characteristic Functions in SurfQuake","text":"<p>Characteristic Functions (CFs) are core tools in seismic signal processing used to detect and quantify features like onset, amplitude, or signal clarity. SurfQuake supports three main CFs as part of its processing stack: Signal-to-Noise Ratio (SNR), Envelope, Spectral Entropy and Kurtosis. Each of these functions enhances interpretability and supports tasks like phase picking, event detection, and signal classification.</p>"},{"location":"characteristic_functions_documentation/#1-signal-to-noise-ratio-snr","title":"1. Signal-to-Noise Ratio (SNR)","text":"<p>This method measures how prominent a signal is relative to background noise, typically over short- and long-term averaging windows.</p>"},{"location":"characteristic_functions_documentation/#supported-methods","title":"Supported Methods","text":"<ul> <li>classic: Uses the classic STA/LTA (Short-Term Average / Long-Term Average) ratio   (Allen, 1978)</li> <li>recursive: Applies a recursive version of STA/LTA for computational efficiency  </li> <li>z_detect: Implements a simplified z-score detector</li> </ul>"},{"location":"characteristic_functions_documentation/#parameters","title":"Parameters","text":"<ul> <li><code>sign_win</code>: Short-term window (seconds)</li> <li><code>noise_win</code>: Long-term window (seconds)</li> </ul>"},{"location":"characteristic_functions_documentation/#config-example","title":"Config Example","text":"<pre><code>Analysis:\n  process_1:\n    name: 'concat'\n  process_2:\n    name: 'rmean'\n    method: 'linear'\n  process_3:\n    name: 'taper'\n    method: 'cosine'\n    max_percentage: 0.05\n  process_4:\n    name: 'filter'\n    type: 'bandpass'\n    fmin: 0.5\n    fmax: 8.0\n    corners: 4\n    zerophase: True\n  process_1:\n    name: 'snr'\n    method: 'classic'\n    sign_win: 1\n    noise_win: 40\n</code></pre>"},{"location":"characteristic_functions_documentation/#2-envelope","title":"2. Envelope","text":"<p>The envelope of a seismic trace provides a smooth representation of amplitude over time, often used for locating energy bursts and improving event visibility.</p>"},{"location":"characteristic_functions_documentation/#methods","title":"Methods","text":"<ul> <li>FULL (default): Uses the analytic signal (Hilbert transform) to compute the full envelope.</li> <li>SMOOTH: Optionally applies lowpass filtering after envelope extraction.</li> </ul>"},{"location":"characteristic_functions_documentation/#parameters_1","title":"Parameters","text":"<ul> <li><code>method</code>: <code>'FULL'</code> or <code>'SMOOTH'</code></li> <li><code>corner_freq</code>: Lowpass filter cutoff (only for <code>'SMOOTH'</code>)</li> </ul>"},{"location":"characteristic_functions_documentation/#config-example_1","title":"Config Example","text":"<pre><code>Analysis:\n  process_1:\n    name: 'concat'\n  process_2:\n    name: 'rmean'\n    method: 'linear'\n  process_3:\n    name: 'taper'\n    method: 'cosine'\n    max_percentage: 0.05\n  process_4:\n    name: 'filter'\n    type: 'bandpass'\n    fmin: 0.5\n    fmax: 8.0\n    corners: 4\n    zerophase: True\n  process_5:\n    name: 'envelope'\n    method: 'SMOOTH'\n    corner_freq: 0.15\n</code></pre>"},{"location":"characteristic_functions_documentation/#reference","title":"Reference","text":"<ul> <li>Kanasewich, E. R. (1981). Time Sequence Analysis in Geophysics.</li> </ul>"},{"location":"characteristic_functions_documentation/#3-spectral-entropy","title":"3. Spectral Entropy","text":"<p>Spectral entropy quantifies the disorder in the frequency content of a seismic trace over time. High entropy indicates noise or dispersed signals, while low entropy points to coherent, narrowband energy.</p>"},{"location":"characteristic_functions_documentation/#parameters_2","title":"Parameters","text":"<ul> <li><code>win</code>: Window length in seconds (default: 2.0)</li> <li><code>overlap</code>: Overlap between windows as a fraction (0.0\u20131.0)</li> <li><code>normalize</code>: Normalize entropy values between 0 and 1</li> </ul>"},{"location":"characteristic_functions_documentation/#config-example_2","title":"Config Example","text":"<pre><code>Analysis:\n  process_1:\n    name: 'entropy'\n    win: 2.0\n    overlap: 0.5\n</code></pre>"},{"location":"characteristic_functions_documentation/#reference_1","title":"Reference","text":"<ul> <li>Ince, T., &amp; Togneri, R. (2007). A new spectral entropy feature for seismic event detection. Digital Signal Processing, 17(3), 675\u2013689.</li> </ul>"},{"location":"characteristic_functions_documentation/#3-kurtosis","title":"3. Kurtosis","text":"<p>The CF kurtosis is considered one of the best detector. The signal is filtered with a bank of overlapping narrow band filters and the the kurtosis is computed in sliding windows.</p>"},{"location":"characteristic_functions_documentation/#parameters_3","title":"Parameters","text":"<ul> <li><code>CF_decay_win</code>: Window length in seconds, smaller (e.g., 2) improves detection but noiser CFs</li> <li><code>fmin</code>: min frequency for narrow band filter bank</li> <li><code>fmax</code>: max frequency for narrow band filter bank</li> </ul>"},{"location":"characteristic_functions_documentation/#config-example_3","title":"Config Example","text":"<pre><code>Analysis:\n  process_1:\n    name: 'kurtosis'\n    CF_decay_win: 4.0\n    fmin: 0.5\n    fmax: 8.5\n</code></pre>"},{"location":"characteristic_functions_documentation/#reference_2","title":"Reference","text":"<ul> <li>Poiata, N., C. Satriano, J.-P. Vilotte, P. Bernard, and K. Obara (2016). Multi-band array detection and location of seismic sources recorded by dense seismic networks, Geophys. J. Int., 205(3), 1548-1573, doi:10.1093/gji/ggw071.</li> </ul>"},{"location":"characteristic_functions_documentation/#applications","title":"Applications","text":"<ul> <li>SNR: Event detection, onset detection, phase picking</li> <li>Envelope: Characterizing energy bursts, highlighting surface waves</li> <li>Spectral Entropy: Identifying coherent signals vs. noise, triggering logic</li> <li>Kurtosis: Detectect phases onset in very noise contaminated signals</li> </ul> <p>These functions can be used as standalone processors or in combination with pickers, visualizations, or further classification models.</p>"},{"location":"characteristic_functions_documentation/#output","title":"Output","text":"<p>Each CF modifies the trace data to represent the calculated function. These modified traces can then be plotted, further processed, or used as features in detection algorithms.</p> <ol> <li>Filtered seismograms: bandpass [0.5-8.0] Hz</li> </ol> <p></p> <ol> <li>SNR: Classic sta/lta (1/40)s</li> </ol> <p></p> <ol> <li>Envelope: Envelope and then smoothing with a lowpass filter of 0.15 Hz</li> </ol> <p></p> <ol> <li>Kurtosis: Kurtosis wit fmin and fmax 0.5 and 8-0 Hz and CF_decay_win of 4.0</li> </ol> <p></p>"},{"location":"coincidence/","title":"Overview","text":"<p>Coincidence Trigger proceess input wavefroms to surrogate it as Charachteristic Functions. Then, associate CFs threshold in time spans to events. It is very useful to roughtly detect event time and separate picks for those events. surfQuake offer a coincidence trigger tool that uses signal-to-noise ratio and kurtosis CFs. The core of the coincidence trigger can be also found here ObsPy</p> <p>References: <pre><code>        Allen, R. (1982). Automatic phase pickers: Their present use and future prospects. Bulletin of the Seismological Society of America, 72(6B), S225-S242.\n\n        Poiata, N., C. Satriano, J.-P. Vilotte, P. Bernard, and K. Obara (2016). Multi-band array detection and \n        location of seismic sources recorded by dense seismic networks, Geophys. J. Int.,\n        205(3), 1548-1573, doi:10.1093/gji/ggw071.\n</code></pre></p>"},{"location":"coincidence/#coincidence-trigger-from-cli","title":"Coincidence Trigger from CLI","text":"<p>The command trigg runs the coincidence trigger. It is important to set the config.ini file to determine the methodoly and the parametrization. Let's see it.</p>"},{"location":"coincidence/#config-file","title":"Config file","text":"<p>This config.ini is used for setting the coincidence trigger for a regional event scenario. Be in mind that is split in specific algorythms parametrization (SNR or Kurtosis), and the clustering, where user configurate the frquency bandwidth for processing waveforms and the association thresholds.</p> <pre><code>[Kurtosis]\nCF_decay_win = 4.0\n\n[STA_LTA]\nmethod = classicstalta\nsta_win = 1\nlta_win = 40\n\n[Cluster]\nmethod_preferred = Kurtosis\ncentroid_radio = 60\ncoincidence = 4\nthreshold_off = 5\nthreshold_on = 35\nfmin = 0.5\nfmax = 8.0\n</code></pre> <p>Details:</p> <p>CF_decay_win: The constant CF_decay_win (in seconds) determines the memory of the recursive envelope/kurtosis.</p> <p>sta_win: signal window in seconds.</p> <p>lta_win: noise window in seconds.</p> <p>method_preferred: SNR or Kurtosis.</p> <p>coincidence: Number of traces satisfying the threshold_on to declare an event.</p> <p>threshold_on / threshold_ff: Threshold to determine when it is trigger the coincidences in the charachteristic functions.</p> <p>fmin / fmax: Frequency range where is be applied the nawworband filter bank (kurtosis) or the bandpass filter for SNR.</p>"},{"location":"coincidence/#usage","title":"Usage","text":"<pre><code>surfquake trigg\n\nKey Arguments:\n            -p, --project_file        [REQUIRED] Path to a saved project files\n            -o, --output_folder       [REQUIRED] Directory for processed output\n            -c, --config_file         [REQUIRED] Processing configuration (YAML)\n            -n, --net                 [OPTIONAL] Network code filter\n            -s, --station             [OPTIONAL] Station code filter\n            -ch, --channel            [OPTIONAL] Channel filter\n            --min_date                [OPTIONAL] Filter Start date (format: YYYY-MM-DD HH:MM:SS), DEFAULT min date of the project\n            --max_date                [OPTIONAL] Filter End date   (format: YYYY-MM-DD HH:MM:SS), DEFAULT max date of the project\n            --span_seconds            [OPTIONAL] Select and merge files in sets of time spans, DEFAULT 86400\n            --plot                    [OPTIONAL] Plot events and Characteristic Functions\n            --picking_file            [OPTIONAL] If set a picking file, this will be separated accoring to found events inside cluster\n\nUsage Example:\n            &gt; surfquake trigg -p ./project.pkl  -c config.yaml -o ./output_folder -ch \"HHZ\" --min_date \"2024-01-01 00:00:00\" \n            --max_date \"2024-01-04 00:00:00\" --span_seconds  86400 --picking_file ./pick.txt --plot\n\n            &gt; surfquake trigg -h # request help\n</code></pre> <p>If user set for example --picking_file ./pick_file.txt, the picking file will be separated according to cluster parametrization. Be in mind that might be you have pick all seismograms inside your project and generate the picking file, but before locate, you need to associate this picks to events. The output of the coincidence trigger is as follows coincidence_sum.txt</p> <pre><code>date;hour;num_traces;coincidence_sum;duration\n2015-09-17;15:12:08.4;17;17.0;56.1\n</code></pre> <p>In the figure it is shown a very simple example of the plotting output from trigg tool. The plot shows a vertical red line with the event and the CFs in orange together with the raw seismograms.</p> <ol> <li> <p>SNR Coincidence Trigger </p> </li> <li> <p>Kurtosis Coincidence Trigger </p> </li> </ol>"},{"location":"coincidence/#classes-methods","title":"Classes &amp; Methods","text":"<p><code>CoincidenceTrigger</code></p> <pre><code>class CoincidenceTrigger:\n    def __init__(self, projects: list, coincidence_config: Union[str, CoincidenceConfig], picking_file=None,\n                 output_folder=None, plot=None)\n\n        \"\"\"\n\n\n        Attributes:\n        - projects (list): a list of projects surfproject objects\n        - coincidence_config (str, CoincidenceConfig): path to config.ini or directly a Dataclass CoincidenceConfig\n        - picking_file (str): path to the picking file\n        - output_folder (str): path to the picking file\n        - plot (bool): if user desires to write the output plot\n\n        Methods:\n        - optimized_project_processing(self)\n        \"\"\"\n</code></pre>"},{"location":"coincidence/#examples-scripts","title":"Examples Scripts","text":"<pre><code>from datetime import datetime\nfrom surfquakecore.project.surf_project import SurfProject\n\n\n\ndef parse_datetime(dt_str: str) -&gt; datetime:\n    # try with microseconds, fall back if not present\n    for fmt in (\"%Y-%m-%d %H:%M:%S.%f\", \"%Y-%m-%d %H:%M:%S\"):\n        try:\n            return datetime.strptime(dt_str, fmt)\n        except ValueError:\n            continue\n    raise ValueError(f\"Date string not in expected format: {dt_str}\")\n\ndef make_abs(path: Optional[str]) -&gt; Optional[str]:\n    return os.path.abspath(path) if path else None\n\n\nif __name__ == '__main__':\n\n    project_file = \"./project.pkl\"\n    config_file = \"./config.ini\"\n    picking_file = \"./nll_picks.txt\" # output of surfquake pick or surfquake polarity\n    output_folder = \"./\"\n\n    net = None\n    channel = \"HHZ|BHZ\"\n    station = None\n    span_seconds = 86400 # 1 day in seconds\n    min_date = None # example 2024-01-01 00:00:00\n    max_date = None # example 2024-01-06 00:00:00\n\n    # --- Load project --\n    sp = SurfProject.load_project(make_abs(project_file))\n\n    # --- Apply key filters ---\n    filters = {}\n    if net:\n        filters[\"net\"] = net\n    if station:\n        filters[\"station\"] = station\n    if channel:\n        filters[\"channel\"] = channel\n    if filters:\n        print(f\"[INFO] Filtering project by: {filters}\")\n        sp.filter_project_keys(**filters)\n\n\n    # --- Decide between time segment or split ---\n    info = sp.get_project_basic_info()\n    min_date_info = info[\"Start\"]\n    max_date_info = info[\"End\"]\n    dt1 = parse_datetime(min_date_info)\n    dt2 = parse_datetime(max_date_info)\n\n    diff = abs(dt2 - dt1)\n    if diff &lt; timedelta(days=1):\n        sp.get_data_files()\n        subprojects = [sp]\n\n    else:\n        print(f\"[INFO] Splitting into subprojects every {parsed_args.span_seconds} seconds\")\n        subprojects = sp.split_by_time_spans(\n            span_seconds=span_seconds,\n            min_date=min_date,\n            max_date=max_date,\n            file_selection_mode=\"overlap_threshold\",\n            verbose=True)\n\n    config_file = make_abs(config_file)\n    picking_file = make_abs(picking_file)\n    output_folder = make_abs(output_folder)\n\n\n    ct = CoincidenceTrigger(subprojects, config_file, picking_file, output_folder, plot=True)\n    ct.optimized_project_processing()\n</code></pre>"},{"location":"db/","title":"DataBase","text":""},{"location":"db/#populate","title":"Populate","text":"<ul> <li>Pick in File/Read Hyp Folder to incorporate all information contained inside hyp folders. Hyp folders are the file output from location.</li> <li>Pick in File/Magnitudes to populate your database with the information from the output file obtained in source toolbox.</li> <li>Pick in File/MTI to populate your database with the information from the output file obtained in MTI toolbox.</li> </ul>"},{"location":"db/#queryng","title":"Queryng","text":"<p>Choose the options on the left widget to filter the hypocenter inside your DataBase. The eartquakes shown in the table can be used in the MTI GUI when runs the inversion.</p>"},{"location":"db/#phase-information","title":"Phase Information","text":"<p>Click with right button to get the phases information corresponding to the selected event</p> <p></p>"},{"location":"db/#visual-options","title":"Visual Options","text":"<p>Double Clik near an epicenter in the map and it will be hilighted the corresponding row in the table.</p> <p>Double Click in a epicenter in the table and (if here is MTI information) it will be plot the beachball in the map</p> <p>Click with right button in a table row and select highlight event to visualize the exact event in the map.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#example-1-using-quick","title":"Example 1: Using quick","text":"<p>This example shows how to basically use  <code>surfquake quick</code> for waveform processing and visualization.</p>"},{"location":"examples/#bash-command-example","title":"Bash Command Example","text":"<pre><code># File paths can be absolute or relative\n\n# reading a simple file\nsurfquake quick -w \"/Users/admin/surf_test/carf220201000000.hhz\"\n\n# reading more than one file. It is a list of comma seprataed paths\nsurfquake quick -w \"/Users/admin/surf_test/carf220201000000.hhz,/Users/admin/surf_test/arbs220201000000.hhz\"\n\n# Using wildcards. In the example will read all files inside the folder\nsurfquake quick -w \"/Users/admin/surf_test/data/*\"\n</code></pre> <p>Let's work with two seismograms.</p> <p>After running the command  <code>surfquake quick -w ...</code> both seismograms are plot on screen. I zoomed in the main event and then I clicked with right button over the first seismogram to select the starttime and I draged to the right to select the endtime. After this operation, I pressed the keyboard <code>v</code> to activate the prompt in the terminal. Over the terminal, I proceed first to cut the seismograms and plotting the spectrum of the first seismogram. Then, I pressed <code>esc</code> to re-activate the prompt. Then, I plot both spectrums and finally I plot the spectrogram. Finally I returned to picking mode. This are the actions I took in the terminal propt:</p> <pre><code>&gt;&gt; cut\n&gt;&gt; sp 0\n&gt;&gt; spec all\n&gt;&gt; spec 0\n&gt;&gt; help spectrogram\n&gt;&gt; spec 0 3.0 95\n&gt;&gt; p\n</code></pre> <p></p> <p>If users want processing with no vilualization use the <code>-a</code> option and set and <code>-o</code> to set an output folder. This idea was designed to automatically process data. Be in mind that you can proceed using commands inside a bash script. Moreover might be useful to set your plotting configuration first <code>--plot_config</code>.</p>"},{"location":"examples/#example-2-using-a-post-script-with-surfquake","title":"Example 2: Using a Post-Script with SurfQuake","text":"<p>This example demonstrates how to use a custom Python post-processing script with <code>surfquake quick</code> for waveform processing and visualization.</p>"},{"location":"examples/#introduction","title":"Introduction","text":"<p>You can enhance your SurfQuake processing by injecting a custom Python script into the workflow using the <code>--post_script</code> option. This allows advanced users to modify waveform data, inject metadata, or apply event-specific logic dynamically.</p>"},{"location":"examples/#bash-command-example_1","title":"Bash Command Example","text":"<pre><code>surfquake quick \\\n    -w \"/Users/admin/Desktop/surf_test/nuclear/cut_event/*\" \\\n    --post_script /Users/admin/surf_test/my_script.py \\\n    -c /Users/admin/surf_test/config.yaml \\\n    -i /Users/admin/surf_test/metadata/metadata.xml\n</code></pre>"},{"location":"examples/#example-output","title":"Example Output","text":"<p>You can visualize the waveform data as output from this processing command. An example of what the output looks like is shown below:</p> <p>Ensure you have your plotting configuration (<code>--plot_config</code>) properly set to produce visual output. In this example is shown the default plotting config. Drag with the right mouse from left to right to set the starttime and endtime. Thsi enable cut wavefrom or plot spectrogram from plotting commands, pressing <code>v</code>.</p> <p></p>"},{"location":"examples/#the-post-processing-script","title":"The Post-Processing Script","text":"<p>This is the contents of the user-defined <code>my_script.py</code>:</p> <pre><code>import numpy as np\n\ndef run(stream, inventory, **kwargs):\n    \"\"\"\n    User-defined post-processing hook for each event.\n    :param stream: obspy Stream\n    :param inventory: obspy Inventory (passed via kwargs)\n    :param event: dict with keys like origin_time, latitude, longitude, depth (passed via kwargs)\n    \"\"\"\n\n    inventory = kwargs.pop('inventory', None)\n    event = kwargs.pop('event', None)\n\n    if event is not None:\n        print(f\"Post-processing {len(stream)} traces from event at {event['origin_time']}\")\n    else:\n        print(f\"Post-processing {len(stream)} traces (no event info provided)\")\n\n    # Example: print mean of all traces\n    all_data = np.hstack([tr.data for tr in stream])\n    print(\"Mean amplitude:\", np.mean(all_data))\n\n    for tr in stream:\n        tr.data = np.diff(tr.data)\n\n    return stream\n</code></pre>"},{"location":"examples/#explanation","title":"Explanation","text":"<p>This script will:</p> <ol> <li>Print metadata about the event (if available).</li> <li>Concatenate all trace data and print the mean amplitude.</li> <li>Apply a numerical difference (<code>np.diff</code>) to each trace in the stream.</li> </ol> <p>Such post-scripts are ideal for user-specific filtering, header modification, or transforming waveform data before further analysis or plotting.</p> <p>You can insert this script into any SurfQuake mode that accepts <code>--post_script</code>, including <code>quick</code>, <code>processing</code>, or daily batch modes.</p>"},{"location":"examples/#tips","title":"Tips","text":"<ul> <li>Always return the modified <code>stream</code>.</li> <li>You can access additional metadata via <code>kwargs</code> (like <code>event</code>, <code>inventory</code>, etc.).</li> <li>You can also modify trace headers using <code>tr.stats.&lt;attribute&gt;</code> if needed.</li> </ul> <p>SurfQuake\u2019s post-script interface allows flexible and programmable interaction with seismic datasets. Great for prototyping, automation, or embedding research logic!</p>"},{"location":"examples/#example-3-standard-events-processing","title":"Example 3: Standard Events Processing","text":"<p>This example demonstrates how to use <code>surfquake processing</code> to perform batch waveform analysis and visualization using event metadata, inventory files, and configuration settings.</p> <pre><code>surfquake processing \\\n    -p /Users/admin/surf_test/project.pkl \\\n    -i /Users/admin/surf_test/metadata/inv_all.xml \\\n    -e /Users/admin/surf_test/events/events.csv \\\n    --plot_config /Users/admin/surf_test/config/plotting_config.yaml \\\n    -ch HHZ \\\n    -c /Users/admin/surf_test/config/config.yaml\n</code></pre> <p>Also, we can cut waveforms around the origin time of the events set in <code>picks.csv</code>, usinf  <code>-r ref</code>. This is nice if we want to chop waveforms around specific time windows. Remember <code>picks.csv</code> have the same structure as <code>events.csv</code> and there is no need to set the columns lat, long...</p> <pre><code>surfquake processing \\\n    -p /Users/admin/surf_test/project.pkl \\\n    -c /Users/admin/surf_test/config.yaml \\\n    -e /Users/admin/surf_test/events/events.csv \\\n    -s CARF -ch HHZ -t 60 -r ref\n</code></pre> <p>Alternatively, we can cut waveforms according a specific wave speed. For example, very useful for cutting surface waves.</p> <pre><code>surfquake processing \\\n    -p /Users/admin/surf_test/project.pkl \\\n    -e /Users/admin/surf_test/events/picks.csv \\\n    --vel 4.5 \\\n    --cut_start_time 5 \\\n    --cut_end_time 25\n</code></pre>"},{"location":"examples/#explanation_1","title":"Explanation","text":"<ul> <li><code>-p</code>: Path to a previously saved project file (used for loading existing data).</li> <li><code>-i</code>: Inventory metadata file (StationXML or RESP).</li> <li><code>-e</code>: CSV file with event origin time and focal parameters, if -r ref option is inlcuded, only set otigin time is required.</li> <li><code>--plot_config</code>: YAML file defining how traces are plotted.</li> <li><code>-s</code>: Select only specific station (e.g., -ch HHN,HHZ). Comma separated list. Valid if a project loaded</li> <li><code>-ch</code>: Select only specific channels (e.g., -s ARNO). Comma separated list. Valid if a project loaded</li> <li><code>-c</code>: YAML config file specifying processing pipeline (filtering, tapering, etc.).</li> </ul> <p>This method cuts traces based on a specified group velocity (useful for surface wave analysis).</p> <ul> <li><code>--vel</code>: Surface wave velocity (km/s) used to calculate expected arrival time.</li> <li><code>--cut_start_time</code>: Seconds before the expected arrival.</li> <li><code>--cut_end_time</code>: Seconds after the expected arrival.</li> </ul>"},{"location":"examples/#sample-eventscsv-format","title":"Sample <code>events.csv</code> Format","text":"<pre><code>date;hour;latitude;longitude;depth;magnitude\n2022-02-01;02:02:58.9;42.5102;1.4284;20.9;3.66\n2022-02-02;23:35:29.7;42.5089;1.4293;20.7;1.71\n2022-02-03;12:01:21.6;42.3047;2.2741;0.0;1.65\n</code></pre>"},{"location":"examples/#sample-plot-configuration-plotting_configyaml","title":"Sample Plot Configuration (<code>plotting_config.yaml</code>)","text":"<pre><code>plotting:\n  traces_per_fig: 3 # default 6\n  sort_by: distance            # options: 'distance', 'backazimuth', or null\n  vspace: 0.05                 # vertical space between subplots\n  show_legend: true\n  title_fontsize: 9\n  plot_type: record         # 'standard', 'record' for record section and overlay for all traces at the same plot \n  sharey: False\n  show_arrivals: False\n  pick_output_file: ./picks.csv # picks will be written here\n  auto_load_pick_file: False   # if user wants that picking file is automatically loaded and plot\n  show_info_picks: False # show pick info on screen interactively\n  backend: TkAgg # TkAgg (maximum robustness, default), MacOSX (only for mac), Qt5Agg (fast, picking  &amp; interactive commands raise problems)\n</code></pre>"},{"location":"examples/#sample-processing-configuration-configyaml","title":"Sample Processing Configuration (<code>config.yaml</code>)","text":"<pre><code>Analysis:\n  process_1:\n    name: 'rmean'\n    method: 'linear'\n  process_2:\n    name: 'taper'\n    method: 'cosine'\n    max_percentage: 0.05\n  process_3:\n    name: 'filter'\n    type: 'bandpass'\n    fmin: 0.5\n    fmax: 5.5\n    corners: 4\n    zerophase: True\n</code></pre> <p>This processing pipeline: 1. Removes the mean. 2. Applies a cosine taper. 3. Applies a zero-phase bandpass filter (0.5\u20135.5 Hz).</p>"},{"location":"examples/#output","title":"Output","text":"<ul> <li>Figures are plotted interactively unless <code>automatic</code> is enabled: surfquake processing -a and it is set and <code>-o</code> to set an output folder.</li> <li>Processed traces can be exported in <code>.h5</code> format.</li> <li>Cuts are based on the first teoretical arrival time metadata (<code>events.csv</code>) or selected method (<code>--vel</code>, <code>-r</code>, etc.).</li> </ul> <p>plot_type: standard </p> <p>plot_type: record </p>"},{"location":"explore_data/","title":"Explore Data Avalability","text":"<p>I personally think that it really worht it explore your data avalability before any further analysis. It is very interesing to have a tool that can show you the data availability inside your folder tree. For this reason surfquake offers a command line tool that easily will show you a plot with lines where you have data, no data and gaps in red.</p> <p>For big data, it can last a little bit but it shows you the progress on screen. </p>"},{"location":"explore_data/#command-line","title":"Command Line","text":"<pre><code>surfquake explore -w [Path to your mseed files, e.g., './data/*']\n</code></pre>"},{"location":"explore_data/#example","title":"Example","text":"<p>In the example, I simply navigate to the forder where I have placed three stations with three channels so nine files, then I run the command:</p> <pre><code>surfquake explore -w \"./*\"\n</code></pre> <p></p>"},{"location":"focmec/","title":"Overview","text":"<p>FocMec is a core library that includes several tools. Each of this tools can be run from the CLI or yo can call them from your python script.</p> <ul> <li>Automatic first motion polarity determination</li> <li>Focal mecanism estimation</li> <li>Plotting tools</li> </ul> <p>References: <pre><code>    Chakraborty, M., Cartaya, C. Q., Li, W., Faber, J., R\u00fcmpker, G., Stoecker, H., &amp; Srivastava, N. (2022). \n    PolarCAP\u2013A deep learning approach for first motion polarity classification of earthquake waveforms. \n    Artificial Intelligence in Geosciences, 3, 46-52.\n\n    Snoke, J. A. (2003). \u201cFOCMEC: Focal Mechanism Determinations.\u201d \n    International Handbook of Earthquake and Engineering Seismology, Part B, Academic Press, pp. 1629\u20131630.\n    https://seiscode.iris.washington.edu/projects/focmec\n</code></pre></p>"},{"location":"focmec/#focmec-from-cli","title":"FOCMEC from CLI","text":"<ol> <li>Automatic P-wave first motion polarity determination. The inputs are the project and the picking file in nonlinloc pick format (i.e., might be generated using command pick). Output is an edited pick file with polarities.</li> <li>FOCal MEChanism determinations (FOCMEC) software  for determining and displaying double-couple earthquake focal mechanisms using the polarity and amplitude ratios of P and S waves.</li> <li>Plot fault planes, T &amp; P axis, and P-wave polarities.</li> </ol>"},{"location":"focmec/#usage","title":"Usage","text":"<pre><code>surfquake polarity\n# Determines Polarities from P-wave first motion\n\nKey Arguments:\n        -p, --project_file_path     [REQUIRED] Path to a surfquake project\n        -f, --picking_file_path     [REQUIRED] Path to a picking files\n        -o, --output_file_path      [REQUIRED] Path to the new picking file edited with polarities\n        -t, --thresh                [OPTIONAL] Threshold for Polarity declaration\n\nExample usage:\n        &gt; surfquake polarity -f ./nll_picks.txt -p ./project_file.pkl -o ./nll_picks_polarities.txt -t 0.95\n\noutput file \"nll_picks_polarities.txt\"\n\nStation_name    Instrument  Component   P_phase_onset   P_phase_descriptor  First_Motion    Date    Hour_min    Seconds Err ErrMag  Coda_duration   Amplitude   Period\nEADA   ?    HHZ  ? P      U 20150917 1512 31.9770 GAU 7.00e-02 -1.0 6.23e+07 -1.0\nETOB   ?    HHZ  ? P      U 20150917 1513 00.3900 GAU 2.00e-02 -1.0 1.64e+08 -1.0\nETOB   ?    HHZ  ? S      ? 20150917 1513 56.3200 GAU 5.00e-01 -1.0 8.22e+07 -1.0\nCEU    ?    HHZ  ? P      U 20150917 1512 08.3600 GAU 2.00e-02 -1.0 1.82e+08 -1.0\n</code></pre> <pre><code>surfquake focmec\n# Focal Mechanism from P-Wave first motion polarity\n\nKey Arguments:\n        -d, --hyp_folder        [REQUIRED] Path to folder containing hyp files\n        -o, --output_folder     [REQUIRED] Path to the output folder\n        -a, --accepted          [OPTIONAL] Number of accepted wrong polarities (float, default 1.0)\n\nExample usage:\n        &gt; surfquake focmec -h # Interactive help\n        &gt; surfquake focmec -d ./folder_hyp_path -a 1.0 -o ./output_folder\n</code></pre> <pre><code>surfquake plotmec\n# Plot fault planes, T &amp; P axis, and P-wave polarities\n\nKey Arguments:\n        -f, --focmec_file           [OPTIONAL] Path to a specific *.lst file\n        -d, --focmec_folder_path    [OPTIONAL] Path to folder with all *.lst files (focmec output)\n        -o, --output_folder         [REQUIRED] Path to the output folder\n        -a, --all_solutions         [OPTIONAL] If set, all searching fault planes will be plot\n        -p, --plot_polarities       [OPTIONAL] If plot P-Wave polarities on the beachball\n        -m, --format                [OPTIONAL] Format output plot (defaults pdf)\n\nExample usage:\n        &gt; surfquake plotmec -h # Interactive help\n        &gt; surfquake plotmec -d ./focmec_folder_path -o ./output_folder\n        &gt; surfquake plotmec -f ./focmec_file_path.lst -o ./output_folder -p -a -m pdf \n</code></pre> <p>Output plot example</p> <p></p>"},{"location":"focmec/#focmec-from-library","title":"Focmec from library","text":""},{"location":"focmec/#classes-methods","title":"Classes &amp; Methods","text":"<p>In this section, we will explain the class RunPolarity and we will explain how to manage your project from a simple example:</p> <p><code>RunPolarity</code></p> <pre><code>class RunPolarity:\n\n    def __init__(self, project, pick_file:str, output_path:str, threshold=0.9):\n        \"\"\"\n        Attributes:\n        - project (SurfProject): SurfProject object\n        - pick_file (str): path to picking file\n        - output_path (str): path to output_path\n        Methods\n        - send_polarities(self): Run automatic determination\n        \"\"\"\n</code></pre> <pre><code>class FirstPolarity:\n\n    def __init__(self):\n        \"\"\"\n        Manage FOCMEC files for run nll program.\n\n        Important: The  obs_file_path is provide by the class :class:`PickerManager`.\n\n        Methods:\n\n        find_hyp_files: \n        return list of hyp_files\n\n        set_head(hyp_file_path): staticmethod, helps to set a string in comments of focmec input, check that input file is valid. \n        return header\n\n        create_input(self, hyp_file_path, header): creates the focmec input from a hyp_file. return path to the input file\n\n        check_no_empty(input_file): staticmethod, check that the input file has been created correctly.\n\n        run_focmec(self, input_focmec_path, num_wrong_polatities, new_output_path=None): Run focal mechanism estimation and write the \n\n        output in folder new_output_path\n        \"\"\"\n</code></pre> <pre><code>class FirstPolarity:\n\n    def __init__(self):\n        \"\"\"\n        Manage FOCMEC files for run nll program.\n\n        Important: The  obs_file_path is provide by the class :class:`PickerManager`.\n\n        Methods:\n\n        find_hyp_files: \n        return list of hyp_files\n\n        set_head(hyp_file_path): staticmethod, helps to set a string in comments of focmec input, check that input file is valid. \n        return header\n\n        create_input(self, hyp_file_path, header): creates the focmec input from a hyp_file. return path to the input file\n\n        check_no_empty(input_file): staticmethod, check that the input file has been created correctly.\n\n        run_focmec(self, input_focmec_path, num_wrong_polatities, new_output_path=None): Run focal mechanism estimation and write the \n\n        output in folder new_output_path\n        \"\"\"\n</code></pre>"},{"location":"focmec/#examples-scripts","title":"Examples Scripts","text":"<p>Next, the example of using this class and its methods.</p>"},{"location":"focmec/#script-for-automatic-polarities-determination","title":"Script for automatic polarities determination","text":"<pre><code>from multiprocessing import freeze_support\nfrom surfquakecore.project.surf_project import SurfProject\nfrom surfquakecore.first_polarity.get_pol import RunPolarity\n\nproject_file = \"/Volumes/LaCie/test_surfquake_core/testing_data/projects/surfquake_project_new.pkl\"\npicking_file = \"/Volumes/LaCie/test_surfquake_core/testing_data/picking_file.txt\"\noutput_file = \"/Volumes/LaCie/test_surfquake_core/testing_data/picking_file_polarities.txt\"\nthreshold = 1.0\n\nif __name__ == '__main__':\n\n    project = SurfProject.load_project(project_file)\n    RunPolarity(project, picking_file, output_file, threshold).send_polarities()\n</code></pre>"},{"location":"focmec/#script-for-running-focmec","title":"Script for running focmec","text":"<pre><code>from multiprocessing import freeze_support\nfrom surfquakecore.first_polarity.first_polarity import FirstPolarity\n\nhyp_folder = \"/Volumes/LaCie/test_surfquake_core/testing_data/loc\"\noutput_folder = \"/Volumes/LaCie/test_surfquake_core/testing_data/output_focmec\"\naccepted_polarities = 1.0\n\nif __name__ == '__main__':\n\n    files_list = FirstPolarity.find_hyp_files(hyp_folder)\n    for file in files_list:\n        try:\n            header = FirstPolarity.set_head(file)\n            if file is not None:\n                file_input = FirstPolarity().create_input(file, header)\n\n                if FirstPolarity.check_no_empty(file_input):\n                    FirstPolarity().run_focmec(file_input, accepted_polarities, output_folder)\n        except Exception as e:\n            print(f\"Error processing file {file}: {e}\")\n            traceback.print_exc()\n</code></pre>"},{"location":"focmec/#script-for-plotting-focal-mechanisms","title":"Script for plotting focal mechanisms","text":"<pre><code>focmec_file = \"/Volumes/LaCie/test_surfquake_core/testing_data/output_focmec\" # extension of focmec output is *.lst\noutput_folder = \"Volumes/LaCie/test_surfquake_core/testing_data/beachballs\"\n\nif __name__ == '__main__':\n\n    firstpolarity_manager = FirstPolarity()\n\n    focmec_files = firstpolarity_manager.find_files(focmec_folder_path)\n\n    for file in focmec_files:\n\n        Station, Az, Dip, Motion = FirstPolarity.extract_station_data(file)\n        cat, focal_mechanism = firstpolarity_manager.extract_focmec_info(file)\n        file_output_name = FirstPolarity.extract_name(file)\n\n        if output_folder:\n            name_str = os.path.basename(file)[:-3] + format\n            output_folder_file = os.path.join(output_folder, name_str)\n        else:\n            output_folder_file = None\n\n        Plane_A = focal_mechanism.nodal_planes.nodal_plane_1\n        strike_A = Plane_A.strike\n        dip_A = Plane_A.dip\n        rake_A = Plane_A.rake\n        extra_info = firstpolarity_manager.parse_solution_block(focal_mechanism.comments[0][\"text\"])\n        P_Trend = extra_info['P,T']['Trend']\n        P_Plunge = extra_info['P,T']['Plunge']\n        T_Trend = extra_info['P,N']['Trend']\n        T_Plunge = extra_info['P,N']['Plunge']\n\n        misfit_first_polarity = focal_mechanism.misfit\n        azimuthal_gap = focal_mechanism.azimuthal_gap\n        number_of_polarities = focal_mechanism.station_polarity_count\n        if all_solutions:\n            solution_collection = cat[0][\"focal_mechanisms\"]\n        else:\n            solution_collection = None\n\n\n        #\n        first_polarity_results = {\"First_Polarity\": [\"Strike\", \"Dip\", \"Rake\", \"misfit_first_polarity\", \"azimuthal_gap\",\n                                                     \"number_of_polarities\", \"P_axis_Trend\", \"P_axis_Plunge\",\n                                                     \"T_axis_Trend\", \"T_axis_Plunge\"],\n                                  \"results\": [strike_A, dip_A, rake_A, misfit_first_polarity, azimuthal_gap,\n                                              number_of_polarities, P_Trend, P_Plunge, T_Trend, T_Plunge]}\n\n        FirstPolarity.print_first_polarity_info(file_output_name, first_polarity_results)\n        FirstPolarity.drawFocMec(strike_A, dip_A, rake_A, Station, Az, Dip, Motion, P_Trend, P_Plunge,\n            T_Trend, T_Plunge, output_folder_file, plot_polarities=True,\n                                 solution_collection=solution_collection)\n</code></pre>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#processor-and-os","title":"Processor and OS","text":"<p>Linux OS and Mac computers with Intel processors can now run surQuake thanks to testing and programming. </p> <p>Additionally, Rosetta allows Mac machines with Apple chips to run applications meant for Mac systems with Intel processors. If this is your case, please install Rosseta before proceed with surfQuake intallation</p> <p>The user can select whether to install only the surfQuake core or the GUI (which also includes the core). </p> <ul> <li> <p>Automatic installation: This is only available in surfQuake GUI and requires a previous installation of Anaconda.</p> </li> <li> <p>Manual Installation: In both circumstances (GUI &amp; Core), SurfQuake must be manually installed from the pip repository after creating an environment (anaconda env or pip env).</p> </li> </ul>"},{"location":"install/#installing-the-gui","title":"Installing the GUI","text":""},{"location":"install/#automatic-installation","title":"Automatic Installation:","text":"<p>This process will create atomatically an anaconda environment and then will install surfquake.</p> <p>Open a terminal and type:</p> <pre><code>&gt;&gt; (base) git clone https://github.com/rcabdia/SurfQuake.git\n&gt;&gt; (base) cd SurfQuake/install\n&gt;&gt; (base) chmod u+x surfquake_installer.sh\n&gt;&gt; (base) ./surfquake_installer.sh\n</code></pre> <p>After this process, if everything is ok and the alias has been correctly included in your path system, just type in your terminal</p> <p><pre><code>&gt;&gt; surfquake\n</code></pre> if you have problems because surfquake is not properly recognized by your system path, go to the root path where you have saved surfquake,</p> <pre><code>&gt;&gt; (base) cd SurfQuake\n&gt;&gt; (base) conda activate surfquake\n&gt;&gt; (surfquake) python start_surfquake.py\n</code></pre> <p>Remenber that installing surfquake GUI also includes the core Library and the Command Line interface. That's means that now you have access to the commands <pre><code>&gt;&gt; (surfquake) surfquake -h\n</code></pre></p> <p>and you can make python scripts calling classes and methods from surfquake core.</p>"},{"location":"install/#manual-installation","title":"Manual Installation:","text":""},{"location":"install/#create-an-environment","title":"Create an environment","text":""},{"location":"install/#anaconda","title":"Anaconda","text":"<pre><code>&gt;&gt; (base) conda create -n surfquake python=3.9\n&gt;&gt; (base) conda activate surfquake\n</code></pre>"},{"location":"install/#pip","title":"pip","text":"<p>Warning, if you are using anaconda, normally is automatically activated the base environment in your terminal. So first of all deactivate it!</p> <pre><code>&gt;&gt; (base) conda deactivate\n</code></pre> <pre><code>&gt;&gt; pip install virtualenv\n&gt;&gt; python&lt;version&gt; -m venv surfquake\n&gt;&gt; source surfquake/bin/activate # activate your environment\n</code></pre>"},{"location":"install/#execute-the-installation","title":"Execute the Installation","text":"<p>Now, you can proceed with the installation of the full surfQuake program:</p> <pre><code>&gt;&gt; (surfquake) cd SurfQuake\n&gt;&gt; (surfquake) pip install -r requirements.txt\n</code></pre> <p>to start the program</p> <pre><code>&gt;&gt; (surfquake) cd SurfQuake\n&gt;&gt; (surfquake) python start_surfquake.py\n</code></pre>"},{"location":"install/#installing-the-core","title":"Installing the core","text":""},{"location":"install/#create-an-environment_1","title":"Create an environment","text":""},{"location":"install/#anaconda-env","title":"Anaconda env","text":"<pre><code>&gt;&gt; conda create -n surfquake python=3.9 # surfQuake for 3.9 &lt;= Python &lt;= 3.11\n&gt;&gt; conda activate surfquake\n</code></pre>"},{"location":"install/#pip-env","title":"pip env","text":"<p>Warning, if you are using anaconda, normally is automatically activated the base environment in your terminal. So first of all deactivate it! </p> <pre><code>&gt;&gt; (base) conda deactivate\n</code></pre> <p>To prevent dependencies from becoming incompatible, please make sure that the version of Python you have installed on your system is greater than 3.9 and less than 3.12. Simply follow these steps to view your Python version:</p> <pre><code>&gt;&gt; python\nPython 3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information\n</code></pre> <pre><code>&gt;&gt; pip install virtualenv # install in your python the virtualenv package (just in case)\n&gt;&gt; python3 -m venv ./surfquake # python&lt;version&gt; -m &lt;virtual-environment-name&gt; &lt;venv_location&gt; \n&gt;&gt; source surfquake/bin/activate #s ource &lt;venv_location&gt;\n</code></pre>"},{"location":"install/#install-the-core-package","title":"Install the core package","text":"<p>You can install surfQuake core after creating your virtual environment (using either Anaconda env or Pip env). After activate your environment,</p> <pre><code>&gt;&gt; (surfquake) pip install surfquake\n</code></pre>"},{"location":"locate/","title":"Event Location","text":"<p>The Event Location toolbox uses Non Lin Loc, Lomax et ., 2009 to locate seismic events.</p>"},{"location":"locate/#previous-knowledge","title":"Previous knowledge","text":"<p>First we include here, previous knowlege about files you need to set to run event locations.</p>"},{"location":"locate/#the-picking-file","title":"The Picking File","text":"<p>The picking file NonLinLoc Phase file format nll_input.txt is automatically generated when the associator. However, you can make it by your own. Here, it's attached how is the file describing one event. For more events just you need yo write the picking info separated by a blank row. The location algorythm will locate all events with picking information inside the file</p> <pre><code>Station_name    Instrument  Component   P_phase_onset   P_phase_descriptor  First_Motion    Date    Hourmin Seconds GAU Err Coda_duration   Amplitude   Period\nARBS    ?   ?   ?   P   ?   20220126    0200    14.700  GAU 2.50E-03    -1.00E+00   85600000.0  -1.00E+00\nARBS    ?   ?   ?   S   ?   20220126    0200    16.320  GAU 7.50E-03    -1.00E+00   85600000.0  -1.00E+00\nCORG    ?   ?   ?   P   ?   20220126    0200    18.210  GAU 2.50E-03    -1.00E+00   17000000.0  -1.00E+00\nCORG    ?   ?   ?   S   ?   20220126    0200    23.020  GAU 7.50E-03    -1.00E+00   17000000.0  -1.00E+00\nPAND    ?   ?   ?   P   ?   20220126    0200    14.200  GAU 2.50E-03    -1.00E+00   132000000000.0  -1.00E+00\nPAND    ?   ?   ?   S   ?   20220126    0200    15.550  GAU 2.50E-03    -1.00E+00   132000000000.0  -1.00E+00\nCSOR    ?   ?   ?   P   ?   20220126    0200    17.510  GAU 2.50E-03    -1.00E+00   45200000.0  -1.00E+00\nCSOR    ?   ?   ?   S   ?   20220126    0200    21.330  GAU 5.00E-03    -1.00E+00   99900000.0  -1.00E+00\nCEST    ?   ?   ?   P   ?   20220126    0200    15.570  GAU 2.50E-03    -1.00E+00   74500000.0  -1.00E+00\nCEST    ?   ?   ?   S   ?   20220126    0200    17.660  GAU 2.50E-03    -1.00E+00   90000000.0  -1.00E+00\nSALF    ?   ?   ?   P   ?   20220126    0200    18.080  GAU 5.00E-03    -1.00E+00   7240000000.0    -1.00E+00\nSALF    ?   ?   ?   S   ?   20220126    0200    21.930  GAU 7.50E-03    -1.00E+00   24900000000.0   -1.00E+00\nGENF    ?   ?   ?   P   ?   20220126    0200    18.470  GAU 2.50E-03    -1.00E+00   20100000.0  -1.00E+00\nGENF    ?   ?   ?   S   ?   20220126    0200    22.730  GAU 5.00E-03    -1.00E+00   43900000.0  -1.00E+00\nCARF    ?   ?   ?   P   ?   20220126    0200    22.230  GAU 2.50E-03    -1.00E+00   1750000000.0    -1.00E+00\nCARF    ?   ?   ?   S   ?   20220126    0200    29.370  GAU 7.50E-03    -1.00E+00   2870000000.0    -1.00E+00\n</code></pre>"},{"location":"locate/#earth-model-settings","title":"Earth Model Settings","text":"<p>We have created a repository with an example of 1D and 3D model that describes the SW Iberian Peninsula using a Grandin et ., 2007 to rapidly visualize the model Cabieces et al 2020 </p> <p>The model 1D: In the root path to your model, in this case at /Volumes/LaCie/surfquake_test/test_nll_final/model1D create two files called modelP and modelS. The files will be exactly the same but called differnlt. The are easily described as follows:</p> <p>LAYER depth VpTop VpGrad VsTop VsGrad rhoTop rhoGrad</p> <ul> <li>depth (float) depth to top of layer (use negative values for layers above z=0)</li> <li>VpTop VsTop rhoTop P velocity, and S velocity in km/s and density in kg/m**3 at the top of the layer.</li> <li>VpGrad VsGrad rhoGrad Linear P velocity and S velocity gradients in km/s/km and density gradient in kg/m**3/km increasing directly downwards from the top of the layer.</li> </ul> <p>Summarizing for the hungry users. Copy and paste this lines into your files called modelP and modelS and place it into the root folder previouly set as PATH_TO_1D_MODEL in the config file or for GUI users in the root folder you select as earth model.</p> <pre><code>LAYER   0.0  6.1 0.0    3.49  0.0  2.7 0.0\nLAYER  11.0  6.4 0.0    3.66  0.0  2.7 0.0\nLAYER  24.0  6.9 0.0    3.94  0.0  2.7 0.0\nLAYER  31.0  8.0 0.0    4.57  0.0  2.7 0.0\nLAYER    45.63   8.0412  0.000118   4.4737  0.000353\nLAYER    56.25   8.0425  0.000118   4.4775  0.000353\nLAYER    66.88   8.0437  0.000118   4.4813  0.000353\nLAYER    77.50   8.0450  0.000118   4.4850  0.000353\nLAYER    88.13   8.0463  0.000118   4.4887  0.000353\nLAYER    98.75   8.0475  0.000118   4.4925  0.000353\nLAYER   109.38   8.0488  0.000120   4.4962  0.000353\nLAYER   120.00   8.0500  0.002775   4.5000  0.000200\n</code></pre> <p>The model 3D:</p> <p>Every depth layer must be placed in files called, for example</p> <pre>\n    <code>\nFor the P wave --&gt; layer.P.mod5.mod\nFor the S wave --&gt; layer.S.mod5.mod\n    </code>\n</pre> <p>Which means that inside this file there is the grid with the value of the Vp or Vs, for the layer at depth 5km.</p> <p>The layer must be a matrix with the values in the rows from top to bottom S -&gt; N, and from left to right E -&gt; W. That's mean following the next example that the file corresponding to a depth layer 5km \"layer.P.mod5.mod\" could be like this:</p> <pre>\n    <code>\n4.5759 4.5735 ...... 4.5707 4.5677\n4.5760 4.5755 ...... 4.5766 4.5670\n...... ...... ...... ...... ......\n4.6800 4.6500 ...... 4.6730 4.5678\n    </code>\n</pre> <p>This matrix means that, for example corresponds to geographic points long,lat (separated in cells of, dx dy of 0.5 x 0.5 degrees.</p> <pre>\n    <code>\n(-10.0,34.0) (-9.5,34.0) ...... (-9.0,34.0) (-8.5,34.0)\n(-10.0,34.5) (-9.5,34.5) ...... (-9.0,34.5) (-8.5,34.5)\n....... ..... ..... .... ...... .... ...... ...........\n(-10.0,40.0) (-9.5,40.0) ...... (-9.0,40.0) (-8.5,40.0)\n    </code>\n</pre> <p>Warning grid cells must be [dx = dy = dz] for a correct interpretation.</p>"},{"location":"locate/#event-location-gui","title":"Event Location GUI","text":"<p>This is a screenshot of the Event Location GUI.</p> <p></p> <ul> <li>Location Parameters: Just set the paths to the workflow.<ul> <li>Work / Ouput Directory: Set the root path where the necessary working directories structure will be created. In this structure will be build the velocity grid, travel-times tables and the location output.</li> <li>Model Folder Path: Set the path to your 1D or 3D model folder. Go to CLI section to see further details of how to configurate your Earth Model.</li> <li>Pick File: Set the path to the pick file. Picking file nll_input.txt output from associator toolbox contains all phase info o run the event locations.</li> </ul> </li> <li>Grid Configuration:<ul> <li>Grid Reference: SW corner of your geographic framework</li> <li>Grid Dimension in the x, y and z number of points and dx, dy dx the size in each dimension. For example the size in the X/East axis, X = dx*(x-1)</li> <li>Geographic Transformation: Simple or Global. Warning if Global is selected go directly to press Run Location </li> <li>Grid Type: Slowness (Default)</li> <li>Wave: P &amp; S or P. This will guide the software to know wich can of velocity grid create.</li> <li>Model: 1D or 3D</li> </ul> </li> <li>Travel Times:<ul> <li>Select type of grid GRID1D or GRID3D and the corresponding wave P &amp; S or S</li> <li>Distance Limit: The maximum distance from the center of the grid  - station,  to compute the travel-time</li> </ul> </li> <li>Location parameters:<ul> <li>search: algorythm to be used in the search of the location solution</li> <li>Method</li> </ul> </li> </ul>"},{"location":"locate/#grid-definition","title":"Grid Definition","text":"<p>One the most important parts is the geographic framework settings. In the figure is displayed an example of stations distrubition (orange triangles and blue squares) inside the region of study(shaded square) with probable epicenters (red circle). Simply, the user must set the grid coordinates origin, the resolution dx, dy and dz, plus the number of points in the grid X, Y, Z. Remind that the configuration follow the left hand rule so positive W-&gt;E, S-&gt;N, Top -&gt; Bottom in depth.</p>"},{"location":"locate/#config-file","title":"Config File","text":"<p>In the next section CLI and Library the user can configurate the event location tool from a config file type file.ini, an example of the typical configuration for locate local/regional events using a 1D-model as follows:</p> <pre><code>[GRID_CONFIGURATION]\nLATITUDE = 41.0000\nLONGITUDE = 0.0000\nDEPTH_KM = -3.0\nX = 400\nY = 400\nZ = 50\nDX = 1\nDY = 1\nDZ = 1\nGEO_TRANSFORMATION = SIMPLE\nGRID_TYPE = SLOW_LEN\nPATH_TO_PICKS = /Volumes/LaCie/surfquake_test/test_real_final/nll_input.txt\nPATH_TO_1D_MODEL = /Volumes/LaCie/surfquake_test/test_nll_final/model1D\nPATH_TO_3D_MODEL = NONE\nMODEL = 1D\nP_WAVE_TYPE = TRUE\nS_WAVE_TYPE = TRUE\n#\n[TRAVEL_TIMES_CONFIGURATION]\nDISTANCE_LIMIT = 500\nGRID = 1D\n\n#\n[LOCATION_PARAMETERS]\nSEARCH = OCT-TREE\nMETHOD = GAU_ANALYTIC\n</code></pre> <p>In the section Location_Parameters SEARCH is just available OCT that corresponds to the Octree algorythm.  In Method you can change to:</p> <ul> <li>GAU_ANALYTIC the inversion approach of Tarantola and Valette (1982) with L2-RMS likelihood function. </li> <li>EDT Equal Differential Time likelihood function cast into the inversion approach of Tarantola and Valette (1982) <ul> <li>EDT_OT_WT Weights EDT-sum probabilities by the variance of origin-time estimates over all pairs of readings. This reduces the probability (PDF values) at points with inconsistent OT estimates, and leads to more compact location PDF's. </li> <li>EDT_OT_WT_ML version of EDT_OT_WT with EDT origin-time weighting applied using a grid-search, maximum-likelihood estimate of the origin time. Less efficient than EDT_OT_WT which uses simple statistical estimate of the origin time.</li> </ul> </li> </ul>"},{"location":"locate/#event-location-from-cli","title":"Event Location from CLI","text":""},{"location":"locate/#usage","title":"Usage","text":"<pre><code>&gt;&gt; surfquake locate seismic event [-h] -i INVENTORY_FILE_PATH -c CONFIG_FILE_PATH -o OUT_DIR_PATH [-g] [-s]\n</code></pre>"},{"location":"locate/#interactive-help","title":"Interactive help","text":"<pre><code>&gt;&gt; surfquake locate -h\n</code></pre>"},{"location":"locate/#run-event-location-from-cli","title":"Run Event Location from CLI","text":"<pre><code>&gt;&gt; surfquake locate -i /surfquake_test/metadata/inv_all.xml -c /surfquake_test/config_files/nll_config.ini -o /surfquake_test/test_nll_final -g -s\n</code></pre>"},{"location":"locate/#event-location-from-library","title":"Event Location from Library","text":""},{"location":"locate/#classes","title":"Classes","text":"<p><code>NllManager</code></p> <pre><code>class NllManager:\n\n    def __init__(self, nll_config: Union[str, NLLConfig], metadata_path, working_directory):\n        \"\"\"\n        Manage NonLinLoc program to locate seismic events.\n        :param nll_config: Path to nll_config.ini file or to NLLConfig object.\n        :param metadata_path: Path to metadata file.\n        :param working_dirctory: Root path to folder to establish the working and output structure.\n        \"\"\"\n        self.__get_nll_config(nll_config)\n        self.__location_output = working_directory\n        self.__create_dirs()\n        self.__dataless_dir = metadata_path\n        self.__metadata_manager = None\n</code></pre>"},{"location":"locate/#methods","title":"Methods","text":"<p><code>vel_to_grid</code> <pre><code># instance method\ndef vel_to_grid(self):\n    \"\"\"\n    # Method to generate the velocity grid #\n    :return: Extracts the velocity grid as layer*.buf and layer*.hdr inside working_dir/model\n    template file temp.txt in working_dir/temp.txt\n    \"\"\"\n</code></pre></p> <p><code>grid_to_time</code> <pre><code># instance method\ndef grid_to_time(self):\n    \"\"\"\n    # Method to generate the travel-time tables file #\n    :return: Extracts the travel-times per wave type as\n    [layer.P.STA.angle.buf, layer.P.STA.time.buf, layer.P.STA.time.hdr]\n    inside ./working_dir/time\n    template file at ./work_dir/temp/G2T_temp.txt\n    \"\"\"\n</code></pre></p> <p><code>run_nlloc</code> <pre><code># instance method\ndef run_nlloc(self):\n    \"\"\"\n    # Method to run the event locations from the picking file and config_file.ini #\n    :return: locations files *hyp inside ./working_dir/loc\n    template file at ./work_dir/temp/run_temp.txt\n    \"\"\"\n</code></pre></p>"},{"location":"locate/#example-using-library","title":"Example using library","text":"<pre><code>import os\nfrom surfquakecore.earthquake_location.run_nll import Nllcatalog, NllManager\n\nif __name__ == \"__main__\":\n    cwd = os.path.dirname(__file__)\n    # Basic input: working_directory, inventory file path and config_file input\n    working_directory = os.path.join(cwd, \"earthquake_locate\")\n    inventory_path = os.path.join(working_directory, \"inventories\", \"inv_surfquakecore.xml\")\n    path_to_configfiles = os.path.join(working_directory, \"config/nll_config.ini\")\n    nll_manager = NllManager(path_to_configfiles, inventory_path, working_directory)\n    nll_manager.vel_to_grid()\n    nll_manager.grid_to_time()\n    for iter in range(0, 10):\n        print('Locating ', iter)\n        nll_manager.run_nlloc()\n    nll_catalog = Nllcatalog(working_directory)\n    nll_catalog.run_catalog(working_directory)\n</code></pre>"},{"location":"mti/","title":"Automatic Moment Tensor Inversion","text":"<p>The focal mechanism of the pre-located events is automatically estimated using a optimized version of Bayesian Isola (Vack\u00e2r et al., 2017). The MTI is run over a set of events. The events can be selected quering the database through the GUI, giving a folder path with mti_config.ini files or even making instances of the class for each event and running the inversion from the core library. Moreover, for Core library and CLI users, it can be automatically generated the MTI config files from a catalog object, see Build MTI config files in Utilities section.</p>"},{"location":"mti/#mti-gui","title":"MTI GUI","text":"<ul> <li> <p>Working Framework:</p> <ul> <li> <p>Working Dirctory (no required): Folder where Green functions a temporal files will be saved.</p> </li> <li> <p>Output Directory: Rooth path to the folder where output from event inversions will be saved.</p> </li> <li> <p>Earth Model File: File path to he earth model. An example as follows:</p> </li> </ul> </li> </ul> <pre><code>Crustal model                  IBERIA\nnumber of layers \n   7\nParameters of the layers\ndepth of layer top(km)   Vp(km/s)    Vs(km/s)    Rho(g/cm**3)    Qp     Qs\n      0.0                 6.10       3.490        2.920         300    300\n     11.0                 6.40       3.660        2.980         300    300\n     24.0                 6.90       3.940        3.080         300    300\n     31.0                 8.00       4.570        3.300         300    300\n     45.6                 8.04       4.474        3.308         300    300\n     56.2                 8.04       4.478        3.309         300    300\n     66.9                 8.04       4.481        3.309         300    300\n*************************************************************************\n</code></pre> <ul> <li> <p>Grid search: Defines a geographic grid centered in the estimated hypocenter where surfquake will proceed with the search of the best MTI.</p> <ul> <li> <p>Horizontal Location Uncertainity: Maximum horizontal range of the search.</p> </li> <li> <p>Horizontal search step: Horizontal resolution of the grid search.</p> </li> <li> <p>Depth Uncertainty: Maximum vertical range of the search.</p> </li> <li> <p>Depth Search step: Vertical resolution of the grid search.</p> </li> <li> <p>Time Uncertainity: Time shift around the event origin time.</p> </li> </ul> </li> <li> <p>Inversion Parameters: Defines parameters realtd to the source and the inversion process.</p> </li> <li>Traces Selection Criteria: Defin the criteria to include or not seismograms to your inversion</li> </ul> <p>Now, that you have parametrized your inversion follow the steps:</p> <ol> <li>Inside Project: Load your project and load your Metadata.</li> <li>Inside The DataBase dedicated GUI.<ul> <li>Fill your DataBase loading that files from the folder where you have the location files.</li> <li>Not required but very recommended populate your DataBase with the information from the output of Source. This will give the database information about Magnitudes and will facilitate the MTI.</li> </ul> </li> <li>Make a Query to filter your Database</li> <li>Without closing DataBase GUI, press Run Inversion.</li> <li>Track the evolution of your inversions in the output dirctory, finally press print Results.</li> <li>Populate your dataBase with the information from the output of MTIs.</li> </ol>"},{"location":"mti/#mti-config-file","title":"MTI Config File","text":"<p>In the following sections CLI and Core library the user can use mti_config.ini files to define each event where surfquake will carry out the MTIs. You can storage a single event per file. So, place all files inside the same folder. Please find here an example of mti_config.ini file (the name of the .ini file doen't matter just extension .ini). Additionally, be in mind that users can create automatically mti config files. Go to section  Build MTI config files using the Library inside Utils. You can use the following mti config file information to make your own template.</p> <pre><code>[ORIGIN]\nORIGIN_DATE = 21/08/2018 00:28:57.000\nLATITUDE = 42.7059\nLONGITUDE= -7.6974\nDEPTH_KM = 11.0\nMAGNITUDE = 3.5\n\n[STATIONS_AND_CHANNELS]\n# add the station name follow by channels split by a comma , and use .+ for all channels\nELOB = HHZ, HHN, HHE \nEPON = .+\nEMAZ = .+\n\n[MTI_PARAMETERS]\nEARTH_MODEL_FILE = /earth_models/Iberia.dat\nLOCATION_UNC = 3000\nTIME_UNC = 0.5\nDEVIATORIC = True\nDEPTH_UNC = 3000\nCOVARIANCE = True\nRUPTURE_VELOCITY = 2500\nSOURCE_TYPE = Triangle\nMIN_DIST = 50\nMAX_DIST = 500\nMAX_NUMBER_STATIONS = 15\nSOURCE_DURATION = 2\n\n[SIGNAL_PROCESSING]\nREMOVE_RESPONSE = True\nMAX_FREQ = 0.08\nMIN_FREQ = 0.04\nRMS_THRESH = 5.0\n</code></pre> <p>Units: LOCATION_UNC [m], TIME_UNC [s], DEPTH_UNC[m], RUPTURE_VELOCITY [m/s], MIN_DIST [km], MAX_DIST [km], SOURCE_DURATION [s],  RMS_THRESH [avarage ratio signal / noise]. If MIN_DIST and MAX_DIST automatically will be used MIN_DIST = 2 * rupture_length, MAX_DIST = 2 ** (mti_config.magnitude) * 2</p>"},{"location":"mti/#mti-from-cli","title":"MTI from CLI","text":""},{"location":"mti/#usage","title":"Usage","text":"<pre><code>surfquake mti -i [inventory_file_path] -p [path_to_project] -c [path to mti_config_file.ini] \n        -o [output_path]  -s [if save plots]\n</code></pre>"},{"location":"mti/#interactive-help","title":"Interactive help","text":"<pre><code>&gt;&gt; surfquake mti -h\n</code></pre>"},{"location":"mti/#run-mti-from-cli","title":"Run MTI from CLI","text":"<pre><code>&gt;&gt; surfquake mti -i /mti_run_inversion_resources/inv_surfquakecore.xml -p /project/surfquake_project_mti.pkl -o /test_mti -c /surfquake_test/mti_configs -s\n</code></pre>"},{"location":"mti/#mti-from-library","title":"MTI from Library","text":""},{"location":"mti/#classes","title":"Classes","text":"<p><code>BayesianIsolaCore</code> <pre><code>class BayesianIsolaCore:\n    def __init__(self, project: SurfProject, inventory_file: str,\n                 output_directory: str, save_plots=False):\n        \"\"\"\n\n        :param project: SurfProject object\n        :param inventory_file: File to the metadata file\n        :param output_directory: Root path to the output directory where inversion results will be saved\n        :param save_plots: if figures summarizing the results for each inversion are desired\n        \"\"\"\n</code></pre></p>"},{"location":"mti/#methods","title":"Methods","text":"<p><code>run_inversion</code> <pre><code># instance method\ndef run_inversion(self, mti_config: Union[str, MomentTensorInversionConfig], **kwargs):\n\n    \"\"\"\n    This method should be to loop over config files and run the inversion.\n    Previously it is needed to load the project and metadata.\n\n    Args:\n        mti_config: Either a directory of .ini files, a .ini file or an instance of MomentTensorInversionConfig\n        **kwargs:\n\n    Returns:\n\n    \"\"\"\n</code></pre></p> <p><code>read_isola_result</code> <pre><code>def read_isola_result(file: str) -&gt; MomentTensorResult:\n    \"\"\"\n    Reads the ISOLA-ObsPy output inversion.json file.\n\n    :param file: The location of inversion.json from isola.\n    :return: Dict\n    \"\"\"\n</code></pre></p>"},{"location":"mti/#example-using-library","title":"Example using library","text":"<pre><code>import os\nfrom surfquakecore.moment_tensor.mti_parse import read_isola_log, read_isola_result\nfrom surfquakecore.moment_tensor.sq_isola_tools.sq_bayesian_isola import BayesianIsolaCore\nfrom surfquakecore.project.surf_project import SurfProject\nfrom surfquakecore.moment_tensor.mti_parse import WriteMTI\n\ndef list_files_with_iversion_json(root_folder):\n    iversion_json_files = []\n\n    for foldername, subfolders, filenames in os.walk(root_folder):\n        for filename in filenames:\n            if filename == \"iversion.json\":\n                iversion_json_files.append(os.path.join(foldername, filename))\n\n    return iversion_json_files\n\nif __name__ == \"__main__\":\n    cwd = os.path.dirname(__file__)\n    resource_root = os.path.join(cwd, \"mti\")\n    inventory_path = os.path.join(resource_root, \"inventories\", \"inv_surfquakecore.xml\")\n    data_dir_path = os.path.join(resource_root, \"waveforms\")\n    path_to_project = os.path.join(resource_root, \"project\")\n    path_to_configfiles = os.path.join(resource_root, \"list_earthquakes\")\n    working_directory = os.path.join(resource_root, \"working_directory\")\n    output_directory = os.path.join(resource_root, \"output_directory\")\n\n    # Load the Project\n    project_name = \"mti_project.pkl\"\n    path_to_project = os.path.join(path_to_project, project_name)\n    sp = SurfProject(path_to_project)\n    sp.search_files(verbose=True)\n    print(sp)\n\n\n    # Build the class\n    bic = BayesianIsolaCore(project=sp, inventory_file=inventory_path, output_directory=output_directory,\n                            save_plots=True)\n\n    # Run Inversion\n    bic.run_inversion(mti_config=path_to_configfiles)\n    print(\"Finished Inversion\")\n    iversion_json_files = list_files_with_iversion_json(output_directory)\n\n    for result_file in iversion_json_files:\n        result = read_isola_result(result_file)\n        print(result)\n\n    # Write a summury from all ouputs    \n    wm = WriteMTI(parsed_args.output_dir_path)\n    wm.mti_summary()\n</code></pre> <p>Alternatively, if you want you would prefer crate moment tensor config objects rather than point to a folder with .ini files. Then, crate objects like this:</p> <pre><code>mti_configs = [] # Create an empty list to storage mti configurations\n\ndate_str = \"28/02/2022 02:07:59.433\"\norigin_date = datetime.strptime(date_str, '%d/%m/%Y %H:%M:%S.%f')\n# still implementing test\nmti_config1 = MomentTensorInversionConfig(\n    origin_date=origin_date,\n    latitude=42.5414,\n    longitude=1.4505,\n    depth_km=5.75,\n    magnitude=3.0,\n    stations=[StationConfig(name=\"TEST1\", channels=[\"NNH\", \"NNZ\", \"NNE\"]), \n    StationConfig(name=\"TEST2\", channels=[\"NNH\", \"NNZ\", \"NNE\"])],\n    inversion_parameters=InversionParameters(\n        earth_model_file=\"earthmodel/Iberia.txt\",\n        location_unc=0.7,\n        time_unc=.2,\n        depth_unc=3.,\n        source_duration=2.0,\n        rupture_velocity=2500.,\n        min_dist=10.,\n        max_dist=300.,\n        source_type='PointSource'\n    ),\n)\n\n\nmti_cnfig2 = MomentTensorInversionConfig(....)\n........\n\nmti_configs = [mti_config1, mti_config2 .... ]\n</code></pre> <p>Then</p> <pre><code>for mti_config in list_of_mti_configs:\n    bic.run_inversion(mti_config=mti_config)\n\niversion_json_files = list_files_with_iversion_json(output_directory)\n\nfor result_file in iversion_json_files:\n    result = read_isola_result(result_file)\n    print(result)\n\nwm = WriteMTI(parsed_args.output_dir_path)\nwm.mti_summary()\n</code></pre>"},{"location":"particle_motion_documentation/","title":"Processing Options","text":""},{"location":"particle_motion_documentation/#particle-motion-analysis","title":"Particle Motion Analysis","text":"<p>The <code>pm</code> (particle motion) method analyzes the motion of a particle as recorded by a three-component seismic station (e.g., Z, N, E components). It provides geometrical parameters such as azimuth, incidence angle, rectilinearity, and planarity, which are critical for understanding wave polarization and propagation direction.</p>"},{"location":"particle_motion_documentation/#description","title":"Description","text":"<p>Particle motion analysis plots the trajectory of ground motion in 3D (or in 2D projections like Z-N, Z-E, and N-E planes). By examining these plots, users can determine:</p> <ul> <li>Azimuth (\u00b0): The horizontal direction of wave propagation.</li> <li>Incidence (\u00b0): The vertical angle of wave arrival relative to the surface.</li> <li>Rectilinearity: A measure of how straight (linear) the motion is, often associated with body waves.</li> <li>Planarity: A measure of how planar (flat) the motion is, often associated with surface waves.</li> </ul> <p>The method automatically detects and maps three-component combinations (e.g., ZNE, Z12, ZYX) and processes them accordingly.</p>"},{"location":"particle_motion_documentation/#parameters","title":"Parameters","text":"<ul> <li><code>output_path</code> (optional): Directory where particle motion plots and results will be saved.  </li> <li>If provided, a text file <code>pm.txt</code> will record azimuth, incidence, rectilinearity, and planarity for each analyzed trace.</li> <li>A <code>.png</code> plot is generated for each station.</li> </ul>"},{"location":"particle_motion_documentation/#config-example","title":"Config Example","text":"<pre><code>Analysis:\n  process_1:\n    name: 'particle_motion'\n    output_path: './pm_results'\n</code></pre>"},{"location":"particle_motion_documentation/#outputs","title":"Outputs","text":"<p>For each valid three-component station: - Plots:   - Z vs. N projection   - Z vs. E projection   - N vs. E projection (with azimuth arrow)   - Information box with computed parameters - Text file (pm.txt): Appends results in CSV format:   <pre><code>NET.STA.LOC.CHAN, YYYY-MM-DD HH:MM:SS, Azimuth, Incidence, Rectilinearity, Planarity\n</code></pre></p>"},{"location":"particle_motion_documentation/#example-output-plot","title":"Example Output Plot","text":"<p>The particle motion figure contains: 1. Z vs. N plane with incidence angle overlay. 2. Z vs. E plane with incidence angle overlay. 3. N vs. E plane with azimuth direction indicated by an arrow. 4. A summary panel showing:    - Azimuth    - Incidence    - Rectilinearity    - Planarity</p>"},{"location":"particle_motion_documentation/#scientific-background","title":"Scientific Background","text":"<p>Particle motion analysis is widely used in seismology for polarization studies, wavefield separation, and source characterization. For more details, see:</p> <ul> <li>Flinn, E. A. (1965). Signal Analysis Using Rectilinearity and Direction of Particle Motion. Proceedings of the IEEE, 53(12), 1874\u20131876.</li> <li>Vidale, J. E. (1986). Complex polarization analysis of particle motion. Bulletin of the Seismological Society of America, 76(5), 1393\u20131405.</li> </ul>"},{"location":"particle_motion_documentation/#notes","title":"Notes","text":"<ul> <li>Ensure that three-component data are present for each station (ZNE or equivalent).</li> <li>The traces are trimmed to a common time window before analysis.</li> <li>If no valid 3-component set is found, the process is skipped for that station.</li> </ul>"},{"location":"picker/","title":"Phase Picker","text":"<p>The Picking algorythm of surfQuake uses the Deep Neural Network of Phasenet (Zhu and Beroza, 2019) to estimate the arrival times of P- and S-wave. The arrival times are saved as a csv file and in daily folders to be ready to be used by the associator. Example of csv header:</p> <pre><code>date,fname,year,month,day,net,station,flag,tt,date_time,weight,amplitude,phase\n20220131,CA.ARBS.P,2022,1,31,CA,ARBS,1,39383.88,2022-01-31T10:56:23.880000,0.5383206605911255,8557892.700195312,P\n20220131,CA.ARBS.S,2022,1,31,CA,ARBS,1,85480.59,2022-01-31T23:44:40.590000,0.30124416947364807,8481788.269042969,S\n</code></pre>"},{"location":"picker/#phase-picker-gui","title":"Phase Picker GUI","text":"<p>We start with the GUI. This is a screenshot of the Project GUI.</p> <p></p> <p>Be sure you have just created a Project or you have loaded one. Then click on Run Auto Pick. This action will start the phase picker and will save the output in Output Directory ready to be used in the associator toolbox and original_picks as csv file for direct reading.</p>"},{"location":"picker/#phase-picker-from-cli","title":"Phase picker from CLI","text":""},{"location":"picker/#usage","title":"Usage","text":"<pre><code>&gt;&gt; surfquake pick -f [path to your project file] -d [path to your pick saving directory] -p [P-wave threshoold] -s [S-wave threshold] --verbose\n</code></pre>"},{"location":"picker/#interactive-help","title":"Interactive help","text":"<pre><code>&gt;&gt; surfquake pick -h\n</code></pre>"},{"location":"picker/#run-phase-picker-from-cli","title":"Run Phase Picker from CLI","text":"<pre><code>&gt;&gt; surfquake pick -f /test_surfquake_core/testing_data/projectssurfquake_project_new.pkl -d /test_surfquake_core/testing_data/picks -p 0.3 -s 0.3 --verbose\n</code></pre>"},{"location":"picker/#phase-picker-from-library","title":"Phase Picker from Library","text":""},{"location":"picker/#classes","title":"Classes","text":"<p><code>PhasenetISP</code></p> <pre><code>class PhasenetISP:\n    def __init__(files, batch_size=3, highpass_filter=0.5, min_p_prob=0.3, min_s_prob=0.3, min_peak_distance=50, amplitude=False):\n\n    \"\"\"\n\n    Main class to initialize the picker\n\n    :param files: Dictionary with kewords addressing to seismograms file path and their corresponding metadata (i.e. sampling rate).\n    :type SurfProject: required (see Project section)\n\n    :param batch_size: Determines the number of samples in each batch (larger batch size uses more memory but can provide more accurate updates)\n    :type float:\n\n    :param highpass_filter: Lower corner frequency of highpass filter to be applied to the raw seismogram. Set to 0 to do not apply any pre-filter\n    :type float:\n\n    :param min_p_prob: Probability threshold for P pick\n    :type float:\n\n    :param min_s_prob: Probability threshold for S pick\n    :type float:\n\n    :param min_peak_distance: Minimum peak distance\n    :type float:\n\n    :param amplitude: if return amplitude value\n    :type float:\n\n    :returns:\n    :rtype: :class:`surfquakecore.phasenet.phasenet_handler.PhasenetISP`\n\n    \"\"\"\n</code></pre> <p><code>PhasenetUtils</code></p>"},{"location":"picker/#methods","title":"Methods","text":"<p><code>phasenet</code></p> <pre><code># instance method\ndef phasenet(self):\n</code></pre> <p><code>PhasenetUtils.split_picks</code></p> <pre><code>@staticmethod\ndef split_picks(picks):\n    \"\"\"\n    :param picks: A DataFrame with all pick information\n    :type picks: Pandas DataFrame\n    \"\"\"\n</code></pre> <p><code>PhasenetUtils.convert2real</code></p> <pre><code>@staticmethod\ndef convert2real(picks, pick_dir: str):\n\"\"\"\n:param picks: picks is output from method split_picks in mseedutils\n:param pick_dir: directory outpur where phases are storaged\n:return:\n\"\"\"\n</code></pre> <p><code>PhasenetUtils.split_picks</code> <pre><code>@staticmethod\ndef save_original_picks(original_picks, original_p_dir):\n    \"\"\"\n\n    :param original_picks: picking output from phasenet (method split_picks in mseedutils)\n    :param original_p_dir: output to storage original_picks\n    :return:\n    \"\"\"\n</code></pre></p>"},{"location":"picker/#example-using-library","title":"Example using library","text":"<pre><code>import os\nfrom multiprocessing import freeze_support\nfrom surfquakecore.phasenet.phasenet_handler import PhasenetUtils\nfrom surfquakecore.phasenet.phasenet_handler import PhasenetISP\nfrom surfquakecore.project.surf_project import SurfProject\n\n### Set Paths to project file and output folder ###\npath_to_project = \"/Volumes/LaCie/test_surfquake_core/project/surfquake_project.pkl\"\noutput_picks = '/Volumes/LaCie/test_surfquake_core/test_picking'\n\nif __name__ == '__main__':\n    freeze_support()\n\n    # Load project\n    sp_loaded = SurfProject.load_project(path_to_project_file=path_to_project)\n\n    # Instantiate the class PhasenetISP\n    phISP = PhasenetISP(sp_loaded.project, amplitude=True, min_p_prob=0.90, min_s_prob=0.65)\n\n    # Running Stage\n    picks = phISP.phasenet()\n\n    \"\"\" PHASENET OUTPUT TO REAL INPUT \"\"\"\n\n    picks_results = PhasenetUtils.split_picks(picks)\n    PhasenetUtils.convert2real(picks_results, output_picks)\n    PhasenetUtils.save_original_picks(picks_results, output_picks)\n</code></pre>"},{"location":"plotting_interaction_documentation/","title":"SurfQuake Plotting and Interactive Commands","text":"<p>SurfQuake provides a powerful plotting interface designed for seismological workflows. It supports trace visualization, interactive pick annotation, metadata overlays, and integration with processing outputs such as characteristic functions, beamforming results, or filtered waveforms.</p> <p>This plotting interface is designed for both exploratory data analysis and high-quality figure export.</p>"},{"location":"plotting_interaction_documentation/#overview-of-interactive-plotting","title":"Overview of Interactive Plotting","text":"<p>When a waveform stream is visualized (e.g., after running a processing pipeline or with the <code>surfquake quick</code> command), SurfQuake launches an interactive window showing a page of traces. This interface supports keyboard-driven interactions to facilitate manual picking and review.</p> <p>Each subplot corresponds to a single seismic trace and includes time axes, amplitude plots, and optional overlays (picks, markers, etc.). Traces are paginated if their count exceeds the configured number per page.</p>"},{"location":"plotting_interaction_documentation/#core-features","title":"Core Features","text":"<ul> <li>Paging: Use <code>n</code> and <code>b</code> to move to next and previous pages.</li> <li>Picking: Press <code>e</code> and click to pick a phase arrival. You'll be prompted for phase type and polarity.</li> <li>Reference markers: Press <code>w</code> to insert a green vertical reference line.</li> <li>Pick management:</li> <li><code>d</code>: Delete the last pick.</li> <li><code>c</code>: Clear all picks.</li> <li><code>m</code>: Remove the most recent reference.</li> <li><code>p</code>: Remove the last reference marker from all traces.</li> <li>Command prompt: Press <code>v</code> to open the SurfQuake command bar within the plotting session.</li> <li>Exit keys: Use <code>enter</code> or <code>esc</code> to exit the interactive session.</li> </ul> <p>All interactions update the internal <code>Trace.stats</code> metadata. For instance, picks are stored in <code>Trace.stats.picks</code>, and reference markers in <code>Trace.stats.references</code>.</p>"},{"location":"plotting_interaction_documentation/#pick-metadata-storage","title":"Pick Metadata Storage","text":"<p>When a user places a pick using the <code>e</code> key, a dictionary is appended to the picking list <code>tr.stats.picks</code>, with the following structure:</p> <pre><code>{\n    \"time\": float,          # UTC timestamp\n    \"phase\": str,           # Phase name (e.g., 'P', 'S', 'Pg', etc.)\n    \"amplitude\": float,     # Interpolated amplitude at pick time\n    \"polarity\": str         # 'U', 'D', or '?' (unclear)\n}\n</code></pre> <p>Reference markers (green vertical lines placed via <code>w</code>) are stored as timestamps in <code>tr.stats.references</code>.</p>"},{"location":"plotting_interaction_documentation/#configuration-options","title":"Configuration Options","text":"<p>The plotting interface is governed by a YAML configuration passed via <code>--plot_config</code>, which supports:</p> <pre><code>plotting:\n  traces_per_fig: 3 # default 6\n  sort_by: distance            # options: 'distance', 'backazimuth', or null\n  vspace: 0.05                 # vertical space between subplots\n  show_legend: true\n  title_fontsize: 9\n  plot_type: standard        # 'standard', 'record' for record section and overlay for all traces at the same plot \n  sharey: False\n  show_arrivals: False\n  pick_output_file: ./picks.csv # picks will be written here\n  auto_load_pick_file: False   # if user wants that picking file is automatically loaded and plot\n  show_info_picks: False # show pick info on screen interactively\n  backend: TkAgg # TkAgg (maximum robustness, default), MacOSX (only for mac), Qt5Agg (fast, picking  &amp; interactive commands raise problems)\n</code></pre>"},{"location":"plotting_interaction_documentation/#output-and-persistence","title":"Output and Persistence","text":"<ul> <li>All user picks and references are stored in memory and (optionally) written to disk.</li> <li>Picks can be logged to CSV using the <code>pick_output_file</code> config key.</li> <li>The waveform figures can be exported as <code>.png</code> files.</li> <li>Header metadata can be updated interactively and saved if exported in HDF5 format.</li> </ul>"},{"location":"plotting_interaction_documentation/#summary-table-of-key-commands-in-picking-mode","title":"Summary Table of Key Commands in picking mode.","text":"Key Action <code>1</code> P-wave polarity ? <code>2</code> P-wave polarity Up <code>3</code> P-wave polarity Down <code>4</code> S-wave polarity ? <code>5</code> S-wave polarity Up <code>6</code> S-wave polarity Down <code>e</code> Add pick <code>w</code> Add reference line <code>d</code> Delete last pick <code>c</code> Clear all picks <code>m</code> Remove most recent reference <code>p</code> Remove last reference from all <code>n</code> Next page <code>b</code> Previous page <code>v</code> Command prompt <code>enter</code> / <code>esc</code> Exit plotting"},{"location":"plotting_interaction_documentation/#command-line-prompt-v-key","title":"Command-Line Prompt (v Key)","text":"<p>SurfQuake provides an embedded terminal-style command prompt for advanced user interaction. Press <code>v</code> during plotting to access it.</p> <p>This prompt allows you to apply real-time processing, visualization, trimming, and export actions without leaving the plot.</p>"},{"location":"plotting_interaction_documentation/#example-session","title":"Example Session","text":"<pre><code>&gt;&gt; filter bandpass 0.1 1.0 --corners 4 --zerophase True\n&gt;&gt; cut --phase P 5 15\n&gt;&gt; beam --method FK --fmin 0.8 --fmax 2.0 --grid 0.05\n&gt;&gt; write --folder_path ./processed_h5/\n&gt;&gt; p\n</code></pre>"},{"location":"plotting_interaction_documentation/#available-commands-detailed","title":"Available Commands (Detailed)","text":"<ul> <li> <p><code>filter &lt;type&gt; &lt;fmin&gt; &lt;fmax&gt; [options]</code>: Filter all displayed traces.</p> <ul> <li>Example: <code>filter bandpass 0.5 8.0 --corners 4 --zerophase True</code></li> </ul> </li> <li> <p><code>sp &lt;index&gt;|all [axis_type]</code>: Plot spectrum (FFT) of a specific or all traces options of axis plot are loglog, xlog, ylog.</p> <ul> <li>Example: <code>spectrum 0</code>, <code>spectrum all loglog</code></li> </ul> </li> <li> <p><code>spec &lt;index&gt; [win overlap]</code>: Plot multitaper spectrogram of a trace. win is the sliding window defined in seconds and overlap is the % of sliding windoe step overlap.</p> <ul> <li>Example: <code>spec 0 4 75</code> # 4 seconds overalp of 75%</li> </ul> </li> <li> <p><code>cwt &lt;index&gt; &lt;wavelet&gt; &lt;param&gt; [fmin fmax]</code>: Continuous wavelet transform visualization.</p> <ul> <li>Example: <code>cwt 1 cm 6 0.1 10</code> options are wavelet: cm (complex morlet wavelet), mh (mehican hat) and pa (Paul Wavelet).</li> </ul> </li> <li> <p><code>beam [options]</code>: Run beamforming in sliding windows (FK method).</p> <ul> <li>Example: <code>beam --fmin 0.5 --fmax 2.5 --win 3 --grid 0.025</code></li> </ul> </li> <li> <p><code>smap [options]</code>: --fmin --fmax --smax --grid --method FK|MTP.COHERENCE|CAPON|MUSIC</p> <p>units: frequency in Hz, smax (maximum slowness search) and grid (Slowness grid spacing) in s/km</p> <ul> <li>Example:            <code>smap --method CAPON --fmin 1.0 --fmax 3.0 --grid 0.01</code> <code>smap --method MUSIC --fmin 1.0 --fmax 3.0 --grid 0.01  --nsignals 1</code></li> </ul> </li> <li> <p><code>stack [options]</code>: list_traces --method [mean|pw|root]</p> <ul> <li>Example:             <code>stack</code> <code>stack 0,3,7</code> <code>stack 0-5 --method root:4</code> <code>stack all --method pw:2</code></li> </ul> </li> <li> <p><code>xcorr [--ref &lt;i&gt;] [--mode full] [--normalize full]</code>: Apply cross-correlation.</p> <ul> <li>Example: <code>xcorr --ref 0 --mode full --normalize full</code></li> </ul> </li> <li> <p><code>plot_type &lt;type&gt;</code>: Change plot layout (<code>standard</code>, <code>record</code>, <code>overlay</code>).</p> <ul> <li>Example: <code>plot_type overlay</code></li> </ul> </li> <li> <p><code>cut</code>: Trim traces based on:</p> <ul> <li>span time selected: <code>cut</code> # cut waveforms according to the starttime and endtime selected by previously draging with right mouse to select start and end time in picking mode.</li> <li>Phase: <code>cut --phase P 5 10</code> # cut the waveforms 10 seconds before and 20 seconds after the P wave (for traces with picked P wave)</li> <li>Reference marker: <code>cut --reference 10 20</code> # cut the waveforms 10 seconds before and 20 seconds after the common reference.</li> <li>UTC: <code>cut --start \"2024-01-01 12:00:00\" --end \"2024-01-01 12:01:00\"</code> # cut all waveforms acoording to the span time</li> </ul> </li> <li> <p><code>shift --phase &lt;name&gt;</code> or <code>--phase_theo &lt;name&gt;</code>: Time-align traces.</p> <ul> <li><code>shift --phase S</code> # according picked phase S-wave</li> <li><code>shift --phase_theo S</code> # according to the theoretically estimated S-wave. Only if event info is in header. see surfquake processing -e event</li> </ul> </li> <li> <p><code>concat</code>: Merge overlapping trace segments using interpolation. Merge daily seismograms by net station channel</p> </li> <li> <p><code>pm</code>: Perform particle motion analysis (Z-N-E style).</p> </li> <li> <p><code>write --folder_path &lt;path&gt;</code>: Export displayed traces to <code>.h5</code>.</p> </li> <li> <p><code>info</code> : Print header information from displayed traces</p> </li> <li> <p><code>p</code>: Return to interactive pick mode.</p> </li> <li> <p><code>n</code> / <code>b</code>: Navigate to next/previous page.</p> </li> <li> <p><code>help [command]</code>: Show help or detailed help for a command.</p> </li> <li> <p><code>exit</code>: End command prompt and close plotting.</p> </li> </ul> <p>Very important: Exit from auxiliary plots, let's say spectrum, spectrogram or cwt by pressing <code>enter</code> / <code>esc</code>, NOT closing the GUI. Otherwise the prompt is frozen an you will not able to back to picking mode.</p> <p>This system offers a lightweight yet highly customizable interface for interactive seismogram analysis in SurfQuake. It blends visual inspection with real-time metadata editing to support modern waveform workflows.</p>"},{"location":"processing_config/","title":"Processing Individual Traces Options","text":""},{"location":"processing_config/#detrending","title":"Detrending","text":"<p>Remove unwanted trends from the raw signal:</p> <ul> <li><code>linear</code>: Remove a best-fit line</li> <li><code>demean</code>: Subtract the mean</li> <li><code>polynomial</code>: Fit and subtract a polynomial (e.g., degree 3)</li> <li><code>spline</code>: Subtract a smooth spline fit</li> </ul>"},{"location":"processing_config/#tapering","title":"Tapering","text":"<p>Apply a window to reduce spectral leakage near signal edges.</p> <ul> <li>Parameter: <code>max_percentage</code> (e.g., 0.05 for 5%)</li> <li>Supported Windows:   <code>cosine</code>, <code>hann</code>, <code>hamming</code>, <code>blackman</code>, <code>kaiser</code>, <code>flattop</code>, <code>slepian</code>.</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'rmean'\n    method: 'linear'\n  process_2:\n    name: 'taper'\n    method: 'cosine'\n    max_percentage: 0.05\n</code></pre>"},{"location":"processing_config/#normalize","title":"Normalize","text":"<p>Rescale amplitude:</p> <ul> <li><code>0</code>: Normalize by maximum</li> <li>Custom value: Divide signal by provided constant</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'normalize'\n    norm: True\n</code></pre>"},{"location":"processing_config/#filtering","title":"Filtering","text":"<p>Apply filters to isolate specific frequency content.</p> <ul> <li>Parameters: <code>freqmin</code>, <code>freqmax</code>, <code>zerophase</code>, <code>corners</code> (poles)</li> <li>Filter Types:</li> <li><code>bandpass</code>, <code>lowpass</code>, <code>highpass</code>, <code>bandstop</code></li> <li>Parametric options: <code>cheby1</code>, <code>cheby2</code>, <code>elliptic</code>, <code>bessel</code></li> </ul> <p>\u2139\ufe0f Ensure <code>freqmin &lt; freqmax</code> for band filters.</p> <pre><code>Analysis:\n  process_1:\n    name: 'rmean'\n    method: 'linear'\n  process_2:\n    name: 'taper'\n    method: 'cosine'\n    max_percentage: 0.05\n  process_3:\n    name: 'normalize'\n    norm: True\n  process_4:\n    name: 'filter'\n    type: 'bandpass'\n    fmin: 0.5\n    fmax: 8.5\n    corners: 4\n    zerophase: True\n</code></pre>"},{"location":"processing_config/#denoising","title":"Denoising","text":""},{"location":"processing_config/#wiener-filter","title":"Wiener Filter","text":"<ul> <li>Parameters: <code>time_window</code> (seconds), <code>noise_power</code> </li> <li>Tip: If <code>noise_power=0</code>, it\u2019s estimated from local variance.</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'rmean'\n    method: 'linear'\n  process_2:\n    name: 'taper'\n    method: 'cosine'\n    max_percentage: 0.05\n  process_3:\n    name: 'filter'\n    type: 'bandpass'\n    fmin: 0.5\n    fmax: 8.5\n    corners: 4\n    zerophase: True\n  process_4:\n    name: \"wiener_filter\"\n    time_window: 1.0\n    noise_power: 0\n</code></pre>"},{"location":"processing_config/#wavelet-denoise","title":"Wavelet Denoise","text":"<ul> <li>Parameters: <code>wavelet_family</code> (e.g., <code>sym8</code>), <code>threshold</code> (e.g., 0.05)</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'rmean'\n    method: 'linear'\n  process_2:\n    name: 'taper'\n    method: 'cosine'\n    max_percentage: 0.05\n  process_3:\n    name: 'filter'\n    type: 'bandpass'\n    fmin: 0.5\n    fmax: 8.5\n    corners: 4\n    zerophase: True\n  process_4:\n    name: 'wavelet_denoise'\n    dwt: 'sym8'\n    threshold: 0.1\n</code></pre>"},{"location":"processing_config/#resampling","title":"Resampling","text":"<p>Adjust the sampling rate (e.g., for standardization):</p> <ul> <li>Parameters: <code>new_sampling_rate</code>, <code>pre_filter</code> (recommended: <code>True</code>)</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'resample'\n    sampling_rate: 20\n    pre_filter: True\n</code></pre>"},{"location":"processing_config/#quick-resampling","title":"Quick Resampling","text":"<p>This strategy makes a very fast resampling specially designg to speed up the plot of seismograms. Warning this resampling not prevents from aliasing effects.</p> <ul> <li>Parameters: <code>factor</code>, <code>integers</code> (recommended: <code>True</code>)</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'raw'\n    factor: 20\n    integers: True\n</code></pre>"},{"location":"processing_config/#fill-gaps","title":"Fill Gaps","text":"<ul> <li>Methods: <code>interpolate</code>, <code>latest</code> (repeat last value)</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'fill_gaps'\n    method: 'latest'\n</code></pre>"},{"location":"processing_config/#differentiate","title":"Differentiate","text":"<p>Estimate the signal\u2019s derivative:</p> <ul> <li>Methods: <code>gradient</code>, <code>spectral</code></li> </ul> <pre><code>Analysis:\n  process_9:\n    name: 'differentiate'\n    method: 'spectral'\n</code></pre>"},{"location":"processing_config/#integrate","title":"Integrate","text":"<p>Reconstruct displacement or velocity:</p> <ul> <li>Methods: <code>cumtrapz</code>, <code>spline</code>, <code>spectral</code></li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'integrate'\n    method: 'spectral'\n</code></pre>"},{"location":"processing_config/#time-shifting","title":"Time Shifting","text":"<p>Shift traces forward or backward in time.</p> <ul> <li>Parameter: List of time shifts in seconds</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'shift'\n    time_shifts: [-5, 2] # list of shift seconds of the starttime. \n    phase: P #  if pick is vailable. \n    phase_theo: P #  if your trace header has event information.\n</code></pre>"},{"location":"processing_config/#remove-instrument-response","title":"Remove Instrument Response","text":"<p>Convert raw counts to physical units (e.g., velocity or displacement):</p> <p>\u26a0\ufe0f WARNING: Remember to includ your inventory: e.g., <code>surfquake quick -w \"waveform_file\" -i \"inventory_file.xml\"</code></p> <ul> <li>Parameters: <code>pre_filt</code> (list), <code>water_level</code>, <code>units</code>, <code>inventory</code></li> </ul> <pre><code>  process_1:\n    name: 'remove_response'\n    water_level: 90\n    units: 'VEL' # [ACC Wood Anderson]\n    pre_filt: [0.005,0.008,45,48] # Hz, incremental frequencies defining the pre-filt\n</code></pre>"},{"location":"processing_config/#add-noise","title":"Add Noise","text":"<p>White Noise: - Parameter: <code>SNR_dB</code> amount of noise relative to maximum amplitude of the original trace.</p> <p>Colored Noise (optional, not in base config): - Use <code>exponent</code> for control: 0 (white), 1 (pink), 2 (brown), -1 (blue), etc.</p> <pre><code>Analysis:\n  process_1:\n    name: 'add_noise'\n    noise_type: 'white'\n    SNR_dB: 1\n</code></pre>"},{"location":"processing_config/#spectral-whitening","title":"Spectral Whitening","text":"<p>Flattens signal spectrum while preserving phase:</p> <ul> <li>Parameters: <code>freq_width</code> (Hz), <code>taper_edge</code> (True/False)</li> </ul> <pre><code>Analysis:\n  process_11:\n    name: 'whitening'\n    freq_width: 0.02\n    taper_edge: True\n</code></pre>"},{"location":"processing_config/#time-normalization","title":"Time Normalization","text":"<p>Standardize temporal energy distribution:</p> <ul> <li>Methods: <code>time-normalization</code>, <code>1bit</code>, <code>clipping</code> and  <code>clipping iteration</code>.</li> <li>Parameters: <code>norm_win</code>, <code>iterations</code></li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'time_normalization'\n    method: 'time normalization'\n    norm_win: 10\n</code></pre>"},{"location":"processing_config/#smoothing","title":"Smoothing","text":"<p>Reduce high-frequency noise or jitter:</p> <ul> <li>Parameters: <code>time_window</code>, <code>fwhm</code></li> <li>Methods: <code>mean</code>, <code>gaussian</code>, <code>tkeo</code></li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'smoothing'\n    method: 'gaussian'\n    time_window: 5\n    FWHM: 0.05\n</code></pre>"},{"location":"processing_config/#spike-removal","title":"Spike Removal","text":"<p>Removes isolated outliers using hampel algorithm:</p> <ul> <li>Parameters: <code>window_size</code>, <code>sigma</code> (standard deviations from the avarage noise)</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'remove_spikes'\n    window_size: 10\n    sigma: 4\n</code></pre>"},{"location":"processing_overview/","title":"Data Processing with SurfQuake","text":"<p>Welcome to the Signal Processing Module of SurfQuake \u2014 a powerful and scalable command-line tool for applying signal processing pipelines to large collections of seismic waveform data.</p> <p>Whether you're analyzing earthquake catalogs or preparing data for machine learning models, this module allows you to define, control, and automate your signal processing workflow with precision.</p> <p>\ud83d\udcd8 Learn more in our Signal Processing Tutorial</p> <p>Remember that you can use real data files and metadata to learn how to use surfquake processing commands with Case of study.</p> <p>In the following link, you will find  config file templates.</p>"},{"location":"processing_overview/#running-the-process","title":"Running the Process","text":"<p>This tool runs via the terminal. First in the terminal you can ask for help. Available commands are quick, processing and processing_daily.</p> <pre><code>surfquake command -h\n</code></pre>"},{"location":"processing_overview/#surfquake-quick","title":"surfquake quick","text":"<p>Let's start explaining quick command:</p> <p>Overview:         Process seismic traces.         You can:             - Apply processing steps (filtering, normalization, etc.)             - Optionally visualize the waveforms (interactive mode)             - Apply a user-defined post-processing script before or after plotting</p> <pre><code>Modes:\n    Default  : Interactive mode (plotting + user prompts)\n    --auto   : Non-interactive mode (no plots, no prompts, outputs written automatically)\n\nPost-Script Logic:\n    Use `--post_script` to apply a custom script to each event stream.\n    Use `--post_script_stage` to control **when** it runs:\n        \u2022 before : script runs before plotting (good for filtering, editing headers)\n        \u2022 after  : script runs after plotting (good if picks or metadata are added)\n\n    Examples:\n        Process waveform files with a config and plot interactively:\n            surfquake quick -w \"./data/*.mseed\" -c ./config.yaml -i ./inventory.xml --plot_config plot.yaml\n\n    Key Arguments:\n        -w, --wave_files         [REQUIRED] Glob pattern or path to waveform files\n        -c, --config_file        [OPTIONAL] YAML config defining processing steps\n        -i, --inventory_file     [OPTIONAL] Station metadata (StationXML or RESP)\n        -o, --output_folder      [OPTIONAL] Directory to save processed traces\n        -a, --auto               [OPTIONAL] Run in automatic (non-interactive) mode\n        --plot_config            [OPTIONAL] Plotting settings YAML\n        --post_script            [OPTIONAL] Python script to apply to each stream\n        --post_script_stage      [OPTIONAL] When to run post-script: 'before' or 'after' (default: before)\n</code></pre>"},{"location":"processing_overview/#surfquake-processing","title":"surfquake processing","text":"<pre><code>Overview:\n    Process or cut waveforms associated with seismic events.\n    You can:\n        - Cut traces using event times and headers\n        - Apply processing steps (filtering, normalization, etc.)\n        - Optionally visualize the waveforms (interactive mode)\n        - Apply a user-defined post-processing script before or after plotting\n\nModes:\n    Default  : Interactive mode (plotting + user prompts)\n    --auto   : Non-interactive mode (no plots, no prompts, outputs written automatically)\n\nPost-Script Logic:\n    Use `--post_script` to apply a custom script to each event stream.\n    Use `--post_script_stage` to control **when** it runs:\n        \u2022 before : script runs before plotting (good for filtering, editing headers)\n        \u2022 after  : script runs after plotting (good if picks or metadata are added)\n\nUsage Example:\n    surfquake processing -p \"./project.pkl\" -i inventory.xml -e events.xml -c config.yaml -o ./output_folder --phases P,S --plot_config plot_settings.yaml --post_script custom_postproc.py\n\nKey Arguments:\n    -p, --project_file         [OPTIONAL] Path to an existing project file\n    -w, --wave_files           [OPTIONAL] Path or glob pattern to waveform files\n    -i, --inventory_file       [OPTIONAL] Station metadata file (XML, RESP)\n    -e, --event_file           [OPTIONAL] Event catalog in QuakeML format\n    -c, --config_file          [OPTIONAL] Processing configuration file (YAML)\n    -a, --auto                 [OPTIONAL] Run in automatic mode\n    -o, --output_folder        [OPTIONAL] Folder where processed files are saved\n    -n, --net                  [OPTIONAL] project net filter Example: NET1,NET2,NET3\n    -s, --station              [OPTIONAL] project station filter Example: STA1,STA2,STA3\n    -ch, --channel,            [OPTIONAL]  project channel filter Example: HHZ,BHN\n    -t, --cut_time             [OPTIONAL] pre &amp; post first arrival in seconds (symmetric)\n    -cs, --cut_start_time      [OPTIONAL] cut pre-first arrival in seconds\n    -ce, --cut_end_time        [OPTIONAL] cut ppost-first arrival in seconds\n    -r, --reference            [OPTIONAL] The pick time for cutting the waveforms is the origin time of the event\n    --phases                   [OPTIONAL] Comma-separated list of phases for arrival estimation (e.g., P,S)\n    --plot_config              [OPTIONAL] Optional plot configuration file (YAML)\n    --post_script              [OPTIONAL] Python script to apply per event stream\n    --post_script_stage        [OPTIONAL] When to apply the post-script: before | after (default: before)\n\nNotes:\nYou might need to set -w for individual files or -p for loading your project file\n</code></pre>"},{"location":"processing_overview/#surfquake-processing_daily","title":"surfquake processing_daily","text":"<p>This command is different to simply surfquake processing because is intended to merge daily seismograms and work for big data sets. For xample, imagine that you need three days of seismograms for normal modes analysis of an event. Or imagine you would like to process datayo your daily mseed files.</p> <pre><code>Overview:\n    Cut seismograms and apply processing to continuous waveforms.\n    You can perform either or both of these operations.\n\nModes:\n    Default : Interactive mode (with plotting and prompts)\n    --auto  : Non-interactive mode (no plots, no prompts, automatic output)\n\nPost-Script Logic:\n    Use --post_script to apply a custom script per station/day stream.\n    Use --post_script_stage to control when the script is applied:\n        \u2022 before : apply script before plotting (e.g., for filtering or cleanup)\n        \u2022 after  : apply script after plotting (e.g., to act on manual picks)\n\nUsage Example:\n    surfquake processing_daily \\\\\n        -i inventory.xml \\\\\n        -c config.yaml \\\\\n        -o ./output_folder \\\\\n        --min_date \"2024-01-01 00:00:00\" \\\\\n        --max_date \"2024-01-02 00:00:00\" \\\\\n        --span_seconds  86400\\\\\n        --plot_config plot_settings.yaml \\\\\n        --post_script my_custom.py \\\\\n        --post_script_stage after\n\nKey Arguments:\n    -p, --project_file        [REQUIRED] Path to a saved project files\n    -o, --output_folder       [OPTIONAL] Directory for processed output\n    -i, --inventory_file      [OPTIONAL] Station metadata (XML/RESP)\n    -c, --config_file         [OPTIONAL] Processing configuration (YAML)\n    -a, --auto                [OPTIONAL] Run in automatic mode\n    -n, --net                 [OPTIONAL] Network code filter\n    -s, --station             [OPTIONAL] Station code filter\n    -ch, --channel            [OPTIONAL] Channel filter\n    --min_date                [OPTIONAL] Filter Start date (format: YYYY-MM-DD HH:MM:SS), DEFAULT min date of the project\n    --max_date                [OPTIONAL] Filter End date   (format: YYYY-MM-DD HH:MM:SS), DEFAULT max date of the project\n    --time_tolerance          [OPTIONAL] Tolerance in seconds for time filtering, excluded files with smaller time span\n    --span_seconds            [OPTIONAL] Select and merge files in sets of time spans, DEFAULT 86400\n    --time_segment            [OPTIONAL] If set, process entire time window as a single merged stream of traces\n    --plot_config             [OPTIONAL] Optional plotting configuration (YAML)\n    --post_script             [OPTIONAL] Path to Python script for custom post-processing\n    --post_script_stage       [OPTIONAL] When to apply the post-script: before | after (default: before)\n\"\"\"\n</code></pre>"},{"location":"processing_overview/#event-file-format-eventtxt","title":"Event File Format (<code>event.txt</code>)","text":"<p>Please find here a very simple event file format. This is typical input for surfquake processing -e ./event.txt</p> <pre><code>date;hour;latitude;longitude;depth;magnitude\n2022-02-02;23:35:29.7;42.5089;1.4293;20.7;1.71\n2022-02-03;12:01:21.6;42.3047;2.2741;0.0;1.65\n</code></pre>"},{"location":"processing_overview/#sample-processing-configuration-configyaml","title":"Sample Processing Configuration (<code>config.yaml</code>)","text":"<p>The macro configuration (provided via a YAML file) defines your signal processing pipeline in a structured, step-by-step format. Each <code>process</code> entry specifies a method, its parameters, and the order in which it will be applied to each trace. You can process either full daily waveform files or extract and process specific segments based on an event file. Additionally, an interactive plotting tool is available to help you visually inspect and validate the processing steps.</p> <p>Please find here a very simple event file format. This is the typical input file for this commands:</p> <pre><code>surfquake quick -w ./mseed_file -c ./config.yaml # reading directly mseed files\n\nsurfquake processing -p ./project.pkl -c ./config.yaml -e ./event.txt # loading a project and using event files\n</code></pre> <p><code>config.yaml</code>: Detaled information of this file in Processing traces and Processing Stream.</p> <pre><code>Analysis:\n  process_1:\n    name: 'rmean'\n    method: 'linear'\n  process_2:\n    name: 'taper'\n    method: 'cosine'\n    max_percentage: 0.05\n  process_3:\n    name: 'filter'\n    type: 'bandpass'\n    fmin: 0.5\n    fmax: 8.0\n    corners: 4\n    zerophase: True\n</code></pre>"},{"location":"processing_stream_config/","title":"Processing Stream Options","text":""},{"location":"processing_stream_config/#rotate-seismograms","title":"Rotate Seismograms","text":"<p>This option rotates the three-component traces in the stream to either the Great Arc Circle (GAC) or a user-defined angle and inclination.</p> <ul> <li> <p><code>type</code>: Defines the rotation transformation.</p> <ul> <li><code>NE-&gt;RT</code>: Rotates North and East components to Radial and Transverse.</li> <li><code>RT-&gt;NE</code>: Rotates Radial and Transverse components back to North and East.</li> <li><code>ZNE-&gt;LQT</code>: Rotates from a left-handed Z, North, East system to an LQT (ray-based, right-handed) coordinate system.</li> <li><code>LQT-&gt;ZNE</code>: Rotates from an LQT system back to a left-handed Z, North, East system.</li> </ul> </li> <li> <p><code>method</code>: Specifies the rotation method.</p> <ul> <li><code>GAC</code>: Rotates according to the Great Arc Circle. Requires event metadata.</li> <li><code>FREE</code>: Rotates to a user-defined angle and inclination (used for <code>LQT</code> transformations).</li> </ul> </li> <li> <p><code>angle</code> (float) \u2013 The back azimuth from station to source in degrees.</p> </li> <li><code>inclination</code> (float). Inclination of the ray at the station in degrees. Only necessary for three component rotations.</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'rotate'\n    type: 'NE-&gt;RT'\n    method: 'GAC'\n    angle: 30.0\n    inclination: 0.0\n</code></pre>"},{"location":"processing_stream_config/#cross-correlation","title":"Cross Correlation","text":"<p>Performs cross-correlation between waveforms.</p> <ul> <li> <p><code>mode</code>: Defines how the cross-correlation is computed, following the NumPy <code>correlate</code> method:</p> <ul> <li><code>full</code>: Returns the complete cross-correlation function. Includes all shifts, even partial overlaps. Best if you want to scan the entire signal.</li> <li><code>valid</code>: Returns only correlation values where the signals fully overlap. Safer when you want to avoid edge effects.</li> <li><code>same</code>: Returns output of the same length as the input data, centered around zero lag.</li> </ul> </li> <li> <p><code>normalize</code>: Specifies how traces are normalized before correlation.</p> <ul> <li><code>full</code>: Uses zero-normalized cross-correlation (ZNCC). Each point is normalized based on local signal energy and optional demeaning. This provides a correlation coefficient between -1 and 1 and is the most statistically meaningful</li> <li><code>naive</code>:Normalizes both signals by their global standard deviation. Quicker but may introduce bias if signals have amplitude variation.</li> <li><code>none</code>: No normalization is applied. Raw amplitudes are used, which may be sensitive to trace scaling or noise.</li> </ul> </li> <li><code>reference_idx</code>: Index of the trace to use as the reference for correlation (typically one of the traces in the stream).</li> <li><code>strict</code>: If <code>True</code>, enforces strict alignment and compatibility between trace lengths and sampling rates.</li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'cross_correlate'\n    mode: 'full'\n    normalize: 'full'\n    reference_idx: 0\n    strict: True\n</code></pre>"},{"location":"processing_stream_config/#stack","title":"Stack","text":"<p>Stacks multiple traces using a specified method.</p> <ul> <li> <p><code>method</code>: Specifies the stacking method:</p> <ul> <li><code>linear</code>: Simple arithmetic mean across traces.</li> <li><code>pw</code>: Phase-weighted stacking, emphasizing coherent phase information. Order 1 is the parameter of this method. Recommended 1.</li> <li><code>root</code>: nth-root stack</li> </ul> </li> <li> <p><code>order</code>: The exponent applied in phase-weighted stacking. A higher value gives stronger weighting to coherent signals.</p> </li> </ul> <pre><code>  process_1:\n    name: 'stack'\n    method: 'pw'\n    order: 1\n</code></pre>"},{"location":"processing_stream_config/#synchronization","title":"Synchronization","text":"<p>In many processing workflows, it is necessary to synchronize all traces to a common reference point in time. This ensures consistent alignment for further processing steps such as stacking, correlation, or rotation.</p> <p>Two synchronization methods are supported:</p> <ul> <li> <p><code>starttime</code>: Aligns all traces to a common absolute start time as defined in their metadata. This method assumes all traces are recorded simultaneously and no additional time correction is needed.</p> </li> <li> <p><code>MCCC</code>: (Multi-Channel Cross-Correlation): Dynamically aligns traces based on their waveform similarity by computing relative delays. This method maximizes coherence across all channels and is especially effective in noisy conditions or when arrival times differ slightly due to path effects or instrument drift. The MCCC approach is based on: VanDecar, J. C., &amp; Crosson, R. S. (1990).</p> </li> </ul> <pre><code>Analysis:\n  process_1:\n    name: 'synch'\n    method: 'starttime' # or MCCC\n</code></pre>"},{"location":"processing_stream_config/#shift","title":"shift","text":"<p>This method applies temporal alignment to traces in a stream using one of several strategies. It is useful when aligning waveforms for stacking, correlation, or interpretation, and offers flexible options including theoretical models, observed picks, or manual time shifts. Please review traces header info to fully understand this method.</p> <p>phase_theo: Aligns traces based on theoretical arrival times. The expected arrival must be stored in trace.stats.geodetic['arrivals'] under the specified phase name. Use case: When synthetic or model-based timing is reliable (e.g., teleseismic body waves).</p> <ul> <li> <p><code>phase</code>: Aligns traces to observed picks, typically generated from prior picking routines or manual annotation. Pick information must be stored in trace.stats.picks. Use case: When you trust the quality of observed picks more than theoretical estimates.</p> </li> <li> <p><code>time_shifts</code>: Applies manual time shifts to each trace, using a list of per-trace time offsets in seconds (float). Use case: When you want full control over the alignment or need to apply empirical or experiment-specific shifts.</p> </li> </ul> <p>Note: If both phase and time_shifts are provided, phase takes precedence. If none of the options are valid, the stream is returned unchanged. </p> <pre><code>Analysis:\n  process_1:\n    name: 'shift'\n    phase_theo: 'P'\n    # Optional alternatives:\n    # phase: 'P'\n    # time_shifts: [0.1, -0.2, 0.05] in seconds\n</code></pre>"},{"location":"processing_stream_config/#concatenate","title":"Concatenate","text":"<p>This method concatenates multiple time-continuous trace segments within the same stream. It is commonly used to join split traces that were originally recorded as separate files or segments (e.g., due to daily files, instrument restarts, or transmission delays). Internally, it uses ObsPy's Stream.merge() functionality with:</p> <p>method=1: Linear interpolation for filling gaps</p> <p>fill_value='interpolate': Gaps are filled using interpolated values</p> <p>Behavior Overlapping traces are merged into a single continuous trace. Gaps are filled using linear interpolation between adjacent segments. Non-overlapping or incompatible traces (e.g. different IDs or sampling rates) are left unmerged.</p> <p>Note: Trace concatenation is automatically done when command is surfquake process or surfquake process_daily but not for surfquake quick for full control. </p> <pre><code>Analysis:\n  process_1:\n    name: 'concat'\n</code></pre>"},{"location":"project/","title":"Create your project","text":"<p>In surfQuake a project is simply a python object that can store in its attributes the path to valid seismogram files plus the associated metadata. This strategy allows to proceed with fast filters or join different projects. Project is the necessary input for the toolboxes Picker, Associator, Event Locator, Seismic Source and MTI.</p> <p>Here we will explain how the user can manage a project and be ready to proceed with the rest of toolboxes.</p>"},{"location":"project/#project-gui","title":"Project GUI","text":"<p>We start with the GUI. This is a screenshot of the Project GUI.</p> <p></p> <p>First, you need to choose between: </p> <ul> <li>Search files using Regular Expressions: Click in this button will open a window explorer to select the available files based on the filter edit line. In the example (.HHZ) and (EMUR*). Please set to blank space if you do not desire apply filters inside the window explorer. Then just select files and accept. The project will be automatically generated. </li> <li>Project Parth Files: This option is intendeed to let surfQuake search for valid seismogram files from a root folder in ahead. Optionally check Filter Time Spam and/or Filter Keys to include seismograms files that only fullfills the filter.</li> </ul> <p>Second:</p> <ul> <li>Save Project It is very remmendable to save the project. So, Proceed to give a name to the project and save it pressing Save Project for later using.</li> </ul> <p>Third:</p> <ul> <li>Load Project This action will open a window explorer so that you can select a project file previously saved and loaded. This will let you go ahead with the following toolboxes such as Picking Phases.</li> </ul> <p>Finally:</p> <ul> <li>Metadata: Metadata file with the in there is information structured as a dictionary nets/stations/channels. Personally, the best way to make your own metadata file is using either the java software PDCC or going to the API Station Management Portal. We also give a tool to make your metadata from a stations file and response files here.</li> </ul> <p></p>"},{"location":"project/#project-from-cli","title":"Project from CLI","text":""},{"location":"project/#overview","title":"Overview","text":"<p>This command allows you to create a seismic project, which is essentially a dictionary storing the paths to seismogram files along with their corresponding metadata.</p>"},{"location":"project/#usage","title":"Usage","text":"<pre><code>&gt;&gt;surfquake project -d [path to data files] -s [path to save directory] -n [project name] --verbose\n</code></pre>"},{"location":"project/#interactive-help","title":"Interactive help","text":"<pre><code>&gt;&gt; surfquake project -h\n</code></pre>"},{"location":"project/#create-project-example-from-cli","title":"Create Project example from CLI","text":"<p>In your termina, activate sufquake enviroment to have access to the commands. Then:</p> <pre><code>&gt;&gt; surfquake project -d /Volumes/LaCie/test_surfquake_core/testing_data -s /Volumes/LaCie/test_surfquake_core/testing_data/projects -n /surfquake_project_new.pkl --verbose\n</code></pre>"},{"location":"project/#project-from-library","title":"Project from library","text":""},{"location":"project/#classes","title":"Classes","text":"<p>In this section, we will explain the class SurfProject and we will explain how to manage your project from a simple example:</p> <p><code>SurfProject</code></p> <pre><code>class SurfProject:\n\n    def __init__(self, root_path: Union[str, List[str]]):\n\n        \"\"\"\n\n        SurfProject class is designed to be able to storage the path to seismograms\n        files plus the file metadata information (i.e. sampling_rate, starttime...)\n\n        Attributes:\n        - root_path (str): The root path to the folder where the user have the data files.\n\n        Methods:\n        - __init__(root_path): Initialize a new instance of MyClass.\n        - load_project(path_to_project_file: str): Load a project from a file storage in hard-drive\n        - save_project(path_file_to_storage: str): Saves a project as a pickle file in hard-drive\n        - search_files(verbose=True, **kwargs): Create a project. It can be used filters by nets,\n        stations, channels selection and/or filter by timestamp\n        - filter_project_keys(**kwargs): Filter a project (once is crated) using regular expressions.\n        \"\"\"\n</code></pre>"},{"location":"project/#methods","title":"Methods","text":"<p><code>search_files</code></p> <pre><code>    def search_files(self, format=\"NONE\", verbose=True, **kwargs):\n\n        \"\"\"\n        Args:\n\n        - verbose (bool): Description of arg1.\n        - nets (str): String with the name of selected nets to be filtered (i.e., \"WM,ES\")\n        - stations (str): String with the name of selected stations to be filtered (i.e., \"ARNO,UCM,EMAL\")\n        - channels (str): String with the name of selected channels to be filtered (i.e., \"HHN,HHZ,HHE\")\n        - starttime (str \"%Y-%m-%d %H:%M:%S\" ): String with the reference starttime, upper time spam threshold\n        (i.e.,\"2023-12-10 00:00:00\")\n        - endtime (str \"%Y-%m-%d %H:%M:%S\" ): String with the reference endtime, lower time spam threshold\n        (i.e.,\"2023-12-23 00:00:00\")\n\n        Returns:\n        - type: Description of the return value.\n        \"\"\"\n</code></pre> <p><code>filter_project_keys</code></p> <pre><code>    def filter_project_keys(self, **kwargs):\n\n        \"\"\"\n        Args:\n        - net (str): String with the name of selected nets to be filtered (i.e., \".\")\n        - station (str): String with the name of selected stations to be filtered (i.e., \"ARNO|UCM|EMAL\")\n        - channel (str): String with the name of selected channels to be filtered (i.e., \"HH.\")\n        \"\"\"\n</code></pre> <p><code>filter_project_time</code></p> <pre><code>    def filter_project_time(self, starttime: str, endtime: str):\n\n        \"\"\"\n        - starttime (str, \"%Y-%m-%d %H:%M:%S\"): String with the reference starttime, upper time spam threshold\n        (i.e., \"2023-12-10 00:00:00\")\n\n        - endtime (str, \"%Y-%m-%d %H:%M:%S\" ): String with the reference endtime, lower time spam threshold\n        (i.e., \"2023-12-23 00:00:00\")\n\n        \"\"\"\n</code></pre> <p><code>save_project</code></p> <pre><code>def save_project(self, path_file_to_storage: str)-&gt;bool\n# Saves the project object as a pickle file.\n</code></pre> <p><code>load_project</code></p> <pre><code>def load_project(path_to_project_file: str):\n</code></pre>"},{"location":"project/#attibutes","title":"Attibutes","text":"<pre><code>project :Dict\ndata_files :List\n</code></pre> <p>Next, the example of using this class and its methods. This example script is available in SurfQuakeCore/examples/manage_project_new.py</p> <pre><code>from multiprocessing import freeze_support\nfrom surfquakecore.project.surf_project import SurfProject\nimport time\n\npath_to_data = \"/Volumes/LaCie/test_surfquake_core/testing_data\"\npath_to_project = \"/Volumes/LaCie/test_surfquake_core/testing_data/projects/surfquake_project_new.pkl\"\n\nif __name__ == '__main__':\n\n    freeze_support()\n    sp = SurfProject(path_to_data)\n    #sp.search_files(starttime=\"2022-01-30 23:55:00\", endtime=\"2022-02-01 00:30:00\", stations=\"SALF,VALC\", channels=\"HHZ\")\n    sp.search_files(verbose=False)\n    #sp_original_project = copy.copy()\n    sp.filter_project_keys(station=\"SALF|VALC|CEST\")\n    sp_original1 = sp.copy()\n    sp_original1.filter_project_keys(station=\"SALF\")\n    sp_original2 = sp.copy()\n    sp_original2.filter_project_keys(station=\"VALC\")\n\n    sp_join = sp_original1 + sp_original2\n    print(\"With no filter\")\n    print(sp_join)\n    sp_join.filter_project_time(starttime=\"2022-01-30 23:55:00\", endtime=\"2022-02-01 00:30:00\")\n    print(\"With filter\")\n    print(sp_join)\n    sp_join.save_project(path_file_to_storage=path_to_project)\n    time.sleep(5)\n    sp_loaded = SurfProject.load_project(path_to_project_file=path_to_project)\n    print(sp_loaded)\n</code></pre> <p>The first step is create the object from the class SurfProject </p> <pre><code>sp = SurfProject\n</code></pre> <p>The necessary input to create the sp object is the root path where you have storage the seismogram files. Then, you can proceed to apply the method \"search_files\". This method includes the possibility of filter the inclusion of files inside the project.</p> <pre><code>sp.search_files(starttime=\"2022-01-30 23:55:00\", endtime=\"2022-02-01 00:30:00\", stations=\"SALF,VALC\", channels=\"HHZ\")\nprint(sp) # To see the contain of the project\n</code></pre> <p>Additionally, once the project has been created, you can also filter it by using \u00b4regular expressions\u00b4 net, station, channel using:</p> <p>Note: some util Regex info at Wiki Regex and Python keywords <pre><code>sp.filter_project_keys(station=\"SALF|VALC|CEST\")\n</code></pre></p> <p>or filterintg the time spam using:</p> <pre><code>sp_join.ilter_project_time(starttime=\"2022-01-30 23:55:00\", endtime=\"2022-02-01 00:30:00\")\n</code></pre> <p>Adding projects using \"+\" symbol</p> <pre><code>sp_original1 = sp.copy()\nsp_original1.filter_project_keys(station=\"SALF\")\nsp_original2 = sp.copy()\nsp_original2.filter_project_keys(station=\"VALC\")\n\nsp_join = sp_original1 + sp_original2\n</code></pre> <p>Finally, you can save the project by, <pre><code>sp_join.save_project(path_file_to_storage=path_to_project)\n</code></pre> and loading it</p> <pre><code>sp_loaded = SurfProject.load_project(path_to_project_file=path_to_project)\n</code></pre>"},{"location":"scripts/","title":"Custom User Scripts in SurfQuake","text":"<p>SurfQuake allows users to inject their own custom Python logic into waveform processing workflows. This functionality is available in both batch processing (<code>surfquake process</code>) and quick interactive workflows (<code>surfquake quick</code>).</p>"},{"location":"scripts/#purpose","title":"Purpose","text":"<p>User scripts provide a flexible, programmable interface to manipulate seismic traces beyond standard configuration options. You can implement your own algorythm in the workflow such as:</p> <ul> <li>Apply custom filtering, transformations, or corrections</li> <li>Modify or inspect waveform data (<code>tr.data</code>)</li> <li>Inject or update metadata in <code>tr.stats</code>. These scripts have direct access to each trace and its metadata (<code>trace.stats</code>), allowing you to inspect, modify, or annotate data based on custom logic. Be in mind the modified header of traces in surfquake.</li> <li>Run scientific algorithms (e.g., polarization, machine learning)</li> <li>Export trace summaries or diagnostics</li> </ul>"},{"location":"scripts/#script-structure","title":"Script Structure","text":"<p>Each script must define a <code>run(stream, **kwargs)</code> function that takes: - <code>stream</code>: an ObsPy <code>Stream</code> object containing the current event or waveform group - <code>kwargs</code>: optional keyword arguments, such as <code>inventory</code> or <code>event</code> info</p>"},{"location":"scripts/#example","title":"Example","text":"<pre><code>import numpy as np\n\ndef run(stream, **kwargs):\n    '''\n    User-defined post-processing hook for each event.\n\n    :param stream: obspy Stream\n    :param inventory: obspy Inventory (passed via kwargs)\n    :param event: dict with keys like origin_time, latitude, longitude, depth (passed via kwargs)\n    '''\n    inventory = kwargs.pop('inventory', None)\n    event = kwargs.pop('event', None)\n\n    if event is not None:\n        print(f\"Post-processing {len(stream)} traces from event at {event['origin_time']}\")\n    else:\n        print(f\"Post-processing {len(stream)} traces (no event info provided)\")\n\n    # Example: calculate global mean amplitude\n    all_data = np.hstack([tr.data for tr in stream])\n    print(\"Mean amplitude:\", np.mean(all_data))\n\n    # Example: apply a simple differencing operator to each trace\n    for tr in stream:\n        tr.data = np.diff(tr.data)\n\n    return stream\n</code></pre>"},{"location":"scripts/#usage-in-cli","title":"Usage in CLI","text":""},{"location":"scripts/#surfquake-process","title":"<code>surfquake process</code>","text":"<pre><code>surfquake process --config config.yaml --post_script myscript.py\n</code></pre> <ul> <li>Executes your script after all config steps are complete.</li> <li>Useful for event-level operations or header editing before writing to disk.</li> </ul>"},{"location":"scripts/#surfquake-quick","title":"<code>surfquake quick</code>","text":"<pre><code>surfquake quick \\\n  -w \"./data/*.mseed\" \\\n  -i inventory.xml \\\n  --post_script myscript.py \\\n  --post_script_stage before\n</code></pre> <ul> <li><code>--post_script</code>: Path to the user script.</li> <li><code>--post_script_stage</code>:</li> <li><code>before</code>: Runs the script before plotting (good for data transforms)</li> <li><code>after</code>: Runs after plotting (useful if user interaction modifies data)</li> </ul>"},{"location":"scripts/#best-practices","title":"Best Practices","text":"<ul> <li>Always return the modified stream from <code>run()</code>.</li> <li>Use <code>tr.stats</code> to store temporary or derived metadata.</li> <li>Print useful info for logging/debugging.</li> <li>Test your script independently before integrating it into batch workflows.</li> </ul>"},{"location":"scripts/#advanced-use","title":"Advanced Use","text":"<p>You may combine user scripts with processing configs to create hybrid workflows. For example, normalize traces with a config, then use a user script to detect and annotate phase arrivals using custom logic.</p> <pre><code># config.yaml\nAnalysis:\n  process_1:\n    name: 'normalize'\n</code></pre> <p>Then:</p> <pre><code>surfquake process --config config.yaml --user_script annotate_picks.py\n</code></pre> <p>This feature makes SurfQuake highly adaptable for both research and operational seismic processing pipelines.</p>"},{"location":"source/","title":"Source Parameters","text":"<p>The tool Source Parameters is designed to estimate events source parameters (i.e., Mw, ML, seismic moment, corner frequency, radiated energy, source size and stress drop) and the attenuation parameters (t-star, quality factor). It is based in the implementation of Satrinano et al., 2023. A detailed theoretical background can be found here</p>"},{"location":"source/#source-parameters-gui","title":"Source Parameters GUI","text":"<ul> <li>Time Window Parameters<ul> <li>P- and S-wave tolerance: Difference in seconds from the detected wave and the theroretical arrival time.</li> <li>noise_pre_time window: Window lenght before the first arrival time</li> <li>signal window: Window leght of signal to be analyse after first phase arrival time</li> <li>spectral window: Time window before the first arrival time</li> </ul> </li> <li>Source Parameters<ul> <li>Source Density: Density of the rock in the source.</li> <li>rpp: Radiation pattern coefficient for the P-wave</li> <li>rps: Radiation pattern coefficient for the S-wave</li> <li>Geometrical Spreading Correction: Spectra will be multiplied by this value to correct for the lost amplitude<ul> <li>r_power_n: geom_spread_n_exponent = 1 (default, body wave in a homogeneous full-space), 0.5 (surface wave in a homogeneous half-space)</li> <li>boatwright: \"r\" (body waves) geometrical spreading for hypocentral distances below a cutoff distance; frequency-dependent geometrical spreading above the cutoff distance (Boatwright et al., 2002).<ul> <li>Geometrical spreading cutoff distance: Geometrical spreading cutoff distance, in km, for the \"boatwright\" model</li> </ul> </li> </ul> </li> </ul> </li> <li>Local Magnitude: Local magnitude parameters: ml = log10(A) + a * log10(R/100) + b * (R-100) + c, where A is the maximum W-A amplitude (in mm) and R is the hypocentral distance (in km)</li> </ul>"},{"location":"source/#config-file","title":"Config file","text":"<pre><code># GENERAL PARAMETERS --------\n# All the fields are optional.\n# The filled in fields will be written to output files.\n# Author information\nauthor_name = SurfQuakeCore\nauthor_email = https://projectisp.github.io/ISP_tutorial.github.io/\n# Agency information\nagency_full_name = Spanish Navy Observatory\nagency_short_name = ROA\nagency_url = https://armada.defensa.gob.es/ArmadaPortal/page/Portal/ArmadaEspannola/cienciaobservatorio/prefLang-es/02InfoGeneral\n# -------- GENERAL PARAMETERS\n\n# TRACE AND METADATA PARAMETERS --------\n# Channel naming for mis-oriented channels (vertical, horiz1, horiz2):\n# Example:\n#   mis_oriented_channels = Z,1,2\nmis_oriented_channels = Z, 1, 2\n\n# Option to specify non standard instrument codes (e.g., \"L\" for accelerometer)\ninstrument_code_acceleration = None\ninstrument_code_velocity = None\n\n# For more complex network.station.location.channel (SCNL) naming scenarios,\n# you can provide a file, in json format, with traceid (SCNL) mapping\ntraceid_mapping_file = None\n\n# List of traceids to ignore.\n# Use network.station.location.channel; wildcards are accepted\n# Example:\n#   ignore_traceids = FR.CIEL.*.*, AM.RA0D3.00.*\nignore_traceids = None\n\n# List of traceids to use.\n# Use network.station.location.channel; wildcards are accepted\n# Example:\n#   use_traceids = FR.CIEL.*.*, AM.RA0D3.00.*\nuse_traceids = None\n\n# Epicentral distance ranges (km) to select stations to be processed.\n# Use a list of alternating min/max values, ex.:\n#   to only use stations between 0 and 100 km:\n#       epi_dist_ranges = 0, 100\n#   to avoid teleseismic distances between 14\u00c2\u00b0 (1300 km) and 29\u00c2\u00b0 (3200 km)\n#   where the P-wave undergoes travel time triplications:\n#       epi_dist_ranges = 0, 1300, 3200, 999999\n# Leave it to None to use all stations.\nepi_dist_ranges = 0, 400.0\n\n# Directory or single file name containing station metadata\n# (instrument response and station coordinates).\n# Note: this parameter can be overridden by the command line option\n#       with the same name.\n# Station metadata files can be in one of the following formats:\n#   StationXML, dataless SEED, SEED RESP, PAZ (SAC polezero format)\n# Notes:\n# 1. SourceSpec will not enter in subdirectories of the given directory\n#    (only one level allowed)\n# 2. Traceid for PAZ files is specified through their name.\n#    The traceid (network.station.location.channel) must be in the last four\n#    fields (separated by a dot \".\") before the file suffix (which can be\n#    \".paz\", \".pz\", or no suffix).\n#    Example:\n#      PREFIX.NET.STA.LOC.CHAN.paz\n#    or (no prefix):\n#      NET.STA.LOC.CHAN.paz\n#    or (no prefix and no suffix):\n#      NET.STA.LOC.CHAN\n# 3. If no traceid is specified through the PAZ file name, then it is assumed\n#    that this is a generic PAZ, valid for all the stations that do not have\n#    a specific PAZ. Use \"trace_units\" below to specify the units of the\n#    generic PAZ.\n# 4. SEED RESP and PAZ files do not contain station coordinates, which\n#    should therefore be in the trace header (traces in SAC format)\n#station_metadata = inventory.xml\n\n# It is also possible to provide a constant sensitivity (i.e., flat instrument\n# response curve) as a numerical value or a combination of SAC header fields\n# (in this case, traces must be in SAC format).\n# This parameter overrides the response curve computed from station_metadata.\n# Leave it to None to compute instrument response from station_metadata.\n# Examples:\n#  sensitivity = 1\n#  sensitivity = 1e3\n#  sensitivity = resp0\n#  sensitivity = resp1*resp2\n#  sensitivity = user3/user2\nsensitivity = None\n\n# SQLite database file for storing output parameters (optional):\ndatabase_file = source_spec.sqlite\n\n# Correct_instrumental_response (optional, default=True):\ncorrect_instrumental_response = True\n\n# Trace units.\n# Leave it to 'auto' to let the code decide, based on instrument type.\n# Manually set it to 'disp', 'vel' or 'acc' if you have already preprocessed\n# the traces.\ntrace_units = auto\n# -------- TRACE AND METADATA PARAMETERS\n\n\n# TIME WINDOW PARAMETERS --------\n# P and S wave velocity (in km/s) for travel time calculation\n# (if None, the global velocity model 'iasp91' is used)\n# Theoretical P or S arrival times are used when a manual P or S pick is not\n# available, or when the manual P or S pick is too different from the\n# theoretical arrival (see 'p_arrival_tolerance' and 's_arrival_tolerance'\n# below).\nvp_tt = None\nvs_tt = None\n# As an alternative, a directory containing NonLinLoc travel time grids\n# can be specified and values defined above will be ignored.\n# Note that reading NonLinLoc grids takes time. For simple 1D models, you\n# can speed up considerably the process using a generic station\n# named \"DEFAULT\". The coordinates of this default station are not important,\n# since they will be superseded by each station's coordinates.\nNLL_time_dir = None\n\n# Arrival tolerances (in seconds) to accept a manual P or S pick\np_arrival_tolerance = 8.0\ns_arrival_tolerance = 8.0\n\n# Start time (in seconds) of the noise window, respect to the P arrival time\nnoise_pre_time = 15.0\n\n# Start time (in seconds) of the signal window, respect to the P or S arrival\n# times (see \"wave_type\" below)\nsignal_pre_time = 1.0\n\n# Length (in seconds) for both noise and signal windows\nwin_length = 10.0\n# -------- TIME WINDOW PARAMETERS\n\n\n# SPECTRUM PARAMETERS --------\n# Wave type to analyse: 'P', 'S', 'SH' or 'SV'\n# If 'SH' or 'SV' are selected, traces are rotated in the radial-transverse\n# system. Transverse component is used for 'SH', radial component (and\n# optionally the vertical component, see 'ignore_vertical' below) is used\n# for 'SV'\nwave_type = S\n\n# Integrate in time domain (default: integration in spectral domain)\ntime_domain_int = False\n\n# Ignore vertical components when building S or SV spectra\n# Note: this option has no effect when 'wave_type' is 'P' (the vertical\n# component is not ignored) and when 'wave_type' is 'SH' (the vertical\n# component is not needed)\nignore_vertical = False\n\n# Taper half width: between 0 (no taper) and 0.5\ntaper_halfwidth = 0.05\n\n# Spectral window length (seconds)\n# Signal is tapered, and then zero padded to\n# this window length, so that the spectral\n# sampling is fixed to 1/spectral_win_length.\n# Comment out (or set to None) to use\n# signal window as spectral window length.\nspectral_win_length = None\n\n# Spectral smoothing window width in frequency decades\n# (i.e., log10 frequency scale).\n# Example:\n#  spectral_smooth_width_decades=1 means a width of 1 decade\n#  (generally, too large, producing a spectrum which is too smooth).\n#  spectrum(f0) is smoothed using values between f1 and f2, so that\n#  log10(f1)=log10(f0)-0.5 and log10(f2)=log10(f0)+0.5\n#    i.e.,\n#  f1=f0/(10^0.5) and f2=f0*(10^0.5)\n#    or,\n#  f2/f1=10 (1 decade width)\n# Default value of 0.2 is generally a good choice\nspectral_smooth_width_decades = 0.2\n\n# Residuals file path\n# (a pickle file with the mean residuals per station,\n# used for station correction):\nresiduals_filepath = None\n\n# Remove the signal baseline after instrument correction and before filtering\nremove_baseline = False\n\n# Band-pass frequencies (Hz) for accelerometers, velocimeters\n# and displacement sensors.\n# Use bp_freqmin_STATION and bp_freqmax_STATION to provide\n# filter frequencies for a specific STATION code.\n# TODO: calculate from sampling rate?\nbp_freqmin_acc = 0.001\nbp_freqmax_acc = 50.0\nbp_freqmin_shortp = 0.01\nbp_freqmax_shortp = 40.0\nbp_freqmin_broadb = 0.001\nbp_freqmax_broadb = 40.0\nbp_freqmin_disp = 0.5\nbp_freqmax_disp = 40.0\n\n# Spectral windowing frequencies (Hz) for accelerometers, velocimeters\n# and displacement sensors.\n# (spectra will be cut between these two frequencies)\n# Use freq1_STATION and freq2_STATION to provide\n# windowing frequencies for a specific STATION code.\nfreq1_acc = 0.05\nfreq2_acc = 30.0\nfreq1_shortp = 0.05\nfreq2_shortp = 30.0\nfreq1_broadb = 0.05\nfreq2_broadb = 30.0\nfreq1_disp = 0.5\nfreq2_disp = 30.0\n# -------- SPECTRUM PARAMETERS\n\n\n# SIGNAL/NOISE PARAMETERS --------\n# Minimum rms (in trace units before instrument corrections)\n# to consider a trace as noise\nrmsmin = 0.0\n\n# Time domain S/N ratio min\nsn_min = 1.0\n\n# Clipping detection algorithm\n# Options:\n#  - 'none': no clipping detection\n#  - 'clipping_score': compute a clipping score for each trace, based on the\n#    shape of the kernel density estimation of the trace amplitude values.\n#    A high clipping score will be obtained for traces with a high number of\n#    samples whose amplitude is close to the trace highest or lowest\n#    amplitude values. Clipping scores for each trace are printed on the\n#    terminal and in the log file.\n#    Note: if \"remove_baseline\" is True (see above), clipping scores are\n#    computed on the baseline-corrected signal.\n#  - 'clipping_peaks': count the number of peaks in the kernel density\n#    estimation of the trace amplitude values. The trace is considered clipped\n#    if at least one peak is found within the trace highest or lowest amplitude\n#    values. Kernel density peaks for each trace are printed on the terminal\n#    and in the log file.\nclipping_detection_algorithm = clipping_score\n# Plot a debug figure for each trace with the results of the clipping algorithm\n# Note: the figures are always shown, even if \"plot_show\" is False (see below)\nclipping_debug_plot = False\n# Threshold for the 'clipping_score' algorithm (between 0 and 100).\n# A value of 100 means no clipping detection.\n# This parameter is ignored if \"clipping_detection_algorithm\" is not set to\n# 'clipping_score'.\nclipping_score_threshold = 10.0\n# Sensitivity for the 'clipping_peaks' algorithm (between 1 and 5).\n# Higher values mean more peaks are detected.\n# This parameter is ignored if \"clipping_detection_algorithm\" is not set to\n# 'clipping_peaks'.\n#clipping_peaks_sensitivity = 3\n# Trace amplitude percentile for the 'clipping_peaks' algorithm (between 0\n# and 100). Example:\n#   clipping_peaks_percentile = 10\n# means that the 10% highest and lowest values of the trace amplitude will be\n# checked for clipping.\n# A value of 0 means that no clipping check will be performed.\n# This parameter is ignored if \"clipping_detection_algorithm\" is not set to\n# 'clipping_peaks'.\n#clipping_peaks_percentile = 10.0\n\n# Maximum gap length for the whole trace, in seconds\ngap_max = None\n# Maximum overlap length for the whole trace, in seconds\noverlap_max = None\n\n# Spectral S/N ratio min, below which a spectrum will be skipped\nspectral_sn_min = 10.0\n# Frequency range (Hz) to compute the spectral S/N ratio\n# (comment out or use None to indicate the whole frequency range)\n# Example:\n#  spectral_sn_freq_range = 0.1, 2\nspectral_sn_freq_range = None\n# -------- SIGNAL/NOISE PARAMETERS\n\n\n# SPECTRAL MODEL PARAMETERS --------\n# Layer top depths (km, positive down), for layered models (see below)\n#  Note: generally, the first layer top depth should be 0 or a negative value\n#layer_top_depths = 0, 3\n# P and S wave velocity close to the source (km/s)\n# It can be a single value or a list of values (layered model)\n# Set to None to use velocity from the global Earth model 'iasp91'\n#   Note: specifying a layered model is useful when the same config file is\n#   used for several SourceSpec runs with sources at different depths\n#vp_source = 4.5, 5.5\n#vs_source = 2.5, 3.2\nvp_source = 5.0\nvs_source = 2.9\n# P and S wave velocity close to the stations (km/s)\n# If set to None, velocity values close to the source will be used\nvp_stations = 4.5\nvs_stations = 2.5\n# As an alternative, a directory containing a NonLinLoc velocity model can be\n# specified. In this case, the values provided above will be ignored\nNLL_model_dir = None\n# Density close to the source (kg/m3)\n# It can be a single value or a list of values (layered model)\n# Set to None to use density from the global Earth model 'iasp91'\n#   Note: specifying a layered model is useful when the same config file is\n#   used for several SourceSpec runs with sources at different depths\n#rho_source = 2400, 2500\nrho_source = 2400\n# Density close to the stations (kg/m3)\n# If set to None, density value close to the source will be used\n#rho_stations = 2400.0\n# Geometrical spreading correction of wave amplitude.\n# Spectra will be multiplied by this value to correct for the lost amplitude.\n# Possible options are:\n#    'r_power_n':   \"r\" to the power of \"n\" (r\u00e2\u0081\u00bf).\n#                   You must provide the value of the exponent \"n\"\n#                   (see \"geom_spread_n_exponent\" below).\n#    'boatwright':  \"r\" (body waves) geometrical spreading for hypocentral\n#                   distances below a cutoff distance; frequency-dependent\n#                   geometrical spreading above the cutoff distance (Boatwright\n#                   et al., 2002). You must provide the cutoff distance (see\n#                   \"geom_spread_cutoff_distance\" below). This coefficient can\n#                   be a valid choice for regional distances (up to 200 km),\n#                   where S-waves, Lg waves and surface waves are mixed.\ngeom_spread_model = r_power_n\n# Exponent \"n\" for the \"r_power_n\" geometrical spreading coefficient (positive\n# float). Examples:\n#   geom_spread_n_exponent = 1 (default, body wave in a homogeneous full-space)\n#   geom_spread_n_exponent = 0.5 (surface wave in a homogeneous half-space)\ngeom_spread_n_exponent = 1.0\n# Geometrical spreading cutoff distance, in km, for the \"boatwright\" model:\ngeom_spread_cutoff_distance = 100.0\n# Minimum distance (in km) to use a teleseismic geometrical spreading\n# model. Above this distance, the model from Okal (1992) for body waves\n# spreading in a spherically symmetric Earth will be used.\ngeom_spread_min_teleseismic_distance = 500.0\n# P-wave average radiation pattern coefficient:\nrpp = 0.52\n# S-wave average radiation pattern coefficient:\nrps = 0.62\n# Radiation pattern coefficient from focal mechanism, if available.\n#   Note: radiation pattern is computed for the first arriving phase and might\n#   not be correct for windows involving multiple phase arrivals (e.g.,\n#   Lg waves, surface waves at regional distances, depth phases at teleseismic\n#   distances)\nrp_from_focal_mechanism = False\n# \"kp\" and \"ks\" coefficients to compute source radius a from the P-wave\n# corner frequency fc_p or the S-wave corner frequency fc_s and the shear\n# wave speed beta (\"vs_source\"):\n#\n#   a = kp * beta / fc_p\n#   a = ks * beta / fc_s\n#\n# (Madariaga, 2009; Kaneko and Shearer, 2014)\n#\n# The default value for S-waves is \"ks = 0.3724\", obtained by Brune (1970)\n# for a static circular crack.\n# Other values are discussed in Kaneko and Shearer (2014) for a dynamic\n# circular crack, as a function of the ratio Vr/beta, where Vr is the rupture\n# speed:\n#\n#   Vr/beta  kp(K&amp;S)   ks(K&amp;S)   kp(Mada)   ks(Mada)   kp(S&amp;H)   ks(S&amp;H)\n#   0.9      0.38      0.26      0.32       0.21       0.42      0.29\n#   0.8      0.35      0.26                            0.39      0.28\n#   0.7      0.32      0.26                            0.36      0.27\n#   0.6      0.30      0.25                            0.34      0.27\n#   0.5      0.28      0.22                            0.31      0.24\n#\n#   K&amp;S: Kaneko and Shearer (2014)\n#   Mada: Madariaga (1976)\n#   S&amp;H: Sato and Hirasawa (1973)\n#kp = 0.38\n#ks = 0.3724\n# -------- SPECTRAL MODEL PARAMETERS\n\n\n# INVERSION PARAMETERS --------\n# Weighting type: 'noise', 'frequency', 'inv_frequency' or 'no_weight'\n#   'noise':         spectral signal/noise ratio weighting\n#   'frequency':     a constant weight is applied for f&lt;=f_weight\n#                    a weight of 1 is used for f&gt;f_weight\n#                    (see \"f_weight\" and \"weight\" below)\n#   'inv_frequency': weight is computed as 1/(f-f0+0.25)**0.25 for f&lt;=f1,\n#                    weight is 0 for f&lt;f0 and f&gt;f1.\n#                    f0 and f1 are the first and last frequencies where\n#                    spectral signal/noise ratio is above 3, or the first and\n#                    last frequencies of the entire spectrum if no noise window\n#                    is available\n#   'no_weight':     no weighting\nweighting = noise\n# Parameters for 'frequency' weighting (ignored for the other weighting types):\n#   weight for f&lt;=f_weight (Hz)\n#   1      for f&gt; f_weight (Hz)\nf_weight = 7.0\nweight = 10.0\n\n# Inversion algorithm:\n# TNC: truncated Newton algorithm (with bounds)\n# LM: Levenberg-Marquardt algorithm\n# (warning: Trust Region Reflective algorithm will be used instead if\n#  bounds are provided)\n# BH: basin-hopping algorithm\n# GS: grid search\n# IS: importance sampling of misfit grid, using k-d tree\ninv_algorithm = GS\n\n# Mw initial value and bounds.\n# Set to True to use the magnitude (or scalar moment) from event file as\n# initial Mw value for the inversion, instead of computing it from the average\n# of the spectral plateau.\n# If the event file does not contain a magnitude value or a scalar moment,\n# then this parameter is ignored\n# Mw_0_from_event_file = False\n# Allowed variability for Mw in the inversion\n# (expressed as a fraction of Mw_0, between 0 and 1).\n# This parameter is interpreted differently, depending on whether\n# Mw_0_from_event_file is True or False:\n#   - If Mw_0_from_event_file is True, then Mw_variability is interpreted as\n#     the allowed variability around the Mw value provided in the event file.\n#   - If Mw_0_from_event_file is False, then the Mw bounds are defined as\n#       Mw_min = min(Mw(f))*(1-Mw_0_variability)\n#       Mw_max = max(Mw(f))*(1+Mw_0_variability),\n#     where Mw(f) is the low frequency spectral plateau in magnitude units.\n#     If noise weighting is used, frequencies for which\n#     S/N(f) &lt; 0.5*max(S/N(f)) will be ignored, where S/N(f) is the spectral\n#     signal to noise ratio.\nMw_0_variability = 0.1\n\n# Bounds for fc (Hz)\n# Specify bounds as a list, ex.:\n#   fc_min_max = 0.1, 40\n# Note:\n#    If not specified, fc bounds will be autoset to fc0/10 and fc0*10, i.e. two\n#    decades around fc0. The value of fc0 is set as the first maximum of\n#    spectral S/N (noise weighting), or at \"f_weight\" (frequency weighting),\n#    or at frequency where weight is 30% below the maximum (inverse-frequency\n#    weighting) or at half of the frequency window (no weighting)\nfc_min_max = 0.05, 50.0\n\n# Initial value and bounds for t_star (seconds)\nt_star_0 = 0.045\n# Try to invert for t_star_0.\n# If False, then the fixed t_star_0 defined above will be used.\n# If the inverted t_star_0 is non-positive, then fixed t_star_0 will be used\ninvert_t_star_0 = False\n# Allowed variability around inverted t_star_0 in the inversion\n# (expressed as a fraction of t_star_0, between 0 and 1).\n# If the inverted t_star_0 is non-positive, then t_star_min_max is used\n# (see below).\nt_star_0_variability = 0.1\n# t_star_min_max does not supersede t_star_0_variability\nt_star_min_max = 0.001, 0.2\n# optional : Qo bounds (converted into t_star bounds in the code).\n# (comment out or use None to indicate no bound)\n# Note: if you want to explore negative t_star values, you have to specify\n# -Qo_min, Qo_min. This because t_star is proportional to 1/Qo.\n# Example, for searching only positive t_star values:\n#   Qo_min_max = 10, 1000\n# If you want to search also negative t_star values:\n#   Qo_min_max = -10, 10\nQo_min_max = None\n# -------- INVERSION PARAMETERS\n\n# POST-INVERSION PARAMETERS --------\n# Post-inversion bounds: use this bounds to reject certain inversion\n# results, per station.\n# Sometimes it is better to be more permissive with inversion parameters and\n# reject \"bad\" solutions after the inversion, rather than forcing the\n# inversion to converge within strict bounds.\n# fc bounds, in Hz\npi_fc_min_max = None\n# t_star bounds, in s\npi_t_star_min_max = None\n# Static stress drop bounds, in MPa\n# pi_ssd_min_max = None\n# Maximum acceptable misfit between inverted and observed spectrum\npi_misfit_max = None\n# -------- POST-INVERSION PARAMETERS\n\n\n# RADIATED-ENERGY PARAMETERS --------\n# Maximum frequency (Hz) to measure radiated energy Er\n# Set it to None to use the whole spectrum, i.e. up to the \"freq2_*\"\n# windowing frequency (see above).\n# The finite-band correction of Di Bona &amp; Rovelli (1988) will be applied\n# to account for the missing energy above the maximum frequency.\nmax_freq_Er = None\n# -------- RADIATED-ENERGY PARAMETERS\n\n\n# LOCAL MAGNITUDE PARAMETERS --------\ncompute_local_magnitude = True\n# Local magnitude parameters:\n#   ml = log10(A) + a * log10(R/100) + b * (R-100) + c\n# where A is the maximum W-A amplitude (in mm)\n# and R is the hypocentral distance (in km)\n# Default values (for California) are:\n#   a = 1., b = 0.00301, c = 3.\na = 1.0\nb = 0.00301\nc = 3.0\n# Band-pass filtering frequencies (Hz) for local magnitude\nml_bp_freqmin = 0.1\nml_bp_freqmax = 20.0\n# -------- LOCAL MAGNITUDE PARAMETERS\n\n# AVERAGES PARAMETERS --------\n# Reject outliers before averaging, using the IQR method.\n# IQR is the interquartile range Q3-Q1, where Q1 is the 25% percentile\n# and Q3 is the 75% percentile.\n# Values that are smaller than (Q1 - nIQR*IQR) or larger than (Q3 + nIQR*IQR)\n# will be rejected as outliers.\n# Set nIQR to None to disable outlier rejection.\n# Note: this parameter also controls the position of \"whiskers\" on the source\n# parameter box plots.\nnIQR = 1.5\n# -------- AVERAGES PARAMETERS\n\n\n# PLOT PARAMETERS --------\n# Show interactive plots (slower)\nplot_show = False\n# Save plots to disk\nplot_save = True\n# Plot file format: 'png', 'pdf' or 'pdf_multipage'\nplot_save_format = png\n# Plots an extra synthetic spectrum with no attenuation\nplot_spectra_no_attenuation = True\n# Plots an extra synthetic spectrum with no fc\nplot_spectra_no_fc = False\n# Max number of rows in plots\nplot_spectra_maxrows = 3\nplot_traces_maxrows = 3\n# Plot ignored traces (low S/N)\nplot_traces_ignored = True\n# Plot ignored spectra (low S/N)\nplot_spectra_ignored = True\n# Plot station map\nplot_station_map = False\n# Plot station names on map\nplot_station_names_on_map = True\n# Text size for station names\nplot_station_text_size = 8.0\n# Coastline resolution\n# Use None to let the code autoset the coastline resolution.\n# Otherwhise choose one of: 'full', 'high', 'intermediate', 'low' or 'crude'\nplot_coastline_resolution = None\n# Zoom level for map tiles\n# Use None to let the code autoset the zoom level\n# Otherwhise choose an integer beteen 1 (minimum zoom) and 18 (maximum zoom)\n# Note: for zoom levels larger than 11, some map tiles could be missing\nplot_map_tiles_zoom_level = None\n# -------- PLOT PARAMETERS\n\n\n# HTML REPORT --------\n# Generate an HTML page summarizing the results of this run\nhtml_report = True\n# Link to event page. If set, the event ID on the HTML page will be a link to\n# the event page. Use $EVENTID to indicate the current event ID.\n# Example:\n#   event_url = https://earthquake.usgs.gov/earthquakes/eventpage/$EVENTID/executive\nevent_url = https://projectisp.github.io/ISP_tutorial.github.io//$EVENTID\n# -------- HTML REPORT\n\n\n# QUAKEML PARAMETERS ----------------\n# Parameters for QuakeML output.\n#\n# A QuakeML file will be generated only if QuakeML is used for input.\n# The output file will be based on the input file, with additional information\n# on seismic moment, Mw and source parameters computed by SourceSpec.\n# Note: if you don't understand the parameters below, then probably you\n# don't need QuakeML output and you can leave all the parameters to their\n# default value\n\n# Set SourceSpec Mw as preferred\nset_preferred_magnitude = True\n# Base for all the object ids (smi)\nsmi_base = smi:local\n# String to strip from the Origin id when constructing the\n# Magnitude and stationMagnitude ids.\nsmi_strip_from_origin_id = \"\"\n# Template for the Magnitude object id (smi).\n# Use $SMI_BASE to indicate smi_base defined above\n# Use $ORIGIN_ID to indicate the id of the associated Origin.\nsmi_magnitude_template = \"$SMI_BASE/Magnitude/Origin/$ORIGIN_ID#sourcespec\"\n# Template for the stationMagnitude object id (smi).\n# Use $SMI_BASE to indicate smi_base defined above\n# Use $ORIGIN_ID to indicate the id of the associated Origin.\n# Use $SMI_MAGNITUDE_TEMPLATE to reuse the template for Magnitude object\n# Use $WAVEFORM_ID to indicate the id of the associated waveform.\nsmi_station_magnitude_template = \"$SMI_MAGNITUDE_TEMPLATE#$WAVEFORM_ID\"\n# Template for the MomentTensor object id (smi) which is used to store\n# the scalar moment value.\n# Use $SMI_BASE to indicate smi_base defined above\n# Use $ORIGIN_ID to indicate the id of the associated Origin.\nsmi_moment_tensor_template = \"$SMI_BASE/MomentTensor/Origin/$ORIGIN_ID#sourcespec\"\n# Template for the FocalMechanism object id (smi) which is used to store\n# the scalar moment value.\n# Use $SMI_BASE to indicate smi_base defined above\n# Use $ORIGIN_ID to indicate the id of the associated Origin.\nsmi_focal_mechanism_template = \"$SMI_BASE/FocalMechanism/Origin/$ORIGIN_ID#sourcespec\"\n# -----------------QUAKEML PARAMETERS\n</code></pre>"},{"location":"source/#source-parameters-from-cli","title":"Source Parameters from CLI","text":""},{"location":"source/#usage","title":"Usage","text":"<pre><code>&gt;&gt; surfquake source parameters estimation [-h] -i INVENTORY_FILE_PATH -p PROJECT_FILE_PATH -c CONFIG_FILE_PATH -l LOC_FILES_PATH [-t] -o OUTPUT_DIR_PATH\n</code></pre>"},{"location":"source/#interactive-help","title":"Interactive help","text":"<pre><code>&gt;&gt; surfquake source -h\n</code></pre> <pre><code>&gt;&gt; surfquake source -i /surfquake_test/metadata/inv_all.xml -p /surfquake_test/project/surfquake_project.pkl -c /surfquake_test/config_files/source_spec.conf -l /surfquake_test/test_nll_final/loc -o /surfquake_test/test_source_final\n</code></pre>"},{"location":"source/#source-parameters-from-library","title":"Source Parameters from Library","text":""},{"location":"source/#classes","title":"Classes","text":"<p><code>Automag</code></p> <pre><code>class Automag:\n\n    def __init__(self, project: SurfProject, locations_directory: str, inventory_path, source_config: str,\n                 output_directory: str, scale: str, gui_mod=None):\n\n        \"\"\"\n        Manage SourceSpec program to estimate source parameters.\n        :param project: SurfProject object.\n        :param inventory_path: Path to metadata file.\n        :param source_config: Path to source config file.\n        :param output_directory: Path to output folder.\n        :param scale: if regional waveforms will cut with small adapted time windows, else will be cut with a \n        long time window\n        \"\"\"\n</code></pre> <p><code>ReadSource</code> <pre><code>class ReadSource:\n    def __init__(self, root_path_to_output: str):\n        \"\"\"\n        The class methods are designed to scan the output of sourcespec\n        root_path_to_output: Root path where sourcespec output is expected\n        \"\"\"\n        self.root_path_to_output = root_path_to_output\n        self.obsfiles = []\n</code></pre></p>"},{"location":"source/#methods","title":"Methods","text":"<p><code>estimate_source_parameters</code> <pre><code># Automag instance method\ndef estimate_source_parameters(self):\n    # Loop over loc folder files and run source parameters estimation\n</code></pre></p> <p><code>generate_source_summary</code> <pre><code># ReadSource instance method\ndef generate_source_summary(self):\n\n    \"\"\"\n    # Generate source parameters summary as dataframe\n    :return List: List of dictionaries containing source parameters\n    \"\"\"\n</code></pre></p> <p><code>write_summary</code> <pre><code># ReadSource instance method\ndef write_summary(self, summary: list, summary_path: str):\n    \"\"\"\n    Transform the summary into txt file using Pandas Dataframe\n    :param summary: List of dictionaries containing source parameters\n    :param summary_path: path to file output including the name of the file.\n    \"\"\"\n</code></pre></p>"},{"location":"source/#example-using-library","title":"Example using library","text":"<pre><code>from surfquakecore.magnitudes.run_magnitudes import Automag\nfrom surfquakecore.magnitudes.source_tools import ReadSource\nfrom surfquakecore.project.surf_project import SurfProject\nimport os\n\nif __name__ == \"__main__\":\n\n    cwd = os.path.dirname(__file__)\n    ## Project definition ##\n    path_to_project = \"/test_surfquake_core/project\"\n    project_path_file = os.path.join(path_to_project, \"surfquake_project.pkl\")\n    print(\"project:\", project_path_file)\n\n    # load the project\n    sp_loaded = SurfProject.load_project(path_to_project_file=project_path_file)\n    print(sp_loaded)\n\n    # Basic input: working_directory, inventory file path and config_file input\n    working_directory = os.path.join(cwd, \"source_estimations\")\n    inventory_path = os.path.join(working_directory, \"inventories\", \"inv_surfquakecore.xml\")\n    path_to_configfiles = os.path.join(working_directory, \"config/source_spec.conf\")\n    locations_directory = os.path.join(working_directory, \"locations\")\n    output_directory = os.path.join(working_directory, \"output\")\n    summary_path = '/Users/roberto/Documents/SurfQuakeCore/examples/source_estimations/source_summary.txt'\n\n    # Running stage\n    mg = Automag(sp_loaded, locations_directory, inventory_path, path_to_configfiles, output_directory, \"regional\")\n    mg.estimate_source_parameters()\n\n    # Now we can read the output and even write a txt summarizing the results\n    rs = ReadSource(output_directory)\n    summary = rs.generate_source_summary()\n    rs.write_summary(summary, summary_path)\n</code></pre>"},{"location":"spectral_analysis_documentation/","title":"Processing Options","text":""},{"location":"spectral_analysis_documentation/#spectral-analysis-tools","title":"Spectral Analysis Tools","text":"<p>SurfQuake provides three main tools for spectral analysis of seismic data: spectrum, spectrogram, and CWT (Continuous Wavelet Transform). These tools enable users to inspect frequency content in different forms\u2014static, time-evolving, and high-resolution wavelet representations.</p>"},{"location":"spectral_analysis_documentation/#spectrum","title":"Spectrum","text":"<p>The <code>spectrum</code> tool computes the frequency-domain representation of a single seismic trace using the multitaper method by default, which is robust against noise and spectral leakage.</p>"},{"location":"spectral_analysis_documentation/#parameters","title":"Parameters","text":"<ul> <li><code>method</code>: Spectral estimation method. Options:</li> <li><code>multitaper</code> (default)</li> <li><code>fft</code> or others if extended</li> </ul>"},{"location":"spectral_analysis_documentation/#config-example","title":"Config Example","text":"<pre><code>Analysis:\n  process_1:\n    name: 'spectrum'\n    method: 'multitaper' # 'fft' for conventional spectrum with single taper\n    output_path: /Users/surf_test/spectral\n</code></pre>"},{"location":"spectral_analysis_documentation/#output","title":"Output","text":"<ul> <li>Amplitude vs. Frequency plot</li> <li>Optional log-log or semi-log axes (<code>loglog</code>, <code>xlog</code>, <code>ylog</code>)</li> <li>Serialized result saved as <code>.sp</code></li> </ul>"},{"location":"spectral_analysis_documentation/#spectrogram","title":"Spectrogram","text":"<p>The <code>spectrogram</code> method produces a time-frequency representation of a seismic trace using short-time Fourier Transform (STFT).</p>"},{"location":"spectral_analysis_documentation/#parameters_1","title":"Parameters","text":"<ul> <li><code>win</code> (float): Window length in seconds (e.g., 5.0)</li> <li><code>overlap_percent</code> (float): Percent overlap between windows (e.g., 50.0)</li> <li><code>linf</code>, <code>lsup</code>: Lower and upper frequency limits for plotting</li> </ul>"},{"location":"spectral_analysis_documentation/#config-example_1","title":"Config Example","text":"<pre><code>Analysis:\n  process_1:\n    name: 'spectrogram'\n    win: 5.0\n    overlap_percent: 50.0\n    linf: 0 # optional\n    lsup: 20 # optional\n    output_path: /Users/surf_test/spectral\n</code></pre>"},{"location":"spectral_analysis_documentation/#output_1","title":"Output","text":"<ul> <li>Power over time and frequency in decibels (dB)</li> <li>Plotted as a color-coded matrix</li> <li>Serialized result saved as <code>.spec</code></li> </ul>"},{"location":"spectral_analysis_documentation/#continuous-wavelet-transform-cwt","title":"Continuous Wavelet Transform (CWT)","text":"<p>CWT provides high-resolution time-frequency analysis using scalable wavelets. Ideal for transient signals and high-precision timing.</p>"},{"location":"spectral_analysis_documentation/#parameters_2","title":"Parameters","text":"<ul> <li><code>wavelet_type</code>: </li> <li><code>\"cm\"</code>: Complex Morlet (default and recommended for phase coherence)</li> <li><code>\"mh\"</code>: Mexican Hat</li> <li><code>\"pa\"</code>: Paul wavelet</li> <li><code>param</code>: Wavelet parameter (e.g., 6.0)</li> <li><code>fmin</code>, <code>fmax</code>: Frequency range</li> <li><code>nf</code>: Number of frequency bins</li> </ul>"},{"location":"spectral_analysis_documentation/#config-example_2","title":"Config Example","text":"<pre><code>Analysis:\n  process_1:\n    name: 'cwt'\n    wavelet_type: 'cm'\n    param: 6.0\n    fmin: 0.5 # optional\n    fmax: 20.0 #optional\n    nf: 80\n    output_path: /Users/surf_test/spectral\n</code></pre>"},{"location":"spectral_analysis_documentation/#output_2","title":"Output","text":"<ul> <li>Scalogram in decibel power</li> <li>Prediction masks showing reliable analysis range</li> <li>Serialized result saved as <code>.cwt</code></li> </ul>"},{"location":"spectral_analysis_documentation/#plotting-and-cli-usage","title":"Plotting and CLI Usage","text":"<p>All spectral results can be visualized via the CLI:</p> <pre><code># Plot a spectrum\nsurfquake specplot --file path/to/file.sp\n\n# Plot a spectrogram\nsurfquake specplot --file path/to/file.spec \n\n# Plot a wavelet scalogram with optional clipping\nsurfquake specplot --file path/to/file.cwt --type cwt --clip -120\n\n# Save output to file\nsurfquake specplot -f file.spec --save_path output.png\n</code></pre>"},{"location":"spectral_analysis_documentation/#cli-options","title":"CLI Options","text":"<ul> <li><code>--file</code>, <code>-f</code>: Path to serialized <code>.sp</code>, <code>.spec</code>, or <code>.cwt</code> file</li> <li><code>--type</code>, <code>-t</code>: Type of spectral result to plot (<code>spectrum</code>, <code>spectrogram</code>, <code>cwt</code>)</li> <li><code>--clip</code>, <code>-c</code>: Optional clipping in dB for spectrogram or CWT</li> <li><code>--save_path</code>: Output path to save the figure</li> </ul>"},{"location":"spectral_analysis_documentation/#scientific-references","title":"Scientific References","text":"<ul> <li>Percival, D. B., &amp; Walden, A. T. (1993). Spectral Analysis for Physical Applications: Multitaper and Conventional Univariate Techniques.</li> <li>Torrence, C., &amp; Compo, G. P. (1998). A practical guide to wavelet analysis. Bulletin of the American Meteorological Society, 79(1), 61\u201378.</li> </ul>"},{"location":"trace_header_documentation/","title":"Trace Header","text":"<p>SurfQuake processing and visualization tools make extensive use of the metadata embedded in ObsPy Trace objects (MiniSEED format). These headers not only contain acquisition-related data (station, channel, sampling rate) but are also extended during analysis to store arrival picks, reference markers, and more.</p> <p>SurfQuake processing and visualization tools make extensive use of the metadata embedded in ObsPy Trace objects, which are often read from MiniSEED (MSEED) files. While MSEED is a widely accepted format for storing seismic time series, it is inherently read-only in terms of header modification\u2014meaning users cannot persistently embed new metadata directly into the original file.</p> <p>To overcome this limitation, SurfQuake dynamically extends the Trace.stats header in memory during processing and interaction. This enriched metadata includes not only core acquisition parameters like network, station, channel, and sampling rate, but also a wide array of analysis results such as:</p> <p>Arrival picks from manual or automatic phase picking</p> <p>Reference markers used for alignment or annotation</p> <p>Geodetic attributes like source-receiver distance, backazimuth, and theoretical arrival times</p> <p>Since these enhancements cannot be stored back into standard MSEED files, SurfQuake writes processed trace data to HDF5 (.h5) format. This format supports flexible, hierarchical storage and allows full preservation of the enriched metadata alongside waveform data\u2014ensuring reproducibility, traceability, and compatibility with downstream processing.</p> <p>By adopting HDF5 for intermediate and final outputs, SurfQuake enables a more transparent and integrated seismic analysis workflow, without losing critical contextual information during each stage of the pipeline.</p> <p>This guide outlines the custom fields and their use cases for SurfQuake users.</p>"},{"location":"trace_header_documentation/#core-header-fields-obspy-standard","title":"Core Header Fields (ObsPy Standard)","text":"<p>These fields are part of the default <code>trace.stats</code> dictionary and are typically populated during reading:</p> Field Description <code>network</code> Network code (e.g., <code>IU</code>, <code>CI</code>) <code>station</code> Station code (e.g., <code>ANMO</code>) <code>location</code> Location code (e.g., <code>00</code>, <code>10</code>) <code>channel</code> Channel name (e.g., <code>BHZ</code>, <code>HHN</code>) <code>starttime</code> Trace start time (<code>UTCDateTime</code>) <code>endtime</code> Trace end time (<code>UTCDateTime</code>) <code>sampling_rate</code> Sampling rate in Hz <code>npts</code> Number of points"},{"location":"trace_header_documentation/#header-info-from-command-line","title":"Header Info from Command Line","text":"<p>surfquake facilitates a tool to explore the information of your seismogram files. Supports standard and SurfQuake-extended headers such as picks, references, and geodetic attributes</p> <pre><code>surfquake -w [path to waveform files (e.g. './data/*Z or './mseed1,./mseed2] -c [Number of traces (columns) to show per file, default 1]\n</code></pre> <p>Example <pre><code>&gt; surfquake info -w './ES.ELOB..HHZ.D.2018.233,./ES.EMAZ..HHZ.D.2018.233' -c 2\n\nFile                 | ES.ELOB..HHZ.D.2018.233   | ES.EMAZ..HHZ.D.2018.233  \nNetwork              | ES                        | ES                       \nStation              | ELOB                      | EMAZ                     \nLocation             |                           |                          \nChannel              | HHZ                       | HHZ                      \nStart Time           | 2018-08-21T00:12:58.4300  | 2018-08-21T00:12:56.3977 \nEnd Time             | 2018-08-21T00:47:06.6900  | 2018-08-21T00:47:03.5077 \nSampling Rate (Hz)   | 100.0                     | 100.0                    \nDelta (s)            | 0.01                      | 0.01                     \nNpts                 | 204827                    | 204712                   \nPicks                | None                      | None                     \nReferences           | None                      | None\n</code></pre></p>"},{"location":"trace_header_documentation/#trace-header-modification","title":"Trace header modification","text":"<p>If you need to modify the header of your traces, I recommend that you use quick or process_daily command and use the rename process. This process simply maps the original traces header information into the new one. Here, you will find an example of config file with the process of mapping network, station, location and channel codes.</p> <pre><code>Analysis:\n  process_1:\n      name: 'rename'\n\n      networks:\n        IM: IH      # Replace network XX \u2192 YY\n        IK: IN\n\n      stations:\n        IL02: XX02\n        ARNO: MELI\n\n      channels:\n        SHZ: BHZ     # Entire channel replacement\n\n      components:\n        Z: E         # Only the last component letter (Z \u2192 E, N \u2192 Z, etc.)\n</code></pre>"},{"location":"trace_header_documentation/#surfquake-extended-header-fields","title":"SurfQuake-Extended Header Fields","text":"<p>These fields are dynamically added during processing or interactive analysis:</p>"},{"location":"trace_header_documentation/#tracestatspicks-list-of-dicts","title":"<code>trace.stats.picks</code> (list of dicts)","text":"<p>Stores manually or automatically picked phase arrivals.</p> <p>Each entry contains: - <code>time</code> (float): UTC timestamp of the pick - <code>phase</code> (str): Picked phase (e.g., <code>P</code>, <code>S</code>, <code>Lg</code>) - <code>amplitude</code> (float): Measured amplitude at pick time - <code>polarity</code> (str): <code>U</code> (up), <code>D</code> (down), or <code>?</code></p> <pre><code>{\n  \"time\": 1727123830.321,\n  \"phase\": \"P\",\n  \"amplitude\": 3.21e-05,\n  \"polarity\": \"U\"\n}\n</code></pre>"},{"location":"trace_header_documentation/#tracestatsreferences-list-of-float","title":"<code>trace.stats.references</code> (list of float)","text":"<p>Stores timestamps marking user-defined reference lines during plotting (e.g., using keypress <code>w</code>). These are used for alignment, cross-checking picks, or beamforming time tags.</p>"},{"location":"trace_header_documentation/#tracestatsgeodetic","title":"<code>trace.stats.geodetic</code>","text":"<p>Stored during preprocessing via <code>set_header_func</code>:</p> Field Description <code>distance</code> Source-receiver distance (km) <code>baz</code> Backazimuth from station to source (\u00b0) <code>az</code> Azimuth from source to station (\u00b0) <code>incidence</code> Theoretical incidence angle (\u00b0) <code>arrivals</code> Dict of theoretical arrival times per phase name (UTC timestamp)"},{"location":"trace_header_documentation/#example","title":"Example:","text":"<pre><code>trace.stats.geodetic = {\n    \"distance\": 254.7,\n    \"baz\": 123.4,\n    \"az\": 303.4,\n    \"incidence\": 31.5,\n    \"arrivals\": {\n        \"P\": 1727123829.20,\n        \"S\": 1727123844.03\n    }\n}\n</code></pre>"},{"location":"trace_header_documentation/#trace-header-management-in-surfquake","title":"Trace Header Management in SurfQuake","text":""},{"location":"trace_header_documentation/#during-processing","title":"During Processing","text":"<ul> <li>Headers are updated by processing steps such as:</li> <li><code>shift</code>: aligns data using theoretical or picked times</li> <li><code>rotate</code>: may use <code>geodetic</code> data to perform LQT rotations</li> <li><code>beamforming</code>: may store attributes related to time or azimuth reference</li> </ul>"},{"location":"trace_header_documentation/#during-plotting","title":"During Plotting","text":"<ul> <li>Interactive actions modify headers:</li> <li>Press <code>e</code>: Add pick to <code>trace.stats.picks</code></li> <li>Press <code>w</code>: Add reference marker to <code>trace.stats.references</code></li> <li>Press <code>d</code> or <code>p</code>: Remove the last pick or reference</li> <li>Press <code>m</code>: Remove all reference lines</li> </ul>"},{"location":"trace_header_documentation/#best-practices","title":"Best Practices","text":"<ul> <li>Pick and reference data are stored in-memory and optionally exported to CSV.</li> <li>Ensure that when traces are saved or passed between steps, <code>trace.stats</code> is retained.</li> <li>You can inspect or modify these attributes using standard Python syntax:   <pre><code>trace.stats.picks.append({...})\ntrace.stats.references = []\n</code></pre></li> </ul>"},{"location":"trace_header_documentation/#why-this-matters","title":"Why This Matters","text":"<ul> <li>You can add custom metadata to <code>trace.stats</code> for later use (e.g., annotations, flags, temporary calculations).</li> <li>You can modify waveform data directly, apply filters, or normalize traces.</li> <li>You can access event and station metadata for context-aware processing.</li> <li>Changes made in the user script are preserved if the output is saved to HDF5 or passed through the pipeline.</li> </ul> <p>This flexibility makes SurfQuake an ideal platform for advanced, reproducible seismic workflows tailored to your research or operational needs.</p>"},{"location":"trace_header_documentation/#notes","title":"Notes","text":"<p>These headers extend the utility of MiniSEED traces in SurfQuake workflows by embedding processing history and interactive annotations, making the data more portable and self-descriptive across analysis steps.</p>"},{"location":"utils/","title":"Utilities","text":"<p>In this section we show some useful utilities to make easier how to create a metadata file and manage a catalog.</p>"},{"location":"utils/#create-metadata","title":"Create Metadata","text":""},{"location":"utils/#create-metadata-from-cli","title":"Create Metadata from CLI","text":"<p>In many ocasions users only have a stations coordinates files. Other times they have the stations coordinates files and resp files (IMPORTANT: one per stations). In this context, we can use the csv2xml tool to create the metadata that is needed in sufquake for running many modules.</p> <p>First, the stations file must be in this simple way.</p> <pre><code>Net Station Lat Lon elevation start_date starttime end_date endtime\nWM ARNO 37.0988 -6.7322 117.0 2007-01-01  00:00:00 2050-12-31  23:59:59\nWM AVE 33.2981 -7.4133 230.0 2007-01-01  00:00:00 2050-12-31  23:59:59\nWM CART 37.5868 -1.0012 65.0 2007-01-01  00:00:00 2050-12-31  23:59:59\nWM CEU 35.8987 -5.3731 320.0 2007-01-01  00:00:00 2050-12-31  23:59:59\nWM CHAS 35.1837 -2.4304 110.0 2007-01-01  00:00:00 2050-12-31  23:59:59\n</code></pre> <p>Just separated by a simple space. Do not worry to much about starttime and end time. it can be something crazy like in the example. Optionally, the user can incorporate the root path where resp files are saved to be incorporated to the metadata.</p>"},{"location":"utils/#usage","title":"Usage","text":"<pre><code>&gt;&gt; surfquake csv2xml -c [csv_file_path] -r [resp_files_path] -o [output_path] -n [stations_xml_name]\n</code></pre>"},{"location":"utils/#interactive-help","title":"Interactive help","text":"<pre><code>&gt;&gt; surfquake csv2xml -h\n</code></pre>"},{"location":"utils/#run-create-metadatafrom-cli","title":"Run Create Metadatafrom CLI","text":"<pre><code>&gt;&gt; surfquake csv2xml -c ./stations_file.txt -r ./resp_files_folder -o ./output_path -n metadata.xml\n</code></pre>"},{"location":"utils/#create-metadata-from-library","title":"Create Metadata from Library","text":""},{"location":"utils/#class","title":"Class","text":"<p><code>Convert</code> <pre><code>class Convert:\n    def __init__(self, file_path, sep='\\s+', resp_files=None):\n        self.file_path = file_path\n        self.respfiles = resp_files\n        self.sep = sep\n        self.all_resps = []\n        if self.respfiles is not None:\n             self.check_resps()\n</code></pre></p>"},{"location":"utils/#method","title":"Method","text":"<p><code>create_stations_xml</code> <pre><code>def create_stations_xml()\n</code></pre></p>"},{"location":"utils/#example-using-library","title":"Example using library","text":"<pre><code>from surfquakecore.utils.create_station_xml import Convert\nstations_file = \"/Users/roberto/Documents/python_notes/my_utils/test_data/coords.txt\"\npath_dest = \"/Users/roberto/Documents/python_notes/my_utils/test_data\"\nresp_files = \"/Users/roberto/Documents/python_notes/my_utils/resp_files\"\nname = \"metadata.xml\"\nsc = Convert(stations_file, resp_files=resp_files)\ndata_map = sc.create_stations_xml()\ninventory = sc.get_data_inventory(data_map)\nsc.write_xml(path_dest, name, inventory)\n</code></pre>"},{"location":"utils/#manage-catalog","title":"Manage catalog","text":"<p>With this tool, the user can create an Obspy catalog merging the information from the location, source spectrum and moment tensor inversion output. Finally, the user can filer the catalog by time spam or geographically. We have added a method to write in human language the catalog. See an example:</p> <pre><code>Event 46: Date 01/02/2022 02:03:00.598999 rms 0.47 s Lat 42.5250 Lon 1.4252 Depth -2.0 km +- 1.3 min_dist 0.089 max_dist 0.621 smin 0.6 km smax 0.7 km ell_azimuth 161.3 gap 65.3 conf_lev 90.0 %\nMagnitudes: Mw 3.58 +- 0.11\nMoment Tensor Solution:\nMw 3.77 Mo 5.07e+14 Nm DC 44.64 % CLVD 22.70 % iso 32.65 % variance_red 34.54\nNodal Plane: Strike 319.9 Dip 40.0 Rake -74.7\nMoment Tensor: mrr -1.05e+14 mtt 2.64e+14 mpp 3.38e+14 mrp 1.09e+13 mrt -7.33e+13 mrp -7.33e+13\nstation phase polarity date time time_residual distance_degrees distance_km azimuth takeoff_angle\nPAND P ? 01/02/2022 02:03:01.900000 -0.32 0.1 9.9 90.8 89.4\nPAND S ? 01/02/2022 02:03:03.330000 0.00 0.1 9.9 90.8 89.4\nARBS P ? 01/02/2022 02:03:02.340000 -0.48 0.1 13.6 137.9 90.8\nARBS S ? 01/02/2022 02:03:04.290000 -0.04 0.1 13.6 137.9 90.8\nCEST P ? 01/02/2022 02:03:03.200000 -0.08 0.1 16.3 300.1 87.5\nCEST S ? 01/02/2022 02:03:05.210000 0.10 0.1 16.3 300.1 87.5\nCSOR P ? 01/02/2022 02:03:05.170000 -0.19 0.3 29.1 234.8 87.9\nCSOR S ? 01/02/2022 02:03:09.050000 0.44 0.3 29.1 234.8 87.9\nSALF S ? 01/02/2022 02:03:09.580000 -0.39 0.3 32.7 322.8 87.4\nCORG P ? 01/02/2022 02:03:05.770000 -0.40 0.3 34.0 194.3 87.1\nCORG S ? 01/02/2022 02:03:10.930000 0.97 0.3 34.0 194.3 87.1\nGENF P ? 01/02/2022 02:03:06.150000 -0.21 0.3 35.1 19.0 87.6\nGENF S ? 01/02/2022 02:03:10.530000 0.26 0.3 35.1 19.0 87.6\nVALC P ? 01/02/2022 02:03:08.590000 -0.70 0.5 53.0 106.7 89.4\nCARF P ? 01/02/2022 02:03:09.900000 -0.38 0.5 59.1 69.0 88.7\nCARF S ? 01/02/2022 02:03:17.300000 0.43 0.5 59.1 69.0 88.7\nFNEB P ? 01/02/2022 02:03:11.830000 -0.09 0.6 69.1 52.5 88.3\nFNEB S ? 01/02/2022 02:03:20.730000 1.11 0.6 69.1 52.5 88.3\n</code></pre>"},{"location":"utils/#manage-catalog-from-cli","title":"Manage Catalog from CLI","text":""},{"location":"utils/#usage_1","title":"Usage","text":"<pre><code>&gt;&gt; surfquake buildcatalog -e [path_event_files_folder] -s [path_source_summary_file] -m [path_mti_summary_file] -t [type_of_catalog] -o [path_to_ouput_folder]\n</code></pre>"},{"location":"utils/#interactive-help_1","title":"Interactive help","text":"<pre><code>&gt;&gt; surfquake buildcatalog -h\n</code></pre>"},{"location":"utils/#run-create-metadatafrom-cli_1","title":"Run Create Metadatafrom CLI","text":"<pre><code>&gt;&gt; surfquake buildcatalog -e ./locations -s source_summary.txt -m mti_source_summary.txt -t \"QUAKEXML\" -o ./catalog \n</code></pre>"},{"location":"utils/#manage-catalog-from-library","title":"Manage Catalog from Library","text":""},{"location":"utils/#classes","title":"Classes","text":"<p><code>BuildCatalog</code> <pre><code>class BuildCatalog:\n    def __init__(self, loc_folder, output_path, format=\"QUAKEML\", source_summary_file=None, mti_summary_file=None):\n\n        \"\"\"\n        BuildCatalog class helps to join information from all surfquake outputs and create a catalog\n\n        Attributes:\n        - loc_folder (str): Path to the folder where the user have the locations files *hyp\n        - output_path (str): Output folder path where catalog object and file will be saved\n        - format (str): https://docs.obspy.org/packages/autogen/obspy.core.event.Catalog.write.html\n        - source_summary_file (str): Path to the output file from source module\n        - mti_summary_file (str): Path to the output file from mti module\n\n        Methods:\n        - __init__(root_path): Initialize a new instance of BuildCatalog.\n        - __merge_info(catalog: Catalog): Merges the information from loc, source and mti\n        - build_catalog(): Starts the process to create the catalog from loc files, then calls __merge_info\n        \"\"\"\n</code></pre></p> <p><code>WriteCatalog</code> <pre><code>class WriteCatalog:\n    def __init__(self, path_catalog):\n\n        \"\"\"\n        WriteCatalog class helps to filter the catalog obj write in the most Readable\n\n        Attributes:\n        - path_catalog (str): Path to the pickle file saved when run build_catalog_loc from BuildCatalog class\n\n        Methods:\n        - __init__(root_path): Initialize a new instance of BuildCatalog.\n        - filter_time_catalog(verbose=True, **kwargs): filter the catalog in time span\n        - filter_geographic_catalog(verbose=True, **kwargs): filter the catalog in geographic contrain and magnitude\n        - write_catalog(catalog: Catalog, format, output_path)\n        - write_catalog_surf(catalog: Union[Catalog, None], output_path)\n        \"\"\"\n\n        self.path_catalog = path_catalog\n        self.catalog = []\n        self.__test_catalog()\n</code></pre></p>"},{"location":"utils/#methods","title":"Methods","text":"<p><code>build_catalog_loc</code> Instance method from BuildCatalog Class <pre><code>def build_catalog_loc(self):\n</code></pre></p> <p><code>filter_time_catalog</code> Instance method from WriteCatalog Class <pre><code>    def filter_time_catalog(self, verbose=True, **kwargs):\n\n        \"\"\"\n        Filter the catalog readed in the class instantiation\n        verbose: bool:\n        **kwargs\n        starttime :str: starttime to filter the catalog in format %d/%m/%Y, %H:%M:%S.%f\n        endtime :str: endtime to filter the catalog in format %d/%m/%Y, %H:%M:%S.%f\n        example:\n        wc = WriteCatalog(catalog_path)\n        catalog_filtered = wc.filter_time_catalog(starttime=\"30/01/2022, 00:00:00.0\",\n        endtime=\"20/02/2022, 00:00:00.0\")\n        return :catalog obj:\n        \"\"\"\n</code></pre></p> <p><code>filter_geographic_catalog</code> Instance method from WriteCatalog Class <pre><code>    def filter_geographic_catalog(self, catalog: Union[Catalog, None], verbose= True, **kwargs):\n\n        \"\"\"\n        Filter the catalog readed in the class instantiation or the catalog provided in bu the user when the method\n        is called\n        verbose: bool:\n        catalog obj (optional), if not found it uses catalog from the catalog attribute\n        **kwargs --&gt; All keys must be used to filter success.\n\n        lat_min:float\n        lat_max:float\n        lon_min:float\n        lon_max:float\n        depth_min:float: km\n        depth_max:float: km\n        mag_min:float\n        mag_max:float\n\n        return :catalog obj:\n        \"\"\"\n</code></pre></p> <p><code>write_catalog_surf</code> Instance method from WriteCatalog Class <pre><code>    def write_catalog_surf(self, catalog: Union[Catalog, None], output_path):\n\n        \"\"\"\n        Writes in human language the catalog instantiated with the class\n        verbose: bool:\n        catalog obj (optional), if not found it uses catalog from the catalog attribute\n        \"\"\"\n</code></pre></p>"},{"location":"utils/#examples-using-library","title":"Examples using library","text":"<p>First, We can join all infrmation from located events in your loc folder, with the information of magnitudes and source parameters plus the ourput from Moment Tensor.</p> <pre><code>from surfquakecore.utils.manage_catalog import BuildCatalog\npath_events_folder = \"/Volumes/LaCie/surfquake_test/test_nll_loc\"\npath_source_file = \"/Volumes/LaCie/surfquake_test/catalog_output/sources.txt\"\noutput_path = \"/Volumes/LaCie/surfquake_test/catalog_output\"\nbc = BuildCatalog(loc_folder=path_events_file, source_summary_file=path_source_file, output_path=output_path,\n                      format=\"QUAKEML\")\nbc.build_catalog_loc()\n</code></pre> <p>Secondly, We can play with the catalog, filtering it (filter_time_catalog and filter_geographic_catalog) or writing a human readable version using write_catalog_surf method.</p> <pre><code>from surfquakecore.utils.manage_catalog import WriteCatalog\n\ncatalog_path = \"/Volumes/LaCie/all_andorra/catalog/catalog_obj.pkl\"\noutput_path = \"/Volumes/LaCie/all_andorra/catalog/catalog_surf.txt\"\nwc = WriteCatalog(catalog_path)\nprint(wc.show_help())\nhelp(wc.filter_time_catalog)\nhelp(wc.filter_geographic_catalog)\ncatalog_filtered = wc.filter_time_catalog(starttime=\"30/01/2022, 00:00:00.0\", endtime=\"20/02/2022, 00:00:00.0\")\n\ncatalog_filtered = wc.filter_geographic_catalog(catalog_filtered, lat_min=42.1, lat_max=43.0, lon_min=0.8, lon_max=1.5,\n                                                depth_min=-10, depth_max=20, mag_min=3.4, mag_max=3.9)\n\nwc.write_catalog_surf(catalog=None, output_path=output_path)\n#wc.write_catalog_surf(catalog=catalog_filtered, output_path=output_path)\n\n# Now you can also save the filtered catalog or even plot the catalog using common Obspy Methods\n#catalog_filtered.plot(projection='local')\n</code></pre>"},{"location":"utils/#build-mti-config-files","title":"Build MTI config files","text":"<p>surfQuake core library incorporates and easy way to generate mti Config Files The only thing you need is a previously crated catalog and a mti.ini files that serves of template (just copy and paste the example in the explanation of config file from MTI). The routine will create the mti.ini files using the information from the event catalogs (date, latitude, lngitude, depth and magnitude) and from the rest of the fields from the template. This action will write a mti.ini file per event and ready to be used by MTI toolbox.</p>"},{"location":"utils/#build-mti-config-files-from-cli","title":"Build MTI config files from CLI","text":""},{"location":"utils/#usage_2","title":"Usage","text":"<pre><code>&gt;&gt; surfquake buildmticonfig -c [catalog_file_path] -t [mti_config_template] -o [output_folder] -s [if starttime] -e [if endtime] -l [if lat_min] -a [ if lat_max] -d [if lon_min] -k [if lon_max] -w [if depth_min] -f [depth_max] -g [if mag_min] -p [if mag_max]\n</code></pre>"},{"location":"utils/#interactive-help_2","title":"Interactive help","text":"<pre><code>&gt;&gt; surfquake buildmticonfig -h\n</code></pre>"},{"location":"utils/#run-build-mti-config-files-from-cli","title":"Run Build MTI config files from CLI","text":"<pre><code>&gt;&gt; surfquake buildmticonfig --catalog_file_path /mti_config_test/catalog_obj.pkl --mti_config_template /mti_confis_test/template.ini --output_folder /mti_confis_test --starttime \"30/09/2021, 00:00:00.0\" --endtime \"30/09/2022, 00:00:00.0\" --lat_min 38.0 --lat_max 44.0 --lon_min -2.0 --lon_max 4.0 --depth_min -3.0 --depth_max 50 --mag_min 3.0 --mag_max 4.0\n</code></pre>"},{"location":"utils/#build-mti-config-files-from-library","title":"Build MTI config files from Library","text":""},{"location":"utils/#classes_1","title":"Classes","text":"<p><code>BuildMTIConfigs</code> <pre><code>class BuildMTIConfigs:\n    def __init__(self, catalog_file_path, mti_config: Union[str, MomentTensorInversionConfig], output_path):\n\n        self.catalog_file_path = catalog_file_path\n        self.config_mti_template = mti_config\n        self.output_path = output_path\n        self.catalog = None\n\n        if isinstance(mti_config, str) and os.path.isfile(mti_config):\n            self.mti_template_configuration = (load_mti_configuration(mti_config),)\n\n        else:\n            raise ValueError(f\"mti_config {mti_config} is not valid. It must be a valid .ini file for \"\n                             f\"MomentTensorInversionConfig\")\n</code></pre></p>"},{"location":"utils/#methods_1","title":"Methods","text":"<p><code>write_mti_ini_file</code> <pre><code>    def write_mti_ini_file(self, **kwargs):\n\n        \"\"\"\n        starttime :str: starttime to filter the catalog in format %d/%m/%Y, %H:%M:%S.%f\n        endtime :str: endtime to filter the catalog in format %d/%m/%Y, %H:%M:%S.%f\n        lat_min:float\n        lat_max:float\n        lon_min:float\n        lon_max:float\n        depth_min:float: km\n        depth_max:float: km\n        mag_min:float\n        mag_max:float\n        \"\"\"\n</code></pre></p>"},{"location":"utils/#examples-using-library_1","title":"Examples using library","text":"<pre><code>from surfquakecore.moment_tensor.mti_parse import BuildMTIConfigs\n\ncatalog_path = \"/Users/admin/Desktop/all_andorra/catalog/catalog_obj.pkl\"\noutput_path = \"/Users/admin/Desktop/all_andorra/mti_configs_created\"\nmti_template_path = '/Users/admin/Desktop/all_andorra/mti_configs/mti_template.ini'\n\nbmc = BuildMTIConfigs(catalog_file_path=catalog_path, mti_config=mti_template_path, output_path=output_path)\nbmc.write_mti_ini_file(starttime=\"01/01/2021, 00:00:00.0\", endtime=\"30/10/2022, 00:00:00.0\", lat_min=40.0,\n                       lat_max=44.0, lon_min=0.0, lon_max=4.2, depth_min=-10, depth_max=60,\n                       mag_min=3.0, mag_max=4.0)\n</code></pre>"}]}